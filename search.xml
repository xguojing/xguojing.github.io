<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[桂林、长沙元旦之旅]]></title>
    <url>%2F%E5%85%83%E6%97%A6%E6%B8%B8%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[趁着元旦假期去了桂林和长沙，感觉还是很开心的😄。 桂林山水甲天下，典型的喀斯特地形构成别具一格的桂林山水：“山青、水秀、洞奇、石美”。来到桂林，则必然要去游漓江，乘坐竹筏从杨堤码头出发，沿着漓江经过九马画山最后到兴坪，能够充分的欣赏别具一格的桂林山水。桂林景点非常多，在市区有象鼻山景区和两江四湖，象鼻山听说是桂林的代表，可惜没去成。还有龙脊梯田，芦笛岩等，桂林景点比较分散，交通也不是很发达，所以在桂林两天也就游了漓江、银子岩和两江四湖。如果喜欢桂林的山水，不怕累的话可以在桂林多玩几天，推荐在4、5月份或者10、11月份去。 长沙是一座非常美丽的城市，有美食美景，还有历史。到了长沙，则必然要去橘子洲头瞻仰青年毛泽东。那一天长沙刚好下大雪，雪中的橘子洲具有别样的景色。 长沙有美景，更有美食，最有名的当然是长沙臭豆腐。长沙有很多小吃街，我们逛得最多的是太平老街，整条街都是吃的。湘菜也是非常美味，我们吃了炊烟时代的小炒黄牛肉，非常赞，本来想去一盏灯吃炒鸭掌筋的，人太多了排不上队。 还去逛了湖南大学，岳麓书院因为雨雪天气闭园没去成，很可惜，还想去湖南广播电视台玩的，也没去成。长沙好玩的好吃的真是太多了，真的是值得去第二次的城市。 下面一些照片供大家欣赏 银子岩 20元人民币背景 桂林双子塔 青年毛浙东艺术雕塑]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之工厂方法模式，附Java代码示例]]></title>
    <url>%2Ffactory-method-pattern%2F</url>
    <content type="text"><![CDATA[模式定义工厂方法模式(Factory Method Pattern)又称为工厂模式，也叫虚拟构造器(Virtual Constructor)模式或者多态工厂(Polymorphic Factory)模式，它属于类创建型模式。在工厂方法模式中，工厂父类负责定义创建产品对象的公共接口，而工厂子类则负责生成具体的产品对象，这样做的目的是将产品类的实例化操作延迟到工厂子类中完成，即通过工厂子类来确定究竟应该实例化哪一个具体产品类。 概念有点抽象，给大家举个栗子： 铁匠制造武器。精灵需要精灵的武器，兽人需要兽人的武器。根据不同的客户，召唤正确类型的铁匠。 它提供了一种将实例化逻辑委托给子类的方法。 模式结构工厂方法模式包含如下角色： Product：抽象产品 ConcreteProduct：具体产品 Factory：抽象工厂 ConcreteFactory：具体工厂 在铁匠那个例子中，武器是抽象产品，精灵的武器和兽人的武器是具体产品；铁匠是抽象工厂，能够打造具体武器的铁匠是具体工厂。 程序实例以铁匠为例。首先，我们有一个blacksmith接口和它的一些实现 123456789101112131415public interface Blacksmith &#123; Weapon manufactureWeapon(WeaponType weaponType);&#125;public class ElfBlacksmith implements Blacksmith &#123; public Weapon manufactureWeapon(WeaponType weaponType) &#123; return new ElfWeapon(weaponType); &#125;&#125;public class OrcBlacksmith implements Blacksmith &#123; public Weapon manufactureWeapon(WeaponType weaponType) &#123; return new OrcWeapon(weaponType); &#125;&#125; 现在，随着客户的到来，正确类型的铁匠被召集起来，要求制造武器 1234Blacksmith blacksmith = new ElfBlacksmith();blacksmith.manufactureWeapon(WeaponType.SPEAR);blacksmith.manufactureWeapon(WeaponType.AXE);// Elvish weapons are created 适用环境适合工厂方法模式的情形： 一个类不知道它所需要的对象的类：在工厂方法模式中，客户端不需要知道具体产品类的类名，只需要知道所对应的工厂即可，具体的产品对象由具体工厂类创建；客户端需要知道创建具体产品的工厂类。 一个类通过其子类来指定创建哪个对象：在工厂方法模式中，对于抽象工厂类只需要提供一个创建产品的接口，而由其子类来确定具体要创建的对象，利用面向对象的多态性和里氏代换原则，在程序运行时，子类对象将覆盖父类对象，从而使得系统更容易扩展。 将创建对象的任务委托给多个工厂子类中的某一个，客户端在使用时可以无须关心是哪一个工厂子类创建产品子类，需要时再动态指定，可将具体工厂类的类名存储在配置文件或数据库中。 参考： 图说设计模式https://java-design-patterns.com/…]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[看懂UML类图和时序图]]></title>
    <url>%2Fdesign-patterns-uml%2F</url>
    <content type="text"><![CDATA[参考：图说设计模式 类之间的六种关系类的继承结构表现在UML中为：泛化(generalize)与实现(realize)。 泛化关系(generalization)继承关系为 is-a的关系；两个对象之间如果可以用 is-a 来表示，就是继承关系：（..是..) eg：自行车是车、猫是动物 泛化关系用一条带空心箭头的实线表示；如下图表示（A继承自B）； 注：最终代码中，泛化关系表现为继承非抽象类； 实现关系(realize)实现关系用一条带空心箭头的虚线表示； 注：最终代码中，实现关系表现为继承抽象类； 聚合关系(aggregation)聚合关系用一条带空心菱形箭头的实线表示，如下图表示A聚合到B上，或者说B由A组成； 聚合关系用于表示实体对象之间的关系，表示整体由部分构成的语义；例如一个部门由多个员工组成； 与组合关系不同的是，整体和部分不是强依赖的，即使整体不存在了，部分仍然存在；例如， 部门撤销了，人员不会消失，他们依然存在； 组合关系(composition)组合关系用一条带实心菱形箭头直线表示，如下图表示A组成B，或者B由A组成； 与聚合关系一样，组合关系同样表示整体由部分构成的语义；比如公司由多个部门组成； 但组合关系是一种强依赖的特殊聚合关系，如果整体不存在了，则部分也不存在了；例如， 公司不存在了，部门也将不存在了； 关联关系(association)关联关系是用一条直线表示的；它描述不同类的对象之间的结构关系；它是一种静态关系， 通常与运行状态无关，一般由常识等因素决定的；它一般用来定义对象之间静态的、天然的结构； 所以，关联关系是一种“强关联”的关系； 比如，乘车人和车票之间就是一种关联关系；学生和学校就是一种关联关系； 关联关系默认不强调方向，表示对象间相互知道；如果特别强调方向，如下图，表示A知道B，但 B不知道A； 注：在最终代码中，关联对象通常是以成员变量的形式实现的； 依赖关系(dependency)依赖关系是用一套带箭头的虚线表示的；如下图表示A依赖于B；他描述一个对象在运行期间会用到另一个对象的关系； 与关联关系不同的是，它是一种临时性的关系，通常在运行期间产生，并且随着运行时的变化； 依赖关系也可能发生变化； 显然，依赖也有方向，双向依赖是一种非常糟糕的结构，我们总是应该保持单向依赖，杜绝双向依赖的产生； 注：在最终代码中，依赖关系体现为类构造方法及类方法的传入参数，箭头的指向为调用关系；依赖关系除了临时知道对方外，还是“使用”对方的方法和属性； 时序图时序图（Sequence Diagram）是显示对象之间交互的图，这些对象是按时间顺序排列的。时序图中显示的是参与交互的对象及其对象之间消息交互的顺序。 时序图包括的建模元素主要有：对象（Actor）、生命线（Lifeline）、控制焦点（Focus of control）、消息（Message）等等。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>UML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何学习之“组块化”总结]]></title>
    <url>%2Fhow-to-learn-chunkings-summary%2F</url>
    <content type="text"><![CDATA[从神经科学的角度说，组块是信息片段通过使用，经常也通过实际意义联系在一起。你可以把组块看成是闪烁的神经网络，将关键想法或动作紧密联系在一起。组块可以扩大和复杂化，同时也是你调动记忆内容的捷径，可以像缎带一样落入工作记忆的插槽中。 构建组块的最佳方式是 高度集中的注意力 对基本概念的理解 以及通过练习帮助你加深对模式和更大范围的情境的理解。回顾 脱离书本努力想起关键点 是促进组块化的最佳方式之一 。这似乎能帮助形成神经挂钩 帮助你更好地理解材料。另一种方式是 尝试在最初的学习场所外来回忆材料 这会使记忆更加深刻 容易调动 任何地方都可以 这对于考试非常有帮助。 知识迁移是指 你在某个领域掌握的组块 可以帮助你学习另一个领域的组块 两个领域可能具有惊人的共通性。 交替学习 选出的各种不同概念 方法与技术 在同一段时间全部加以练习 组块非常重要 但并不意味着能提高灵活性 而这一点对于真正精通所学内容十分重要 学习时对能力的错觉 也有研究 学会意识到你是否在欺骗自己 你是否真的在对材料进行学习 时不时自测 用一些小测验看看自己是否真正理解材料 还是在欺骗自己 你认为自己在学习 事实却不是这样 回忆 也是一种小测验的方式 要避免过度依赖拿荧光笔给要点做标记 这会让你以为自己记住了那些内容 实际却并未掌握 学习时不要害怕犯错 这能让你察觉到对自己能力的错觉 不要只练习简单的部分 这会给你一种自己已完全掌握材料的错觉 有意练习你认为困难的部分 这样可以更好地全面掌握材料 思维定势 是对事物已有的看法 或者说是你已经充分发展巩固了的神经模式 这会妨碍你产生更好的想法 或者让你不能灵活地接受那些 更好或更合适的新方法 幸运法则也很有帮助 幸运女神会眷顾努力之人 从微不足道的点开始学起 一个接一个 不断努力 你会得到满意的结果。]]></content>
      <categories>
        <category>如何学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何学习之回顾法]]></title>
    <url>%2Fhow-to-learn-recall%2F</url>
    <content type="text"><![CDATA[最常见的学习方法之一 就是反复阅读， 不过心理学家Jeffrey Karpicke证明：这种方法的成效远不及另一种简单技巧，回顾——阅读材料后，移开视线，看看你能回忆起多少内容。 在回顾知识时，我们并非机械地复述，而是在通过回顾这个过程加深理解，这有助于我们形成知识组块，就好像回忆过程帮助我们在神经上嵌入了“钩子” 以便我们串联起前后知识。 更让研究者们出乎意料的是，学生们单纯地阅读和回顾材料并不是最佳的学习方法。他们认为思维导图——即画出概念之间的联系才是最佳途径。然而根基还没打牢就开始空建框架联系，实属徒劳无功 。 比起被动重复阅读，回顾即在心里检索关键概念可以使你的学习更加专注高效。只有隔上一定时间后再重读才会有效果，因为重读就更像是间隔重复练习。 回顾是一种有效工具，不过这里有另一个小贴士：在常规学习场所以外回顾材料会帮助你加深对材料的理解。你可能没有意识到这一点，但是当你学习新事物的时候，你通常会把最开始接触材料的地方当作潜意识中的提示，但一到考试就乱了阵脚。因为考试与学习场所通常不同，通过在不同物理环境下回顾和思考学习资料，你会脱离对给定场所的依赖，这会帮助你避免由于考试与学习场所的不同而产生的问题。]]></content>
      <categories>
        <category>如何学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[学术英语之12种动词时态]]></title>
    <url>%2Facademic-english-verb-tenses%2F</url>
    <content type="text"><![CDATA[Simple Tenses首先是一般时态，一般现在时，一般过去时，一般将来时 Simple Present Tense Jerry plays tennis everyday. Simple Past Tense Jerry played tennis yesterday. Simple Future Tense Jerry will play tennis next weekend. 一般现在时用来表示通常会发生或不会发生的事情，它表示一种习惯，因此“Jerry每天都打网球”表示他经常做的事情。动词plays,你会发现末尾有个s对于单数名词来讲，在一般现在时态里我们必须在动词末尾加s，如果是复数名词，你在动词末尾不能再加任何字母。 一般过去时表示我们想说明某事发生在过去并且在过去某个特定时间已经做完，Jerry昨天打了网球，当他做某事的时候是个特定时间，为了表示动作发生在一般过去时，对于常规动词来讲，只要末尾加-ed。比较难得部分是不常规动词比如sit或eat你必须要去背所有这些不常规动词的特殊的过去时态。 要表示一般将来时，我们在动词原形前加上单词will所以，will play用来表示将来时态。 Progressive Tenses进行时 Present Progressive Jennifer is walking to class. Past Progressive Jennifer was walking to class. Future Progressive Jennifer will be walking to class. 现在进行时表示正在发生的事情。为了表示现在进行时，我们两个动词部分我们要有be动词和动词ing形式，如果主语是单数，你就用单数be动词is如果主语是复数比如说Jennifer and Mary 那你就用be动词are, 加上动词ing。 过去进行时我们要表达的是一个动作在过去某段时间持续进行。同样的，我们要有动词的两个部分，be动词和-ing 但是在过去进行时里，你要用动词的过去形式，was 和 were ，was表示单数，were表示复数，然后加上动词ing将来进行时表示某件事情在未来的某段时间里会持续发生。 要表示将来进行时，我们要用到will加be,再加上动词ing形式will后面是自动的要跟动词原形的，所以我们不需要is和are在will后你跟be动词原形再加动词ing形式。 Perfect Tenses完成时态 Present Perfect Steve has eaten sushi before. Past Perfect Alan had not studied before he took the test. Future Perfect By next weekend, I will have seen the new movies six times. 现在完成时用来表示某事发生在现在之前的某个不确定时间里，或者某事在过去经常发生大部分时间，但是同样的是个不确定的时间。现在完成时的重点是，表示事情发生在过去即现在之前。因此他和当前有联系，但是事情来自过去在第一个例子里，Steve has eaten sushi before 他什么时候吃了寿司？之前的某个时间，但我们不知道具体什么时候。 要表示现在完成时，我们需要两个部分的动词我们需要现在时态的动词have如果是单数的话我们用has，然后我们要一个过去分词eat的过去分词是eaten 过去完成时有点难，过去完成时与现在没有任何关系，这就是它和现在完成时的区别。现在完成时“现在”很重要，过去完成时“现在”是不重要的。 当我们使用过去完成时的时候，我们是在讲过去的两个动作我们要表达的是一个动作发生在另一个动作之前都是过去的动作。因此，在这个例子里，Alan had not studied before he took the test 是哪两个动作？学习和考试所以我们要表达的是，在过去他考试之前没有学习，这是一个否定句，过去完成时由have的过去时态也就是had，以及study的过去分词，也就是studied，构成。在这里动词只是一般过去形式，一个动作发生在另一个动作之前就是过去完成时了 将来完成时涉及到将来的将来的两个动作，同样的，“现在”在将来完成时里并不重要。 在这个例子里，I will have seen the new movie six times by nest weekend 有两个时间节点，看电影和下个周末。所以在下个周末之前，我已经看了这部电影。表达一个事情将发生在另一个之前，我是用将来完成时，will have seen我用will 加上动词have 的原型，因为它在will之后 以及see的过去分词，也就是seen将来完成时告诉我这个动作将会发生在这个时间之前。 Perfect Progressive Tenses Present Perfect Progressive She has been waiting for a long time. Past Perfect Progressive He had been sleeping for ten hours. Future Perfect Progressive We will have been studing for a month. 现在完成进行时有三个动词部分,你要有have动词，have或has。因为这表示完成，就在这个时候已经完成。你要有be动词的过去分词been，因为这也是完成时一部分，你还需要在be后面加-ing动词，这表示动作持续进行。 一般现在完成时表示某事情开始于之前并且持续进行，而且可能持续更长时间。一般现在完成时和现在完成时的差别不是很大，重点在于当时动作在继续进行。 过去完成进行时表示某事发生在过去，并且可能继续发生在另一件事之前。He had been sleeping for ten hours. 那表示他一直在进行这个行为，持续了10个小时，重点在于他做这件事情的时间长度。过去完成进行时的构成是，have的过去式，因此我们又要用had了然后be动词的过去分词been, 最后一个-ing动词 将来完成进行时强调一个发生在未来的持续动作，这个动作还会保持继续。We will have been studying for a month 和将来完成时有相同的意思，we will have studied for a month但是因为它既是完成时又是进行时我们强调的是在某个时间段里动作的持续进行]]></content>
      <categories>
        <category>Academic English</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树遍历]]></title>
    <url>%2Fbinary-tree-traversal%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/songwenjie/p/8955856.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[杭州的雪]]></title>
    <url>%2Fxihulake-snow%2F</url>
    <content type="text"><![CDATA[今年杭州早早就下起了大雪，来杭州两年，有幸遇见两场大雪。 大雪恰逢周末，于是一大早就去西湖赏雪，看断桥残雪。 断桥上还是熟悉的场景 西湖的雪景还是非常美的 大爱西湖 雪后的码头 登上了宝石山，山顶的景色很特别 山顶看白堤 哈哈哈，我堆的 山顶雪景很美，人也非常多 冬菊]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>杭州</tag>
        <tag>断桥残雪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 226. 翻转二叉树]]></title>
    <url>%2Fleetcode-226-invert-binary-tree%2F</url>
    <content type="text"><![CDATA[题目描述翻转一棵二叉树。 示例： 输入： 123456&gt; 4&gt; / \&gt; 2 7&gt; / \ / \&gt; 1 3 6 9&gt; 输出： 123456&gt; 4&gt; / \&gt; 7 2&gt; / \ / \&gt; 9 6 3 1&gt; 备注:这个问题是受到 Max Howell 的 原问题 启发的 ： 谷歌：我们90％的工程师使用您编写的软件(Homebrew)，但是您却无法在面试时在白板上写出翻转二叉树这道题，这太糟糕了。 思路递归 AC代码（Java）1234567891011121314151617181920/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public TreeNode invertTree(TreeNode root) &#123; if (root == null) return root; TreeNode tmp = root.left; root.left = invertTree(root.right); root.right = invertTree(tmp); return root; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 219. 存在重复元素 II]]></title>
    <url>%2Fleetcode-219-contain-duplicate-two%2F</url>
    <content type="text"><![CDATA[题目描述给定一个整数数组和一个整数 k，判断数组中是否存在两个不同的索引 i 和 j，使得 nums [i] = nums [j]，并且 i 和 j 的差的绝对值最大为 k。 示例 1: 123&gt; 输入: nums = [1,2,3,1], k = 3&gt; 输出: true&gt; 示例 2: 123&gt; 输入: nums = [1,0,1,1], k = 1&gt; 输出: true&gt; 示例 3: 123&gt; 输入: nums = [1,2,3,1,2,3], k = 2&gt; 输出: false&gt; 思路想将数组进行排序，排序后的数组容易检查是否存在相同元素，若存在相同元素在进一步判断索引绝对值是否符合要求 AC代码（Java）1234567891011121314151617181920212223242526class Solution &#123; public boolean containsNearbyDuplicate(int[] nums, int k) &#123; int[] temps = Arrays.copyOf(nums, nums.length); Arrays.sort(temps); int temp = 0; boolean flag = true; for (int i = 0; i &lt; temps.length - 1; i++) &#123; if (temps[i] == temps[i + 1]) &#123; temp = temps[i]; flag = false; break; &#125; &#125; if (flag) return false; for (int j = 0; j &lt; nums.length - 1; j++) &#123; if (nums[j] == temp) &#123; for (int index = 1; index &lt;= k; index++) &#123; if (j + index &lt; nums.length &amp;&amp; nums[j + index] == temp) return true; &#125; &#125; &#125; return false; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Hadoop之HDFS架构]]></title>
    <url>%2Fhdfs-architecture%2F</url>
    <content type="text"><![CDATA[Hadoop分布式文件系统（HDFS）是一种分布式文件系统。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的差异是值得我们注意的： HDFS具有高度容错能力，旨在部署在低成本硬件上。(高容错) HDFS提供对数据的高吞吐量访问，适用于具有海量数据集的应用程序。（高吞吐量） HDFS放宽了一些POSIX要求，以实现对文件系统数据的流式访问。（流式访问） HDFS最初是作为Apache Nutch网络搜索引擎项目的基础设施而构建的。HDFS是Apache Hadoop Core项目的一部分。项目URL是http://hadoop.apache.org/ 目标和假设硬件故障检测：硬件故障是常态而非例外。Hadoop通常部署在低成本的硬件上，并且通常包含成百上千的服务器，每个服务器都存储文件系统数据的一部分。由于存在大量的组件，并且每个组件都具有不可忽略（non-trivial ）的故障概率，这意味着HDFS的某些组件始终都不起作用。因此，故障检测并快速恢复是HDFS的核心架构目标。 流式访问：HDFS更适合批处理而不是交互式使用，更加注重数据访问的高吞吐量而不是数据访问的低延迟。在HDFS上运行的应用程序需要对其数据集进行流式访问。 海量数据集：运行在HDFS上的应用程序具有大型数据集，HDFS中的一个典型文件的大小是g到tb，因此，HDFS被调优为支持大文件。它应该提供高聚合数据带宽，并可扩展到单个集群中的数百个节点。它应该在一个实例中支持数千万个文件。 一致性模型：HDFS应用程序需要一个一次写入多次读取的文件访问模型。文件一旦创建、写入和关闭，除了追加和截断操作外，无需要更改。支持将内容追加到文件末尾，但无法在任意点更新。该假设简化了数据一致性问题并实现了高吞吐量数据访问。MapReduce应用程序或Web爬虫应用程序完全适合此模型。 移动计算比移动数据便宜：应用程序请求的计算如果在其操作的数据附近执行，效率会高得多。当数据集的大小很大时尤其如此。这可以最大限度地减少网络拥塞并提高系统的整体吞吐量。因此更好的做法是将计算迁移到更靠近数据所在的位置，而不是将数据移动到运行应用程序的位置。HDFS为应用程序提供了一些接口，使它们自己更接近数据所在的位置。 跨平台和可移植：Hadoop使用Java语言开发，使得Hadoop具有良好的跨平台性。 NameNode和DataNodesHDFS具有主/从（ master/slave）架构。HDFS集群由一个NameNode和许多DataNode组成，NameNode是一个主服务器（master），管理文件系统名称空间并管理客端对数据的访问（NameNode在Hadoop集群中充当管家的角色）。此外集群中每个节点通常是一个DataNode，DataNode管理它们的节点上存储的数据。 HDFS公开文件系统名称空间，并允许用户数据存储在文件中。在内部，文件被分成一个或多个块（block），这些块存储在DataNode中。NameNode执行文件系统名称空间的相关操作，如打开、关闭和重命名文件和目录。它还确定了块到DataNode的映射（块存储到哪个DataNode中）。数据节点负责服务来自文件系统客户端的读写请求。数据节点还根据NameNode的指令执行块创建、删除和复制。 集群中单一NameNode的结构大大简化了系统的架构。NameNode是所有HDFS元数据的仲裁者和管理者，这样，用户数据永远不会流过NameNode。 文件系统名称空间(namespace)HDFS支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名称空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。 NameNode负责维护文件系统的名称空间，任何对文件系统名称空间或属性的修改都将被NameNode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由NameNode保存的。 如果想深入了解HDFS文件系统名称空间可以查看这篇博文：http://leotse90.com/… 数据复制HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。 NameNode全权管理数据块的复制，它周期性地从集群中的每个DataNode接收心跳信号(Heartbeat )和块状态报告(Blockreport)。 接收到心跳信号意味着该DataNode节点工作正常。 块状态报告包含了一个该Datanode上所有数据块的列表。 副本存放: 最最开始的一步副本的存放是HDFS可靠性和性能的关键。优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。目前实现的副本存放策略只是在这个方向上的第一步。实现这个策略的短期目标是验证它在生产环境下的有效性，观察它的行为，为实现更先进的策略打下测试和研究的基础。 大型HDFS实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通讯需要经过交换机。在大多数情况下，同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。 通过一个机架感知的过程，NameNode可以确定每个DataNode所属的机架id。一个简单但没有优化的策略就是将副本存放在不同的机架上。这样可以有效防止当整个机架失效时数据的丢失，并且允许读数据的时候充分利用多个机架的带宽。这种策略设置可以将副本均匀分布在集群中，有利于当组件失效情况下的负载均衡。但是，因为这种策略的一个写操作需要传输数据块到多个机架，这增加了写的代价。 在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。这种策略减少了机架间的数据传输，这就提高了写操作的效率。机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。于此同时，因为数据块只放在两个（不是三个）不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。在这种策略下，副本并不是均匀分布在不同的机架上。三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能。 副本选择为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。(就近原则) 安全模式NameNode启动后会进入一个称为安全模式的特殊状态。处于安全模式的NameNode是不会进行数据块的复制的。NameNode从所有的 DataNode接收心跳信号和块状态报告。块状态报告包括了某个DataNode所有的数据块列表。每个数据块都有一个指定的最小副本数。当NameNode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全(safely replicated)的；在一定百分比（这个参数可配置）的数据块被NameNode检测确认是安全之后（加上一个额外的30秒等待时间），NameNode将退出安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他DataNode上。 文件系统元数据的持久化NameNode上保存着HDFS的DataNode空间。对于任何对文件系统元数据产生修改的操作，NameNode都会使用一种称为EditLog的事务日志记录下来。例如，在HDFS中创建一个文件，NameNode就会在Editlog中插入一条记录来表示；同样地，修改文件的副本系数也将往Editlog插入一条记录。NameNode在本地操作系统的文件系统中存储这个Editlog。整个文件系统的DataNode空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为FsImage的文件中，这个文件也是放在NameNode所在的本地文件系统上。 NameNode在内存中保存着整个文件系统的DataNode空间和文件数据块映射(Blockmap)的映像。这个关键的元数据结构设计得很紧凑，因而一个有4G内存的NameNode足够支撑大量的文件和目录。当NameNode启动时，它从硬盘中读取Editlog和FsImage，将所有Editlog中的事务作用在内存中的FsImage上，并将这个新版本的FsImage从内存中保存到本地磁盘上，然后删除旧的Editlog，因为这个旧的Editlog的事务都已经作用在FsImage上了。这个过程称为一个检查点(checkpoint)。在当前实现中，检查点只发生在NameNode启动时，在不久的将来将实现支持周期性的检查点。 Datanode将HDFS数据以文件的形式存储在本地的文件系统中，它并不知道有关HDFS文件的信息。它把每个HDFS数据块存储在本地文件系统的一个单独的文件中。Datanode并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目录中支持大量的文件。当一个Datanode启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有HDFS数据块的列表，然后作为报告发送到NameNode，这个报告就是块状态报告。 通讯协议所有的HDFS通讯协议都是建立在TCP/IP协议之上。客户端通过一个可配置的TCP端口连接到NameNode，通过ClientProtocol协议与NameNode交互。而Datanode使用DatanodeProtocol协议与NameNode交互。一个远程过程调用(RPC)模型被抽象出来封装ClientProtocol和Datanodeprotocol协议。在设计上，NameNode不会主动发起RPC，而是响应来自客户端或 Datanode 的RPC请求。 健壮性HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：NameNode出错, Datanode出错和网络割裂(network partitions)。 磁盘数据错误，心跳检测和重新复制每个Datanode节点周期性地向NameNode发送心跳信号。网络割裂可能导致一部分Datanode跟NameNode失去联系。NameNode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号Datanode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机Datanode上的数据将不再有效。Datanode的宕机可能会引起一些数据块的副本系数低于指定值，NameNode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。在下列情况下，可能需要重新复制：某个Datanode节点失效，某个副本遭到损坏，Datanode上的硬盘错误，或者文件的副本系数增大。 集群均衡HDFS的架构支持数据均衡策略。如果某个Datanode节点上的空闲空间低于特定的临界点，按照均衡策略系统就会自动地将数据从这个Datanode移动到其他空闲的Datanode。当对某个文件的请求突然增加，那么也可能启动一个计划创建该文件新的副本，并且同时重新平衡集群中的其他数据。这些均衡策略目前还没有实现。 数据完整性从某个Datanode获取的数据块有可能是损坏的，损坏可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFSDataNode空间下。当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode获取该数据块的副本。 元数据磁盘错误FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS实例都将失效。因而，NameNode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低NameNode每秒处理的DataNode空间事务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当NameNode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。 增加故障恢复能力的另一个选择是使用多个NameNode 在NFS上使用共享存储或使用分布式编辑日志（称为Journal）来启用高可用性。后者是推荐的方法。 快照快照支持在特定时刻存储数据副本。快照功能的一种用途可以是将损坏的HDFS实例回滚到先前已知的良好时间点。 数据组织数据块HDFS被设计成支持大文件，适用HDFS的是那些需要处理大规模的数据集的应用。这些应用都是只写入数据一次，但却读取一次或多次，并且读取速度应能满足流式读取的需要。HDFS支持文件的“一次写入多次读取”语义。一个典型的数据块大小是128MB。因而，HDFS中的文件总是按照128M被切分成不同的块，每个块尽可能地存储于不同的Datanode中。 流水线复制当客户端向HDFS文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副本系数设置为3，当本地临时文件累积到一个数据块的大小时，客户端会从NameNode获取一个Datanode列表用于存放副本。然后客户端开始向第一个Datanode传输数据，第一个Datanode一小部分一小部分(4 KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个Datanode节点。第二个Datanode也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个Datanode。最后，第三个Datanode接收数据并存储在本地。因此，Datanode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个Datanode复制到下一个。 可访问性可以通过多种不同方式从应用程序访问HDFS。本地，HDFS 为应用程序提供了FileSystem Java API。一本Java API的C语言包装和REST API也是可用的。此外，还有HTTP浏览器，也可用于浏览HDFS实例的文件。通过使用NFS网关，HDFS可以作为客户端本地文件系统的一部分进行安装。 FS ShellHDFS以文件和目录的形式组织用户数据。它提供了一个命令行的接口(FS Shell)让用户与HDFS中的数据进行交互。命令的语法和用户熟悉的其他shell(例如 bash, csh)工具类似。下面是一些动作/命令的示例： 创建一个名为/foodir的目录 bin /hadoop dfs -mkdir /foodir 删除名为/foodir的目录 bin /hadoop fs -rm -R /foodir 查看名为/foodir/myfile.txt的文件的内容 bin /hadoop dfs -cat /foodir/myfile.txt FS shell适用于需要脚本语言与存储数据交互的应用程序。 DFSAdmin典型的HDFS安装配置Web服务器以通过可配置的TCP端口公开HDFS命名空间。这允许用户使用Web浏览器导航HDFS命名空间并查看其文件的内容。 存储空间回收文件的删除和恢复如果启用了回收站配置，当用户或应用程序删除某个文件时，这个文件并没有立刻从HDFS中删除。实际上，HDFS会将这个文件重命名转移到/trash目录(/user//.Trash)。只要文件还在/trash目录中，该文件就可以被迅速地恢复。文件在/trash中保存的时间是可配置的，当超过这个时间时，NameNode就会将该文件从DataNode空间中删除。删除文件会使得该文件相关的数据块被释放。注意，从用户删除文件到HDFS空闲空间的增加之间会有一定时间的延迟。 以下是一个示例，它将显示FS Shell如何从HDFS中删除文件。我们在目录delete下创建了2个文件（test1和test2） 123456$ hadoop fs -mkdir -p delete/test1$ hadoop fs -mkdir -p delete/test2$ hadoop fs -ls delete/Found 2 itemsdrwxr-xr-x - hadoop hadoop 0 2015-05-08 12:39 delete/test1drwxr-xr-x - hadoop hadoop 0 2015-05-08 12:40 delete/test2 我们将删除文件test1。下面的注释显示该文件已移至/trash目录。 12$ hadoop fs -rm -r delete/test1Moved: hdfs://localhost:8020/user/hadoop/delete/test1 to trash at: hdfs://localhost:8020/user/hadoop/.Trash/Current 现在我们将使用skipTrash选项删除该文件，该选项不会将文件发送到Trash。它将从HDFS中完全删除。 12$ hadoop fs -rm -r -skipTrash delete/test2Deleted delete/test2 我们现在可以看到Trash目录只包含文件test1。 123$ hadoop fs -ls .Trash/Current/user/hadoop/delete/Found 1 items\drwxr-xr-x - hadoop hadoop 0 2015-05-08 12:39 .Trash/Current/user/hadoop/delete/test1 因此文件test1进入垃圾箱并永久删除文件test2。 减少副本系数当一个文件的副本系数被减小后，NameNode会选择过剩的副本删除。下次心跳检测时会将该信息传递给Datanode。Datanode遂即移除相应的数据块，集群中的空闲空间加大。同样，在调用setReplication API结束和集群中空闲空间增加间会有一定的延迟。 参考资料Hadoop JavaDoc API HDFS源代码：http://hadoop.apache.org/… Hadoop Doc: http://hadoop.apache.org/docs/…]]></content>
      <categories>
        <category>大数据</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 190.颠倒二进制位]]></title>
    <url>%2Fleetcode-190-revers-bits%2F</url>
    <content type="text"><![CDATA[题目描述颠倒给定的 32 位无符号整数的二进制位。 示例: 12345&gt; 输入: 43261596&gt; 输出: 964176192&gt; 解释: 43261596 的二进制表示形式为 00000010100101000001111010011100 ，&gt; 返回 964176192，其二进制表示形式为 00111001011110000010100101000000 。&gt; 进阶:如果多次调用这个函数，你将如何优化你的算法？ 思路通过位运算wiki，依次截取n的二进制位的低位，放入result的高位。这一题是简单题，需掌握位运算 AC代码（Java）123456789public class Solution &#123; // you need treat n as an unsigned value public int reverseBits(int n) &#123; int ans=0; for(int i=0;i&lt;32;i++) ans|=((n&gt;&gt;&gt;i)&amp;1)&lt;&lt;(31-i); return ans; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 557. 反转字符串中的单词 III]]></title>
    <url>%2Fleetcode-557-reverse-words%2F</url>
    <content type="text"><![CDATA[题目描述给定一个字符串，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。 示例 1: 123&gt; 输入: &quot;Let&apos;s take LeetCode contest&quot;&gt; 输出: &quot;s&apos;teL ekat edoCteeL tsetnoc&quot; &gt; 注意：在字符串中，每个单词由单个空格分隔，并且字符串中不会有任何额外的空格。 思路由于字符串中不会有任何额外的空格，根据空格来找单词，将每个单词依次进行字符串的反转即可，字符串的反转可以看我这篇博客：LeetCode反转字符串 AC代码（Java）123456789101112131415161718192021222324class Solution &#123; public String reverseWords(String s) &#123; char[] cl = s.toCharArray(); int start = 0; int nextSpace = s.indexOf(' ',start); // 返回字符串s从索引start开始的第一个空格的索引，若没有，则返回-1 while(nextSpace != -1) &#123; reverse(cl,start,nextSpace - 1); start = nextSpace + 1; nextSpace = s.indexOf(' ',start); &#125; reverse(cl,start,cl.length - 1); return new String(cl); &#125; public void reverse(char[] cl,int start,int end)&#123; while(start &lt; end)&#123; char temp = cl[start]; cl[start] = cl[end]; cl[end] = temp; start ++; end --; &#125; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle机器学习入门教程一]]></title>
    <url>%2Fkaggle-learn-machine-learning-introduction%2F</url>
    <content type="text"><![CDATA[模型是如何工作的原文链接：https://www.kaggle.com/… 这门课程将从机器学习模型如何工作以及如何使用它们开始，如果您以前做过统计建模或机器学习，这可能会觉得很基础。别担心，我们很快就会建立强大的模型。 本课程将让您为以下场景构建模型: （房价预测）你表弟在房地产投机上赚了数百万美元。由于你对数据科学的兴趣，他愿意和你成为商业伙伴。他提供钱，你提供模型来预测不同房子的价值。 你问你表弟他过去是如何预测房地产价值的。他说这只是直觉。但更多的问题表明，他已经从过去看到的房子中识别出了价格模式，并利用这些模式对他正在考虑的新房子做出了预测。 机器学习也是如此。我们将从一个叫做决策树的模型开始。用更漂亮的模型可以给出更准确的预测。但是决策树很容易理解，它们是数据科学中一些最佳模型的基本构建块。 为了简单起见，我们将从最简单的决策树开始。 它只把房子分为两类。任何被考虑的房屋的预测价格是同一类别房屋的历史平均价格。 我们用数据来决定如何把房子分成两组，然后再确定每组的预测价格。从数据中获取模式的这一步称为模型拟合或训练。用于拟合模型的数据称为训练数据。 模型如何拟合(例如如何分割数据)的细节非常复杂，我们将在以后进行详细讲解。模型拟合后，您可以将其应用于新的数据，以预测其他房屋的价格。 改进决策树以下两种决策树中，哪一种更有可能通过拟合房地产训练数据而得到结果? 左边的决策树(Decision Tree 1)可能更有意义，因为它捕捉到了这样一个事实: 卧室较多的房子往往比卧室较少的房子售价更高。这种模式最大的缺点是它没有捕捉到影响房价的其他的大部分因素，如浴室数量、面积、位置等。 您可以使用具有更多“分支”的树捕获更多的因子。这些树被称为“深”树。一个决策树，也考虑每个房子的面积大小，可能是这样的: 可以通过追踪决策树来预测房子的价格，每次决策都选择与房子特征相对应的路径。这所房子的预测价格在树的底部。在底部的点叫做叶子节点。 叶节点的分割和值将由数据决定，所以现在是您检查将要处理的数据的时候了。 探索数据使用pandas探索数据任何机器学习项目的第一步都是熟悉数据。pandas库是科学家用来探索和操做数据的主要工具。大多数人在他们的代码中将pandas缩写为pd。我们用下面的命令导入pandas库： 1import pandas as pd pandas库最重要的部分是DataFrame。DataFrame保存您可能认为是表的数据类型。这类似于Excel中的工作表，或SQL数据库中的表。 pandas提供了强大的方法，可以处理您想要处理的大多数此类数据。 举个例子，我们来看看澳大利亚墨尔本的房价数据。在实践练习中，您将把相同的过程应用到一个新的数据集，该数据集包含衣阿华州的房价。 示例(墨尔本)数据位于文件路径../input/melbourne-housing-snapshot/melb_data.csv 我们使用以下命令加载和浏览数据： 123456# save filepath to variable for easier accessmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'# read the data and store data in DataFrame titled melbourne_datamelbourne_data = pd.read_csv(melbourne_file_path) # print a summary of the data in Melbourne datamelbourne_data.describe() Rooms Price Distance Postcode Bedroom2 Bathroom Car Landsize BuildingArea YearBuilt Lattitude Longtitude Propertycount count 13580.000000 1.358000e+04 13580.000000 13580.000000 13580.000000 13580.000000 13518.000000 13580.000000 7130.000000 8205.000000 13580.000000 13580.000000 13580.000000 mean 2.937997 1.075684e+06 10.137776 3105.301915 2.914728 1.534242 1.610075 558.416127 151.967650 1964.684217 -37.809203 144.995216 7454.417378 std 0.955748 6.393107e+05 5.868725 90.676964 0.965921 0.691712 0.962634 3990.669241 541.014538 37.273762 0.079260 0.103916 4378.581772 min 1.000000 8.500000e+04 0.000000 3000.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1196.000000 -38.182550 144.431810 249.000000 25% 2.000000 6.500000e+05 6.100000 3044.000000 2.000000 1.000000 1.000000 177.000000 93.000000 1940.000000 -37.856822 144.929600 4380.000000 50% 3.000000 9.030000e+05 9.200000 3084.000000 3.000000 1.000000 2.000000 440.000000 126.000000 1970.000000 -37.802355 145.000100 6555.000000 75% 3.000000 1.330000e+06 13.000000 3148.000000 3.000000 2.000000 2.000000 651.000000 174.000000 1999.000000 -37.756400 145.058305 10331.000000 max 10.000000 9.000000e+06 48.100000 3977.000000 20.000000 8.000000 10.000000 433014.000000 44515.000000 2018.000000 -37.408530 145.526350 21650.000000 解释Data Description结果为原始数据集中的每一列显示8个数字。第一个数字是count，它显示有多少行没有缺失值。 缺失值的原因有很多。例如，在测量1居室的房子时，不会收集第2居室的大小。 第二个值是均值mean，也就是平均值。在这种情况下，std是标准偏差，它度量数值的分布情况。 要解释最小值min、25%、50%、75%和最大值max ，请想像对每个列从最低值到最高值进行排序。第一个(最小的)值是最小值。如果您遍历列表的四分之一，您会发现一个值大于25%，小于75%。这就是25%的值。第50百分位和第75百分位的定义类似，最大值是最大的数字。 练习：探索数据kaggle链接：https://www.kaggle.com/… 这个练习将测试您读取数据文件和理解有关数据的统计信息的能力。 你的数据中最新的房子并不新鲜。对此有几个可能的解释: 他们还没有在收集这些数据的地方建造新房子。 这些数据是很久以前收集的。数据发布后建造的房屋不会出现。 如果原因是上面的解释#1，那么这会影响您对使用这些数据构建的模型的信任吗?如果这是第二个原因呢? 你如何深入研究这些数据，看看哪种解释更合理?]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：多节点集群]]></title>
    <url>%2Fhadoop-multi-node-cluster%2F</url>
    <content type="text"><![CDATA[本章介绍了Hadoop多节点集群在分布式环境中的设置。 由于无法演示整个集群，我们使用三个系统(一个主系统和两个从系统)解释Hadoop集群环境;以下是他们的IP地址。 Hadoop Master: 192.168.1.15 (hadoop-master) Hadoop Slave: 192.168.1.16 (hadoop-slave-1) Hadoop Slave: 192.168.1.17 (hadoop-slave-2) 按照下面给出的步骤设置Hadoop多节点集群。 安装Java参考：Hadoop安装与环境设置 映射节点您必须在/etc/文件夹中编辑所有节点上的hosts 文件，指定每个系统的IP地址及其主机名。 12345# vi /etc/hostsenter the following lines in the /etc/hosts file.192.168.1.109 hadoop-master 192.168.1.145 hadoop-slave-1 192.168.56.1 hadoop-slave-2 配置基于密钥的登录在每个节点上设置ssh，这样它们就可以彼此通信，而无需提示输入密码。 1234567# su hadoop $ ssh-keygen -t rsa $ ssh-copy-id -i ~/.ssh/id_rsa.pub tutorialspoint@hadoop-master $ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop_tp1@hadoop-slave-1 $ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop_tp2@hadoop-slave-2 $ chmod 0600 ~/.ssh/authorized_keys $ exit 安装Hadoop在主服务器中，使用以下命令下载和安装Hadoop。 1234567# mkdir /opt/hadoop # cd /opt/hadoop/ # wget http://apache.mesi.com.ar/hadoop/common/hadoop-1.2.1/hadoop-1.2.0.tar.gz # tar -xzf hadoop-1.2.0.tar.gz # mv hadoop-1.2.0 hadoop# chown -R hadoop /opt/hadoop # cd /opt/hadoop/hadoop/ 配置Hadoop您必须配置Hadoop服务器，方法如下所示。 core-site.xml打开core-site.xml文件并编辑它，如下所示。 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://hadoop-master:9000/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml打开hdfs-site.xml 文件并编辑它，如下所示。 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/hadoop/dfs/name/data&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/hadoop/dfs/name&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml打开mapred-site.xml文件并编辑它，如下所示。 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;hadoop-master:9001&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hadoop-env.sh打开hadoop-env.sh文件并编辑JAVA_HOME、hadoop op_conf_dir和hadoop op_opts，如下所示。 注意:按照系统配置设置JAVA_HOME。 1export JAVA_HOME=/opt/jdk1.7.0_17 export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true export HADOOP_CONF_DIR=/opt/hadoop/hadoop/conf 在从服务器上安装Hadoop按照给定的命令在所有从服务器上安装Hadoop。 1234# su hadoop $ cd /opt/hadoop $ scp -r hadoop hadoop-slave-1:/opt/hadoop $ scp -r hadoop hadoop-slave-2:/opt/hadoop 在主服务器上配置Hadoop打开主服务器并按照给定的命令配置它。 12# su hadoop $ cd /opt/hadoop/hadoop 配置主节点12$ vi etc/hadoop/mastershadoop-master 配置从节点123$ vi etc/hadoop/slaveshadoop-slave-1 hadoop-slave-2 在Hadoop Master上格式化NameNode123# su hadoop $ cd /opt/hadoop/hadoop $ bin/hadoop namenode –format 123456789101111/10/14 10:58:07 INFO namenode.NameNode: STARTUP_MSG: /************************************************************ STARTUP_MSG: Starting NameNode STARTUP_MSG: host = hadoop-master/192.168.1.109 STARTUP_MSG: args = [-format] STARTUP_MSG: version = 1.2.0 STARTUP_MSG: build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2 -r 1479473; compiled by 'hortonfo' on Mon May 6 06:59:37 UTC 2013 STARTUP_MSG: java = 1.7.0_71 ************************************************************/ 11/10/14 10:58:08 INFO util.GSet: Computing capacity for map BlocksMap editlog=/opt/hadoop/hadoop/dfs/name/current/edits………………………………………………….………………………………………………….…………………………………………………. 11/10/14 10:58:08 INFO common.Storage: Storage directory /opt/hadoop/hadoop/dfs/name has been successfully formatted. 11/10/14 10:58:08 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at hadoop-master/192.168.1.15 ************************************************************/ 启动Hadoop服务下面的命令是在Hadoop- master上启动所有Hadoop服务。 12$ cd $HADOOP_HOME/sbin$ start-all.sh 在Hadoop集群中添加一个新的DataNode下面给出了向Hadoop集群添加新节点的步骤。 配置网络使用适当的网络配置向现有Hadoop集群添加新节点。假设以下网络配置。 新节点配置: 123IP address : 192.168.1.103 netmask : 255.255.255.0hostname : slave3.in 添加用户和SSH访问添加用户在新节点上，使用以下命令添加“hadoop”用户，并将hadoop用户的密码设置为“hadoop op123”或您想要的任何值。 12useradd hadooppasswd hadoop 设置从主服务器到新从服务器的连接。 在主服务器上执行以下操作1234567mkdir -p $HOME/.ssh chmod 700 $HOME/.ssh ssh-keygen -t rsa -P '' -f $HOME/.ssh/id_rsa cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys chmod 644 $HOME/.ssh/authorized_keysCopy the public key to new slave node in hadoop user $HOME directoryscp $HOME/.ssh/id_rsa.pub hadoop@192.168.1.103:/home/hadoop/ 对从服务器执行以下操作登录到hadoop。 1su hadoop ssh -X hadoop@192.168.1.103 将公钥内容复制到文件“$HOME/.ssh/authorized_keys”中。然后通过执行以下命令更改相同的权限。 12345cd $HOMEmkdir -p $HOME/.ssh chmod 700 $HOME/.sshcat id_rsa.pub &gt;&gt;$HOME/.ssh/authorized_keys chmod 644 $HOME/.ssh/authorized_keys 检查主机上的ssh登录。现在检查是否可以在不需要主节点密码的情况下ssh到新节点。 1ssh hadoop@192.168.1.103 or hadoop@slave3 设置新节点的主机名您可以在文件/etc/sysconfig/network中设置主机名 123On new slave3 machineNETWORKING=yes HOSTNAME=slave3.in 要使更改生效，可以重新启动机器，也可以使用相应的主机名将主机名命令运行到新机器上(重新启动是一个不错的选择)。 slave3节点机器上: 主机名slave3.in 使用以下行更新集群所有机器上的/etc/hosts: 1192.168.1.102 slave3.in slave3 现在尝试用主机名ping计算机，检查它是否解析到IP。 在新节点机器上: 1ping master.in 在新节点上启动DataNode使用$HADOOP_HOME/bin/hadoop-daemon.sh手动启动datanode守护进程。它将自动联系主节点(NameNode)并加入集群。我们还应该将新节点添加到主服务器中的conf/slave文件中。基于脚本的命令将识别新节点。 登录到新节点 1su hadoop or ssh -X hadoop@192.168.1.103 使用以下命令在新添加的从节点上启动HDFS 1./bin/hadoop-daemon.sh start datanode 使用jps命令检查新节点 123$ jps7141 DataNode10312 Jps 从Hadoop集群中删除DataNode我们可以在节点运行时动态地从集群中删除节点，而不会丢失任何数据。HDFS提供了一个退役特性，可以确保安全地删除节点。使用方法如下: Step 1: 登录到主服务器登录到安装Hadoop的主机用户。 1$ su hadoop Step 2: 改变集群配置在启动集群之前，必须配置一个排除文件。添加一个名为dfs.hosts的键。排除到我们的$HADOOP_HOME/etc/hadoop/hdfs-site.xml文件。与此键关联的值提供了NameNode本地文件系统上文件的完整路径，该文件系统包含不允许连接到HDFS的机器列表。 例如，将这些行添加到etc/hadoop/hdfs-site.xml文件。 12345&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop-1.2.1/hdfs_exclude.txt&lt;/value&gt; &lt;description&gt;DFS exclude&lt;/description&gt; &lt;/property&gt; Step 3: 确定要删除的主机要删除的每台机器都应该添加到hdfs_exclude.txt文件中，每行一个域名。这将阻止它们连接到NameNode。如果您想删除DataNode2，则“/home/hadoop/hadoop-1.2.1/hdfs_exclude.txt”的内容文件如下所示。 1slave2.in Step 4: 强制重加载配置运行“$HADOOP_HOME/bin/hadoop dfsadmin -refreshNodes”命令，不带引号。 1$ $HADOOP_HOME/bin/hadoop dfsadmin -refreshNodes 这将强制NameNode重新读取其配置，包括最新更新的“exclude”文件。它将在一段时间内对节点进行退役，允许将每个节点的块复制到计划保持活动的机器上。 在slave2.in，检查jps命令输出。一段时间后，您将看到DataNode进程自动关闭。 Step 5: 关闭节点删除过程完成后，删除的硬件可以安全停机进行维护。向dfsadmin运行report命令检查状态。下面的命令将描述删除节点和连接到集群的节点的状态。 1$ $HADOOP_HOME/bin/hadoop dfsadmin -report Step 6: 再次排除文件机器一旦退役，就可以从“exclude”文件中删除它们。再次运行“$HADOOP_HOME/bin/hadoop dfsadmin -refreshNodes”会将排斥文件读入NameNode;允许数据节点在维护完成后重新加入集群，或者在集群中再次需要额外的容量，等等。 特别提示：如果遵循上述流程，且tasktracker流程仍在节点上运行，则需要关闭该流程。一种方法是断开机器，就像我们在上述步骤中所做的那样。主进程将自动识别进程，并将声明为已死。删除任务跟踪器不需要遵循相同的过程，因为与DataNode相比，任务跟踪器并不重要。DataNode包含您希望安全地删除的数据，而不会丢失任何数据。 任务跟踪器可以在任何时间点通过以下命令动态运行/关闭。 1$ $HADOOP_HOME/bin/hadoop-daemon.sh stop tasktracker $HADOOP_HOME/bin/hadoop-daemon.sh start tasktracker 原文链接： https://www.tutorialspoint.com/…]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：流]]></title>
    <url>%2Fhadoop-streaming%2F</url>
    <content type="text"><![CDATA[Hadoop流是Hadoop发行版附带的实用程序。这个实用程序允许您使用任何可执行文件或脚本作为mapper 和/或reducer创建和运行Map/Reduce作业。 Python例子对于Hadoop流，我们正在考虑word-count 问题。Hadoop中的任何工作都必须有两个阶段:mapper和reducer。我们已经在python脚本中为mapper和reducer编写了在Hadoop下运行它的代码。也可以用Perl和Ruby编写相同的代码。 Mapper Phase Code1234567!/usr/bin/pythonimport sys# Input takes from standard input for myline in sys.stdin: # Remove whitespace either side myline = myline.strip() # Break the line into words words = myline.split() # Iterate the words list for myword in words: # Write the results to standard output print '%s\t%s' % (myword, 1) 确保该文件具有执行权限(chmod +x /home/ expert/hadoop-1.2.1/mapper.py)。 Reducer Phase Code123456789101112131415161718192021222324#!/usr/bin/pythonfrom operator import itemgetter import sys current_word = ""current_count = 0 word = "" # Input takes from standard input for myline in sys.stdin: # Remove whitespace either side myline = myline.strip() # Split the input we got from mapper.py word, count = myline.split('\t', 1) # Convert count variable to integer try: count = int(count) except ValueError: # Count was not a number, so silently ignore this line continueif current_word == word: current_count += count else: if current_word: # Write result to standard output print '%s\t%s' % (current_word, current_count) current_count = count current_word = word# Do not forget to output the last word if needed! if current_word == word: print '%s\t%s' % (current_word, current_count) 将mapper和reducer代码分别保存在Hadoop home目录下的mapper.py and reducer.py 文件。确保这些文件具有执行权限(chmod +x mapper.py)。和chmod +x reducer.py)。由于python对缩进敏感，所以可以从下面的链接下载相同的代码。 WordCount程序的执行123456$ $HADOOP_HOME/bin/hadoop jar contrib/streaming/hadoop-streaming-1.2.1.jar \ -input input_dirs \ -output output_dir \ -mapper &lt;path/mapper.py \ -reducer &lt;path/reducer.py 其中“\”用于行延续，以确保清晰的可读性。 例如 1./bin/hadoop jar contrib/streaming/hadoop-streaming-1.2.1.jar -input myinput -output myoutput -mapper /home/expert/hadoop-1.2.1/mapper.py -reducer /home/expert/hadoop-1.2.1/reducer.py 流是如何工作的在上面的例子中，mapper和reducer都是python脚本，它们从标准输入读取输入并将输出输出到标准输出。该实用程序将创建Map/Reduce作业，将作业提交到适当的集群，并监视作业的进度，直到作业完成。 当为mappers指定脚本时，每个mapper任务将在初始化mapper时作为单独的进程启动脚本。当mapper任务运行时，它将其输入转换为行，并将这些行提供给流程的标准输入(STDIN)。同时，mapper从流程的标准输出(STDOUT)中收集面向行的输出，并将每一行转换为键/值对，作为mapper的输出进行收集。默认情况下，直到第一个制表符的行前缀是键，行其余部分(不包括制表符)是值。如果行中没有制表符，则将整行视为键，值为null。但是，这可以根据需要定制。 当为reducers指定脚本时，每个reducer任务将作为单独的进程启动脚本，然后初始化reducer。当reducer任务运行时，它将输入键/值对转换为行，并将这些行提供给流程的标准输入(STDIN)。同时，reducer从流程的标准输出(STDOUT)中收集面向行的输出，将每一行转换为键/值对，作为reducer的输出进行收集。默认情况下，直到第一个制表符的行前缀是键，行其余部分(不包括制表符)是值。但是，这可以根据特定的需求进行定制。 重要的命令 Parameters Description -input directory/file-name Input location for mapper. (Required) -output directory-name Output location for reducer. (Required) -mapper executable or script or JavaClassName Mapper executable. (Required) -reducer executable or script or JavaClassName Reducer executable. (Required) -file file-name Makes the mapper, reducer, or combiner executable available locally on the compute nodes. -inputformat JavaClassName Class you supply should return key/value pairs of Text class. If not specified, TextInputFormat is used as the default. -outputformat JavaClassName Class you supply should take key/value pairs of Text class. If not specified, TextOutputformat is used as the default. -partitioner JavaClassName Class that determines which reduce a key is sent to. -combiner streamingCommand or JavaClassName Combiner executable for map output. -cmdenv name=value Passes the environment variable to streaming commands. -inputreader For backwards-compatibility: specifies a record reader class (instead of an input format class). -verbose Verbose output. -lazyOutput Creates output lazily. For example, if the output format is based on FileOutputFormat, the output file is created only on the first call to output.collect (or Context.write). -numReduceTasks Specifies the number of reducers. -mapdebug Script to call when map task fails. -reducedebug Script to call when reduce task fails. 原文链接： https://www.tutorialspoint.com/…]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：MapReduce]]></title>
    <url>%2Fhadoop-mapreduce%2F</url>
    <content type="text"><![CDATA[MapReduce是一个框架，我们可以使用它编写应用程序，以一种可靠的方式，并行地在大型商品硬件集群上处理大量数据。 MapReduce是什么MapReduce是一种基于java的分布式计算处理技术和程序模型。MapReduce算法包含两个重要的任务，即Map和Reduce。Map接受一组数据并将其转换为另一组数据，其中单个元素被分解为元组(键/值对)。其次是reduce task，它将来自映射的输出作为输入，并将这些数据元组组合成较小的元组集合。顾名思义，reduce任务总是在映射作业之后执行。 MapReduce的主要优点是，它很容易在多个计算节点上扩展数据处理。在MapReduce模型下，数据处理原语称为映射器和约简器。将数据处理应用程序分解为映射器和还原器有时是很重要的。但是，一旦我们在MapReduce表单中编写了一个应用程序，将应用程序扩展到集群中的成百上千甚至上万台机器上，这仅仅是一个配置更改。正是这种简单的可伸缩性吸引了许多程序员使用MapReduce模型。 算法 通常MapReduce范例是基于将计算机发送到数据所在的位置! MapReduce程序分三个阶段执行，即map阶段、shuffle阶段和reduce阶段。 Map stage : 映射或映射程序的工作是处理输入数据。通常输入数据以文件或目录的形式存储在Hadoop文件系统(HDFS)中。输入文件逐行传递给mapper函数。映射器处理数据并创建几个小数据块。 Reduce stage : 这一阶段是Shuffle 阶段和Reduce 阶段的结合。Reduce 的工作是处理来自mapper的数据。处理之后，它会生成一组新的输出，这些输出将存储在HDFS中。 在MapReduce作业期间，Hadoop将映射和Reduce任务发送到集群中的适当服务器。 该框架管理数据传递的所有细节，例如发出任务、验证任务完成以及在节点之间围绕集群复制数据。 大多数计算发生在节点上，节点上的数据位于本地磁盘上，从而减少了网络流量。 在完成给定的任务后，集群收集并减少数据，形成适当的结果，并将其发送回Hadoop服务器。 输入和输出(Java方面)MapReduce框架对对进行操作，即框架将作业的输入视为一组对，并生成一组对作为作业的输出，可以想象为不同类型。 键和值类应该由框架序列化，因此需要实现可写接口。此外，关键类必须实现可写可比较的接口，以便按框架进行排序。MapReduce作业的输入输出类型:(Input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt;-&gt; reduce -&gt; &lt;k3, v3&gt;(Output) Input Output Map &lt;k1, v1&gt; list (&lt;k2, v2&gt;) Reduce &lt;k2, list(v2)&gt; list (&lt;k3, v3&gt;) 术语 PayLoad - 应用程序实现了映射和Reduce函数，构成了作业的核心。 Mapper - Mapper将输入键/值对映射到一组中间键/值对。 NamedNode - 管理Hadoop分布式文件系统(HDFS)的节点。 DataNode - 存放数据的节点。 MasterNode - 作业跟踪程序运行的节点，它接受来自客户端的作业请求。 SlaveNode - 节点，Map和Reduce程序在此运行。 JobTracker - 计划作业并跟踪分配给Task tracker的作业。 Task Tracker - 跟踪任务并向JobTracker报告状态。 Job - 程序是Mapper 和Reducer 在数据集上的执行。 Task - 在数据片上执行Mapper 或Reducer 。 Task Attempt - 试图在SlaveNode上执行任务的特定实例。 示例场景下面是关于一个组织的电力消耗的数据。它包含了每个月的用电量和不同年份的年平均用电量。 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Avg 1979 23 23 2 43 24 25 26 26 26 26 25 26 25 1980 26 27 28 28 28 30 31 31 31 30 30 30 29 1981 31 32 32 32 33 34 35 36 36 34 34 34 34 1984 39 38 39 39 39 41 42 43 40 39 38 38 40 1985 38 39 39 39 39 41 41 41 00 40 39 39 45 如果以上述数据作为输入，我们必须编写应用程序来处理它，并产生诸如查找最大使用年、最小使用年等结果。这是一个针对记录数量有限的程序员的演练。它们将简单地编写逻辑来生成所需的输出，并将数据传递给所写的应用程序。 但是，想想一个州自形成以来所有大型工业的电力消耗数据。 当我们编写应用程序来处理这种大容量数据时， 它们将花费大量的时间来执行。 当我们将数据从源移动到网络服务器等时，将会有大量的网络流量。 为了解决这些问题，我们有MapReduce框架。 输入数据以上数据保存为sample.txt作为输入。输入文件如下所示 123451979 23 23 2 43 24 25 26 26 26 26 25 26 25 1980 26 27 28 28 28 30 31 31 31 30 30 30 29 1981 31 32 32 32 33 34 35 36 36 34 34 34 34 1984 39 38 39 39 39 41 42 43 40 39 38 38 40 1985 38 39 39 39 39 41 41 41 00 40 39 39 45 程序实例下面是使用MapReduce框架对示例数据的程序 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package hadoop; import java.util.*; import java.io.IOException; import java.io.IOException; import org.apache.hadoop.fs.Path; import org.apache.hadoop.conf.*; import org.apache.hadoop.io.*; import org.apache.hadoop.mapred.*; import org.apache.hadoop.util.*; public class ProcessUnits &#123; //Mapper class public static class E_EMapper extends MapReduceBase implements Mapper&lt;LongWritable ,/*Input key Type */ Text, /*Input value Type*/ Text, /*Output key Type*/ IntWritable&gt; /*Output value Type*/ &#123; //Map function public void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException &#123; String line = value.toString(); String lasttoken = null; StringTokenizer s = new StringTokenizer(line,"\t"); String year = s.nextToken(); while(s.hasMoreTokens()) &#123; lasttoken=s.nextToken(); &#125; int avgprice = Integer.parseInt(lasttoken); output.collect(new Text(year), new IntWritable(avgprice)); &#125; &#125; //Reducer class public static class E_EReduce extends MapReduceBase implements Reducer&lt; Text, IntWritable, Text, IntWritable &gt; &#123; //Reduce function public void reduce( Text key, Iterator &lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException &#123; int maxavg=30; int val=Integer.MIN_VALUE; while (values.hasNext()) &#123; if((val=values.next().get())&gt;maxavg) &#123; output.collect(key, new IntWritable(val)); &#125; &#125; &#125; &#125; //Main function public static void main(String args[])throws Exception &#123; JobConf conf = new JobConf(ProcessUnits.class); conf.setJobName("max_eletricityunits"); conf.setOutputKeyClass(Text.class); conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(E_EMapper.class); conf.setCombinerClass(E_EReduce.class); conf.setReducerClass(E_EReduce.class); conf.setInputFormat(TextInputFormat.class); conf.setOutputFormat(TextOutputFormat.class); FileInputFormat.setInputPaths(conf, new Path(args[0])); FileOutputFormat.setOutputPath(conf, new Path(args[1])); JobClient.runJob(conf); &#125; &#125; 将上述程序保存为Process .java。下面将解释程序的编译和执行。 编译和执行让我们假设我们在Hadoop用户的主目录中(例如/home/hadoop)。 按照下面给出的步骤编译和执行上述程序 Step 1下面的命令是创建一个目录来存储编译后的java类。 1$ mkdir units Step 2下载Hadoop-core-1.2.1.jar，它用于编译和执行MapReduce程序。请访问以下链接http://mvnrepository.com/…下载jar。让我们假设下载的文件夹是/home/hadoop/。 Step 3以下命令用于编译ProcessUnits.java程序，并为该程序创建一个jar。 12$ javac -classpath hadoop-core-1.2.1.jar -d units ProcessUnits.java $ jar -cvf units.jar -C units/ . Step 4下面的命令用于在HDFS中创建一个输入目录。 1$HADOOP_HOME/bin/hadoop fs -mkdir input_dir Step 7下面的命令用于通过从输入目录中获取输入文件来运行Eleunit_max应用程序。 1$HADOOP_HOME/bin/hadoop jar units.jar hadoop.ProcessUnits input_dir output_dir 稍等片刻，直到执行该文件。执行后，如下图所示，输出将包含输入分割数、映射任务数、reducer任务数等。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657INFO mapreduce.Job: Job job_1414748220717_0002 completed successfully 14/10/31 06:02:52 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=61 FILE: Number of bytes written=279400 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=546 HDFS: Number of bytes written=40 HDFS: Number of read operations=9 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=2 Launched reduce tasks=1 Data-local map tasks=2 Total time spent by all maps in occupied slots (ms)=146137 Total time spent by all reduces in occupied slots (ms)=441 Total time spent by all map tasks (ms)=14613 Total time spent by all reduce tasks (ms)=44120 Total vcore-seconds taken by all map tasks=146137 Total vcore-seconds taken by all reduce tasks=44120 Total megabyte-seconds taken by all map tasks=149644288 Total megabyte-seconds taken by all reduce tasks=45178880 Map-Reduce Framework Map input records=5 Map output records=5 Map output bytes=45 Map output materialized bytes=67 Input split bytes=208 Combine input records=5 Combine output records=5 Reduce input groups=5 Reduce shuffle bytes=6 Reduce input records=5 Reduce output records=5 Spilled Records=10 Shuffled Maps =2 Failed Shuffles=0 Merged Map outputs=2 GC time elapsed (ms)=948 CPU time spent (ms)=5160 Physical memory (bytes) snapshot=47749120 Virtual memory (bytes) snapshot=2899349504 Total committed heap usage (bytes)=277684224 File Output Format Counters Bytes Written=40 Step 8下面的命令用于验证输出文件夹中生成的文件。 1$HADOOP_HOME/bin/hadoop fs -ls output_dir/ Step 9下面的命令用于查看Part-00000文件中的输出。这个文件是由HDFS生成的。 1$HADOOP_HOME/bin/hadoop fs -cat output_dir/part-00000 下面是MapReduce程序生成的输出 1231981 34 1984 40 1985 45 Step 10下面的命令用于将输出文件夹从HDFS复制到本地文件系统进行分析。 1$HADOOP_HOME/bin/hadoop fs -cat output_dir/part-00000/bin/hadoop dfs get output_dir /home/hadoop 重要的命令所有Hadoop命令都由$HADOOP_HOME/bin/hadoop命令调用。在没有任何参数的情况下运行Hadoop脚本将打印所有命令的描述。 Usage : hadoop [–config confdir] COMMAND 下表列出了可用的选项及其描述。 Options Description namenode -format Formats the DFS filesystem. secondarynamenode Runs the DFS secondary namenode. namenode Runs the DFS namenode. datanode Runs a DFS datanode. dfsadmin Runs a DFS admin client. mradmin Runs a Map-Reduce admin client. fsck Runs a DFS filesystem checking utility. fs Runs a generic filesystem user client. balancer Runs a cluster balancing utility. oiv Applies the offline fsimage viewer to an fsimage. fetchdt Fetches a delegation token from the NameNode. jobtracker Runs the MapReduce job Tracker node. pipes Runs a Pipes job. tasktracker Runs a MapReduce task Tracker node. historyserver Runs job history servers as a standalone daemon. job Manipulates the MapReduce jobs. queue Gets information regarding JobQueues. version Prints the version. jar Runs a jar file. distcp Copies file or directories recursively. distcp2 DistCp version 2. archive -archiveName NAME -p Creates a hadoop archive. * classpath Prints the class path needed to get the Hadoop jar and the required libraries. daemonlog Get/Set the log level for each daemon 如何与MapReduce作业交互Usage: hadoop job [GENERIC_OPTIONS] 以下是Hadoop作业中可用的通用选项。 GENERIC_OPTIONS Description -submit Submits the job. -status Prints the map and reduce completion percentage and all job counters. -counter Prints the counter value. -kill Kills the job. -events &lt;fromevent-#&gt; &lt;#-of-events&gt; Prints the events’ details received by jobtracker for the given range. -history [all] - history &lt; jobOutputDir&gt; Prints job details, failed and killed tip details. More details about the job such as successful tasks and task attempts made for each task can be viewed by specifying the [all] option. -list[all] Displays all jobs. -list displays only jobs which are yet to complete. -kill-task Kills the task. Killed tasks are NOT counted against failed attempts. -fail-task Fails the task. Failed tasks are counted against failed attempts. -set-priority Changes the priority of the job. Allowed priority values are VERY_HIGH, HIGH, NORMAL, LOW, VERY_LOW 查看工作状态123$ $HADOOP_HOME/bin/hadoop job -status &lt;JOB-ID&gt; e.g. $ $HADOOP_HOME/bin/hadoop job -status job_201310191043_0004 查看作业输出目录的历史123$ $HADOOP_HOME/bin/hadoop job -history &lt;DIR-NAME&gt; e.g. $ $HADOOP_HOME/bin/hadoop job -history /user/expert/output kill作业123$ $HADOOP_HOME/bin/hadoop job -kill &lt;JOB-ID&gt; e.g. $ $HADOOP_HOME/bin/hadoop job -kill job_201310191043_0004 原文链接： https://www.tutorialspoint.com/…]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：命令手册]]></title>
    <url>%2Fhadoop-command-reference%2F</url>
    <content type="text"><![CDATA[与这里演示的相比，“$HADOOP_HOME/bin/hadoop fs” 中有更多的命令，尽管这些基本操作可以帮助您入门。不带附加参数运行 ./bin/hadoop dfs 将列出所有可以与 FsShell 系统一起运行的命令。此外，如果遇到问题，$HADOOP_HOME/bin/hadoop fs -help命令名将显示有关操作的简短使用摘要。 所有操作的表如下所示。参数使用以下约定: 12345"&lt;path&gt;" means any file or directory name. "&lt;path&gt;..." means one or more file or directory names. "&lt;file&gt;" means any filename. "&lt;src&gt;" and "&lt;dest&gt;" are path names in a directed operation. "&lt;localSrc&gt;" and "&lt;localDest&gt;" are paths as above, but on the local file system. 所有其他文件和路径名都引用HDFS中的对象。 序号 命令 1. ls 列出路径指定的目录的内容，显示每个条目的名称、权限、所有者、大小和修改日期。 2. lsr 行为类似于-ls，但是递归地显示path的所有子目录中的条目。 3. du 显示与路径匹配的所有文件的磁盘使用情况(以字节为单位);使用完整的HDFS协议前缀报告文件名。 4. dus 与-du类似，但打印路径中所有文件/目录的磁盘使用情况摘要。 5. mv 将src指示的文件或目录移动到HDFS内的dest。 6. cp 在HDFS中将src标识的文件或目录复制到dest。 7. rm 删除路径标识的文件或空目录。 8. rmr 删除路径标识的文件或目录。递归地删除任何子条目 (i.e., files or subdirectories of path). 9. put 将由localSrc标识的本地文件系统中的文件或目录复制到DFS中的dest。 10. copyFromLocal -put相同 11. moveFromLocal 将由localSrc标识的本地文件系统中的文件或目录复制到HDFS中的dest，然后成功删除本地副本。 12. get [-crc] 将src标识的HDFS中的文件或目录复制到localDest标识的本地文件系统路径。 13. getmerge 检索与HDFS中的路径src匹配的所有文件，并将它们复制到localDest标识的本地文件系统中合并的单个文件。 14. cat 在标准输出上显示文件名的内容。 15. copyToLocal 与 -get相同 16. moveToLocal 类似于-get，但成功时删除HDFS副本。 17. mkdir 在HDFS中创建一个名为path的目录。在路径中创建缺少的任何父目录(e.g., mkdir -p in Linux). 18. setrep [-R][-w] rep 为通过路径到rep标识的文件设置目标复制因子(随着时间的推移，实际复制因子将向目标移动) 19. touchz 在包含当前时间作为时间戳的路径上创建一个文件。如果文件在路径上已经存在，则失败，除非文件的大小已经为0。 20. test -[ezd] 如果路径存在，返回1;长度为零;或者是目录，或者是0。 21. stat [format] 打印关于路径的信息。Format是一个字符串，它接受块大小(%b)、文件名(%n)、块大小(%o)、复制(%r)和修改日期(%y， %y)。 22. tail [-f] 显示stdout上文件的最后1KB。 23. chmod [-R] mode,mode,… …与一个或多个相关联的文件权限对象的更改了路径….使用r模式递归执行更改是3位八进制模式，或{augo}+/-{rwxX}。假设没有指定范围且不应用umask。 24. chown [-R][owner][:[group]] …集拥有用户和/或组的文件或目录被路径….如果指定-R，则递归设置所有者。 25. chgrp [-R] group …设置拥有小组确认的文件或目录路径….如果指定-R，则递归地设置组。 26. help 返回上述命令之一的使用信息。你必须省略cmd中的“-”字符。 原文链接：https://www.tutorialspoint.com/…]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：HDFS操作]]></title>
    <url>%2Fhadoop-hdfs-operations%2F</url>
    <content type="text"><![CDATA[启动HDFS首先，您必须格式化配置的HDFS文件系统，打开namenode (HDFS服务器)，并执行以下命令。 1$ hadoop namenode -format 格式化HDFS之后，启动分布式文件系统。下面的命令将启动namenode以及数据节点作为集群。 1$ start-dfs.sh 列出HDFS中的文件在服务器中加载信息后，我们可以使用“ls”查找目录中的文件列表、文件状态。下面给出了可以作为参数传递到目录或文件名的ls语法。 1$ $HADOOP_HOME/bin/hadoop fs -ls &lt;args&gt; 将数据插入HDFS假设我们在本地系统中一个名为file.txt的文件，应该保存在hdfs文件系统中。按照下面给出的步骤在Hadoop文件系统中插入所需的文件。 Step 1您必须创建一个输入目录。 1$ $HADOOP_HOME/bin/hadoop fs -mkdir /user/input Step 2使用put命令将数据文件从本地系统传输和存储到Hadoop文件系统。 1$ $HADOOP_HOME/bin/hadoop fs -put /home/file.txt /user/input Step 3您可以使用ls命令验证该文件。 1$ $HADOOP_HOME/bin/hadoop fs -ls /user/input 从HDFS检索数据假设HDFS中有一个名为outfile的文件。下面是一个从Hadoop文件系统检索所需文件的简单演示。 Step 1首先，使用cat命令查看来自HDFS的数据。 1$ $HADOOP_HOME/bin/hadoop fs -cat /user/output/outfile Step 2使用get命令将文件从HDFS获取到本地文件系统。 1$ $HADOOP_HOME/bin/hadoop fs -get /user/output/ /home/hadoop_tp/ 关闭HDFS可以使用以下命令关闭HDFS 1$ stop-dfs.sh 原文链接：https://www.tutorialspoint.com/…]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：HDFS概述]]></title>
    <url>%2Fhadoop-hdfs-overview%2F</url>
    <content type="text"><![CDATA[Hadoop文件系统采用分布式文件系统设计开发。它在普通硬件上运行。与其他分布式系统不同，HDFS具有很高的容错性，并且使用低成本的硬件进行设计。 HDFS存储大量数据并提供更容易的访问。为了存储如此巨大的数据，文件被存储在多台机器上。这些文件以冗余的方式存储，以便在发生故障时将系统从可能的数据损失中拯救出来。HDFS还使应用程序可用于并行处理。 HDFS的特点 适用于分布式存储和处理。 Hadoop提供了一个与HDFS交互的命令接口。 namenode和datanode的内置服务器可以方便地检查集群的状态。 对文件系统数据的流访问。 HDFS提供文件权限和身份验证。 HDFS架构下面给出Hadoop文件系统的架构。 HDFS遵循主从体系结构，它具有以下元素。 Namenodenamenode是包含GNU/Linux操作系统和namenode软件的商品硬件。它是一种可以在普通硬件上运行的软件。具有namenode的系统充当主服务器，它执行以下任务: 管理文件系统名称空间。 管理客户对文件的访问。 它还执行文件系统操作，如重命名、关闭和打开文件和目录。 Datanodedatanode是一种具有GNU/Linux操作系统和datanode软件的普通硬件。对于集群中的每个节点(商品硬件/系统)，都将有一个datanode。这些节点管理其系统的数据存储。 数据节点根据客户端请求在文件系统上执行读写操作。 它们还根据namenode的指令执行块创建、删除和复制等操作。 Block用户数据一般存储在HDFS文件中。文件系统中的文件将被分成一个或多个段和/或存储在单个数据节点中。这些文件段称为块。换句话说，HDFS可以读写的最小数据量称为块。默认块大小为64MB，但是可以根据需要在HDFS配置中进行更改而增加。 HDFS的目标 故障检测与恢复: 由于HDFS包含大量的商用硬件，部件故障频繁。因此，HDFS应该具有快速、自动的故障检测和恢复机制。 海量的数据集: HDFS每个集群应该有数百个节点，以管理拥有庞大数据集的应用程序。 硬件在数据上 当计算发生在数据附近时，可以有效地完成请求的任务。特别是在涉及到大量数据集的情况下，它会减少网络流量并增加吞吐量。 原文链接：https://www.tutorialspoint.com/…]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：安装与环境设置]]></title>
    <url>%2Fhadoop-enviornment-setup%2F</url>
    <content type="text"><![CDATA[Hadoop支持Windows, Mac, Linux, 但推荐是用Linux环境学习Hadoop。因此，我们必须安装一个Linux操作系统来设置Hadoop环境。如果您的操作系统不是Linux，那么您可以在其中安装一个Virtualbox软件，并在Virtualbox中包含Linux。 安装前配置在将Hadoop安装到Linux环境之前，我们需要使用ssh(Secure Shell)来设置Linux。按照下面给出的步骤设置Linux环境。 创建用户首先，建议为Hadoop创建一个单独的用户，以便将Hadoop文件系统与Unix文件系统隔离开来。按照以下步骤创建用户: 使用“su”命令打开根目录。 使用“useradd username”命令从根帐户创建一个用户。 现在您可以使用“su用户名”命令打开一个现有的用户帐户。 打开Linux终端，输入以下命令来创建用户。 123456$ su password: # useradd hadoop # passwd hadoop New passwd: Retype new passwd SSH设置和密钥生成在集群上执行操作需要设置SSH，例如启动、停止、分布式守护进程shell操作。为了验证Hadoop的不同用户，需要为Hadoop用户提供公钥/私钥对，并与不同的用户共享。 下面的命令用于使用SSH生成键值对。从id_rsa.pub复制公钥到authorized_keys，并分别向所有者提供authorized_keys文件的读写权限。 123$ ssh-keygen -t rsa $ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys 安装JavaHadoop必须安装Java。首先，您应该使用“java -version”命令验证系统中是否存在java。查看java版本命令的语法如下所示。 1$ java -version 如果一切正常，它将给出以下输出。 123java version "1.7.0_71" Java(TM) SE Runtime Environment (build 1.7.0_71-b13) Java HotSpot(TM) Client VM (build 25.0-b02, mixed mode) 如果您的系统中没有安装java，那么按照下面给出的步骤安装java。 Step 1下载java (JDK &lt;最新版本&gt; - X64.tar.gz)，请访问以下链接http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads1880260.html。 然后jdk-7u71-linux-x64.tar.gz将被下载到您的系统中。 Step 2通常您会在下载文件夹中找到下载的java文件。验证它并提取jdk-7u71-linux-x64.gz文件使用以下命令。 123456$ cd Downloads/ $ ls jdk-7u71-linux-x64.gz $ tar zxf jdk-7u71-linux-x64.gz $ ls jdk1.7.0_71 jdk-7u71-linux-x64.gz Step 3要让所有用户都可以使用java，必须将其移动到“/usr/local/”位置。打开root，并键入以下命令。 1234$ su password: # mv jdk1.7.0_71 /usr/local/ # exit Step 4要设置PATH和JAVA_HOME变量，请向~/.bashrc添加以下命令。 12export JAVA_HOME=/usr/local/jdk1.7.0_71 export PATH=$PATH:$JAVA_HOME/bin 现在令所有更改生效。 1$ source ~/.bashrc Step 5使用以下命令配置java替代方案: 1234567891011# alternatives --install /usr/bin/java java usr/local/java/bin/java 2# alternatives --install /usr/bin/javac javac usr/local/java/bin/javac 2# alternatives --install /usr/bin/jar jar usr/local/java/bin/jar 2# alternatives --set java usr/local/java/bin/java# alternatives --set javac usr/local/java/bin/javac# alternatives --set jar usr/local/java/bin/jar 现在验证来自终端的java版本命令，使用之前提到的查看Java版本命令。 下载Hadoop使用以下命令从Apache software foundation下载并提取Hadoop 2.4.1（根据需要选择版本）。 12345678$ su password: # cd /usr/local # wget http://apache.claz.org/hadoop/common/hadoop-2.4.1/ hadoop-2.4.1.tar.gz # tar xzf hadoop-2.4.1.tar.gz # mv hadoop-2.4.1/* to hadoop/ # exit Hadoop的操作模式下载Hadoop后，可以使用以下三种支持模式之一来操作Hadoop集群: 本地/独立模式: 在您的系统中下载Hadoop之后，默认情况下，它是在独立模式下配置的，可以作为单个java进程运行。 伪分布模式: 这是一个模拟在单机上的分布式。每个Hadoop守护进程(如hdfs、yarn、MapReduce等)将作为一个单独的java进程运行。这种模式对开发很有用。 全分布模式: 这种模式是完全分布式的，集群中至少有两台或多台机器。我们将在接下来的章节中详细介绍这种模式。 以独立模式安装Hadoop这里我们将讨论Hadoop 2.4.1在独立模式下的安装。 没有运行守护进程，所有东西都在单个JVM中运行。独立模式适合在开发过程中运行MapReduce程序，因为它易于测试和调试。 Hadoop配置您可以通过向~/.bashrc文件添加以下命令来设置Hadoop环境变量。 1export HADOOP_HOME=/usr/local/hadoop 在继续之前，您需要确保Hadoop工作正常。使用以下命令查看hadoop是否安装成功: 1$ hadoop version 如果您的设置一切正常，那么您应该会看到以下结果: 12345Hadoop 2.4.1 Subversion https://svn.apache.org/repos/asf/hadoop/common -r 1529768 Compiled by hortonmu on 2013-10-07T06:28Z Compiled with protoc 2.5.0From source with checksum 79e53ce7994d1628b240f09af91e1af4 这意味着Hadoop的独立模式设置工作正常。默认情况下，Hadoop被配置为在一台机器上以非分布式模式运行。 例子让我们查看Hadoop的一个简单示例。Hadoop安装提供了以下示例MapReduce jar文件，它提供了MapReduce的基本功能，可以用于计算，如Pi值、给定文件列表中的字数等。 1$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar 让我们有一个输入目录，在这里我们将推动一些文件，我们的要求是计数总字数在这些文件。要计算单词总数，我们不需要编写MapReduce，只要.jar文件包含单词计数的实现即可。您可以使用相同的.jar文件尝试其他示例;只要发出以下命令，检查hadoop- MapReduce -example -2.2.0.jar文件支持的MapReduce功能程序。 1$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduceexamples-2.2.0.jar Step 1在输入目录中创建临时内容文件。您可以在希望工作的任何地方创建此输入目录。 123$ mkdir input $ cp $HADOOP_HOME/*.txt input $ ls -l input 它将在您的输入目录中提供以下文件: 1234total 24 -rw-r--r-- 1 root root 15164 Feb 21 10:14 LICENSE.txt -rw-r--r-- 1 root root 101 Feb 21 10:14 NOTICE.txt-rw-r--r-- 1 root root 1366 Feb 21 10:14 README.txt 这些文件是从Hadoop安装主目录复制的。对于您的实验，您可以拥有不同的大文件集。 Step 2步骤2将进行所需的处理，并将输出保存在output/part-r00000文件中，您可以使用以下命令进行检查: 1$cat output/* 它将列出输入目录中所有文件中可用的所有单词及其总数。 123456789101112131415161718192021222324252627"AS 4 "Contribution" 1 "Contributor" 1 "Derivative 1"Legal 1"License" 1"License"); 1 "Licensor" 1"NOTICE” 1 "Not 1 "Object" 1 "Source” 1 "Work” 1 "You" 1 "Your") 1 "[]" 1 "control" 1 "printed 1 "submitted" 1 (50%) 1 (BIS), 1 (C) 1 (Don't) 1 (ECCN) 1 (INCLUDING 2 (INCLUDING, 2 ............. 以伪分布式模式安装Hadoop按照下面给出的步骤以伪分布式模式安装Hadoop 2.4.1。 Step 1: 环境配置您可以通过向~/.bashrc文件添加以下命令来设置Hadoop环境变量。 12345678export HADOOP_HOME=/usr/local/hadoop export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin export HADOOP_INSTALL=$HADOOP_HOME 现在将所有更改应用到当前运行的系统中。 1$ source ~/.bashrc Step 2: Hadoop配置您可以在“$HADOOP_HOME/etc/hadoop”位置找到所有Hadoop配置文件。需要根据Hadoop基础设施对这些配置文件进行更改。 1$ cd $HADOOP_HOME/etc/hadoop 为了用java开发Hadoop程序，必须在Hadoop -env.sh文件中替换JAVA_HOME值 1export JAVA_HOME=/usr/local/jdk1.7.0_71 以下是配置Hadoop需要编辑的文件列表。 core-site.xmlcore-site.xml文件包含一些信息，例如Hadoop实例使用的端口号、为文件系统分配的内存、存储数据的内存限制以及读/写缓冲区的大小。 打开core-site.xml并在, 之间添加以下属性 12345678&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xmlhdfs-site.xml文件包含数据备份数量值、namenode路径和本地文件系统的datanode路径等信息。它意味着您希望存储Hadoop基础结构的地方。 123456dfs.replication (data replication value) = 1 (In the below given path /hadoop/ is the user name. hadoopinfra/hdfs/namenode is the directory created by hdfs file system.) namenode path = //home/hadoop/hadoopinfra/hdfs/namenode (hadoopinfra/hdfs/datanode is the directory created by hdfs file system.) datanode path = //home/hadoop/hadoopinfra/hdfs/datanode 打开该文件，并在该文件中 标签之间添加以下属性 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;file:///home/hadoop/hadoopinfra/hdfs/namenode &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;file:///home/hadoop/hadoopinfra/hdfs/datanode &lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 注意： 在上面的文件中，所有属性值都是用户定义的，您可以根据Hadoop基础结构进行更改。 yarn-site.xml这个文件用于将yarn配置到Hadoop中。打开yarn-site.xml文件并在该文件中的, 标签之间添加 12345678&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml这个文件用于指定我们正在使用的MapReduce框架。默认情况下，Hadoop包含一个yarn-site.xml模板。首先，需要从mapred-site.xml.template复制文件到mapred-site.xml文件。使用以下命令 1$ cp mapred-site.xml.template mapred-site.xml 打开mapred-site.xml文件。并在该文件中的, 标签之间添加以下属性。 12345678&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration 验证Hadoop安装以下步骤用于验证Hadoop的安装。 Step 1: Name Node使用“hdfs namenode -format”命令设置namenode，如下所示。 12$ cd ~ $ hdfs namenode -format 预期结果如下。 123456789101112131415161710/24/14 21:30:55 INFO namenode.NameNode: STARTUP_MSG: /************************************************************ STARTUP_MSG: Starting NameNode STARTUP_MSG: host = localhost/192.168.1.11 STARTUP_MSG: args = [-format] STARTUP_MSG: version = 2.4.1 ......10/24/14 21:30:56 INFO common.Storage: Storage directory /home/hadoop/hadoopinfra/hdfs/namenode has been successfully formatted. 10/24/14 21:30:56 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0 10/24/14 21:30:56 INFO util.ExitUtil: Exiting with status 0 10/24/14 21:30:56 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at localhost/192.168.1.11 ************************************************************/ Step 2: 验证Hadoop dfs下面的命令用于启动dfs。执行此命令将启动Hadoop文件系统。 1$ start-dfs.sh 预期输出如下: 123456710/24/14 21:37:56 Starting namenodes on [localhost] localhost: starting namenode, logging to /home/hadoop/hadoop2.4.1/logs/hadoop-hadoop-namenode-localhost.out localhost: starting datanode, logging to /home/hadoop/hadoop2.4.1/logs/hadoop-hadoop-datanode-localhost.out Starting secondary namenodes [0.0.0.0] Step 3: 验证Yarn脚本下面的命令用于启动纱线脚本。执行此命令将启动纱线守护进程。 1$ start-yarn.sh 预期输出如下: 12345starting yarn daemons starting resourcemanager, logging to /home/hadoop/hadoop2.4.1/logs/yarn-hadoop-resourcemanager-localhost.out localhost: starting nodemanager, logging to /home/hadoop/hadoop2.4.1/logs/yarn-hadoop-nodemanager-localhost.out Step 4: 在浏览器上访问Hadoop访问Hadoop的默认端口号是50070。使用以下url在浏览器上获取Hadoop服务。 ｛% qnimg hadoop_on_browser.jpg title:hadoop_on_browser alt: hadoop_on_browser%｝ Step 5: 验证集群中的所有应用程序访问集群所有应用程序的默认端口号是8088。使用以下url访问此服务。 原文链接：https://www.tutorialspoint.com/…]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：Hadoop介绍]]></title>
    <url>%2Fhadoop-introduction%2F</url>
    <content type="text"><![CDATA[Hadoop是一个用java编写的Apache开源框架，它允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序工作在一个跨计算机集群提供分布式存储和计算的环境中。Hadoop被设计成从单个服务器扩展到数千台机器，每台机器都提供本地计算和存储。 Hadoop架构Hadoop框架包括以下四个模块: Hadoop Common: 这是其他Hadoop模块依赖的Java库和工具。这些库提供文件系统和操作系统级别的抽象，并包含启动Hadoop所需的Java文件和脚本。 Hadoop YARN: 这是一个用于作业调度和集群资源管理的框架。 Hadoop分布式文件系统(HDFS™): 一种分布式文件系统，提供对应用程序数据的高吞吐量访问。 MapReduce: 这是一个基于YARN的大型数据集并行处理系统。 我们可以使用下面的图来描述Hadoop框架中可用的这四个组件。 自2012年以来，“Hadoop”一词通常不仅指上述基本模块，还指可以安装在Hadoop之上或与Hadoop并行的附加软件包集合，如Apache Pig、Apache Hive、Apache HBase、Apache Spark等。 MapReduceHadoop MapReduce是一个易于编写应用程序的软件框架，这些应用程序以可靠、容错的方式并行处理大集群(数千个节点)上的海量数据。 MapReduce这个词实际上是指Hadoop程序执行的两个不同的任务: Map任务: 这是第一个任务，它接受输入数据并将其转换为一组数据集合，其中单个元素被分解为元组(键/值对)。 Reduce任务: 该任务将map任务的输出作为输入，并将这些数据元组组合成较小的元组集合。reduce任务总是在map任务之后执行。 通常输入和输出都存储在文件系统中。该框架负责调度任务、监视任务并重新执行失败的任务。 MapReduce框架由一个master JobTracker和每个集群节点的一个slave TaskTracker组成。master 负责资源管理，跟踪资源消耗/可用性，调度从服务器上的作业组件任务，监视它们并重新执行失败的任务。slave 按照主任务的指示执行任务，并定期向主任务提供任务状态信息。 JobTracker是Hadoop MapReduce服务的一个单点故障，这意味着如果JobTracker宕机，所有正在运行的作业都会停止。 Hadoop Distributed File SystemHadoop可以直接与任何可挂载的分布式文件系统(如本地FS、HFTP FS、S3 FS等)一起工作，但是Hadoop最常用的文件系统是Hadoop分布式文件系统(HDFS)。 Hadoop分布式文件系统(HDFS)是基于谷歌文件系统(GFS)的，它提供了一个分布式文件系统，可以在小型计算机的大型集群(数千台计算机)上以可靠、容错的方式运行。 HDFS使用主/从（master/slave）架构，其中主架构由管理文件系统元数据的单个名称节点NameNode和存储实际数据的一个或多个从数据节点DataNodes组成。 HDFS名称空间中的文件被分成几个块，这些块存储在一组数据节点DataNodes中。NameNode确定块到DataNodes的映射。DataNodes 负责文件系统的读写操作。它们还根据NameNode给出的指令负责块的创建、删除和复制。 HDFS提供了与任何其他文件系统一样的shell，可以使用命令列表与文件系统交互。这些shell命令将在单独的一章中介绍，并提供适当的示例。 Hadoop是如何工作的Stage 1用户/应用程序可以通过以下项目将作业提交给Hadoop (Hadoop作业客户端)进行所需的处理: 在分布式文件系统中输入和输出文件的位置。 java类以jar文件的形式包含map和reduce函数的实现。 通过设置特定于作业的不同参数进行作业配置。 Stage 2然后Hadoop客户端将作业(jar/可执行文件等)和配置提交给JobTracker, JobTracker负责将软件/配置分发给slaves服务器，调度和监视任务，向客户端提供状态和诊断信息。 Stage 3不同节点上的TaskTrackers 按照MapReduce实现执行任务，reduce函数的输出存储在文件系统上的输出文件中。 Hadoop的优点 Hadoop框架允许用户快速编写和测试分布式系统。它是高效的，它自动分配数据并跨机器工作，反过来利用CPU核心的底层并行性。 Hadoop不依赖硬件来提供容错和高可用性(FTHA)，Hadoop库本身被设计来检测和处理应用层的故障。 可以动态地从集群中添加或删除服务器，Hadoop可以不间断地继续运行。 Hadoop的另一个巨大优势是，它不仅是开源的，而且可以在所有平台上兼容，因为它是基于Java的。 原文链接：https://www.tutorialspoint.com/hadoop/hadoop_introduction.htm]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：大数据解决方案]]></title>
    <url>%2Fhadoop-big-data-solutions%2F</url>
    <content type="text"><![CDATA[传统方法在这种方式下，企业将使用一台计算机来存储和处理数据，处理所需的数据，并将其呈现给用户以供分析之用。在这里，数据将存储在RDBMS，如：Oracle数据库、MS SQL Server或DB2以及可以与数据库交互的复杂软件。 局限性这种方法在标准数据库服务器可以容纳的数据量更少或处理数据的处理器的极限的情况下工作得很好。但是当涉及到处理大量数据时，通过传统的数据库服务器来处理这些数据确实是一项非常繁琐的任务。 Google的解决方案谷歌使用MapReduce算法解决了这个问题。该算法将任务划分为多个小部分，并将这些小部分分配给通过网络连接的多台计算机，最后收集结果形成最终的结果数据集。 上图显示了各种各样的商品硬件，这些硬件可以是单CPU机器，也可以是容量更大的服务器。 HadoopDoug Cutting、Mike Cafarella和团队采用了谷歌提供的解决方案，并在2005年启动了一个名为HADOOP 的开源项目，Doug以他儿子的玩具大象命名了这个项目。现在Apache Hadoop是Apache软件基金会的注册商标。 Hadoop使用MapReduce算法运行应用程序，数据在不同的CPU节点上并行处理。简而言之，Hadoop框架有足够的能力开发能够在计算机集群上运行的应用程序，并且能够对大量数据执行完整的统计分析。 原文链接：https://www.tutorialspoint.com/hadoop/hadoop_big_data_solutions.html]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程：大数据概述]]></title>
    <url>%2Fhadoop-big-data-overview%2F</url>
    <content type="text"><![CDATA[由于新技术、新设备和社交网站等通信手段的出现，人类产生的数据量每年都在迅速增长。2003年之前的所有数据量总和是50亿G。如果你把数据以磁盘的形式堆起来，它可能会填满整个足球场。到了2011年，每两天就能创造同样的数量，2013年每十分钟创造同样的数量。这一比例仍在大幅增长。虽然所有这些信息都是有意义的，并且在处理时很有用，但它却被忽略了。 世界上90%的数据是在过去几年生成的。 什么是大数据大数据其实就是海量的数据，它是不能用传统计算技术处理的海量数据集的集合。大数据不仅仅是一种数据，它已经成为一门完整的学科，涉及到各种工具、技术和框架。 大数据的来源大数据涉及不同设备和应用产生的数据。以下是大数据保护下的一些领域。 黑匣子数据：是直升机、飞机、喷气机等的组成部分，它可以捕捉机组人员的声音、麦克风和耳机的录音，以及飞机的性能信息。 社交媒体数据：Facebook和Twitter等社交媒体包含全球数百万人发布的信息和观点。 股票交易数据：股票交易数据包含客户对不同公司股票的“买入”和“卖出”决策的信息。 电网数据：电网数据包含特定节点相对于基站所消耗的信息。 搜索引擎数据：搜索引擎从不同的数据库检索大量数据。 ｛% qnimg big_data.jpg title: big data alt: Big Data %｝ 因此，大数据包括大容量、高速度和可扩展的各种数据。其中的数据有三种类型。 结构化数据: 关系型数据库。 半结构化数据: XML数据。 非结构化数据: Word、PDF、文本、媒体日志。 大数据带来的好处大数据对我们的生活至关重要，它正在成为现代世界最重要的技术之一。下面是我们大家都知道的几个好处: 利用Facebook等社交网络中保存的信息，营销机构正在了解他们的活动、促销和其他广告媒介的效果。 利用社交媒体上的信息，如消费者的喜好和对产品满意度，产品公司和零售组织正在优化他们的生产。 利用患者既往病史资料，医院提供更好、更快的服务。 大数据技术大数据技术在提供更准确的分析方面有很重要的作用，这可以提供更具体的决策，从而提高运营效率，降低成本，降低业务风险。 想要利用大数据的力量，你需要一个能够实时管理和处理海量结构化和非结构化数据、能够保护数据隐私和安全的基础设施。 市场上有来自亚马逊、IBM、微软等不同厂商的各种处理大数据的技术。在研究处理大数据的技术时，我们考察了以下两类技术: 大数据操作这包括像MongoDB这样的系统，它提供了实时、交互式工作负载的操作能力，数据主要是在这些工作负载中捕获和存储的。 NoSQL大数据系统旨在利用过去十年出现的新的云计算架构，以低成本和高效率运行大量计算。这使得操作大数据工作负载更容易管理、更便宜、实现更快。 一些NoSQL系统可以提供基于实时数据的模式和趋势的洞察，而只需最少的编码，并且不需要数据科学家和额外的基础设施。 大数据分析这包括大规模并行处理(Massively Parallel Processing)数据库系统和MapReduce系统，它们提供可追溯和复杂的分析能力，可能涉及大部分或所有数据的分析。 MapReduce提供了一种新的数据分析方法，它是SQL提供的功能的补充，并且基于MapReduce的系统可以从单个服务器扩展到数千台高端和低端机器。 这两类技术是互补的，经常一起部署。 操作VS分析 操作 分析 延迟 1 ms - 100 ms 1 min - 100 min 并发 1000 - 100,000 1 - 10 访问模式 Writes and Reads Reads 查询 Selective Unselective 数据使用范围 Operational Retrospective End User Customer Data Scientist 技术 NoSQL MapReduce, MPP Database 大数据的挑战与大数据相关的主要挑战如下: 数据采集 管理 存储 搜索 共享 传输 分析 展示 为了完成上述挑战，通常需要企业服务器的帮助。 原文链接：https://www.tutorialspoint.com/hadoop/hadoop_big_data_overview.htm]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop教程]]></title>
    <url>%2Fhadoop-tutorial-home%2F</url>
    <content type="text"><![CDATA[Hadoop教程目录： 大数据概述 大数据解决方案 Hadoop介绍 Hadoop安装与环境设置 HDFS概述 HDFS操作 Hadoop命令手册 MapReduce Hadoop流 Hadoop全分布式安装 Hadoop以Apache 2.0许可协议发布的开源软件框架。用户可以通过简单的程序模型在分布式环境集群中存储和处理大数据。 它旨在从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和存储。 本教程简要介绍了大数据，MapReduce算法和Hadoop分布式文件系统（HDFS）。 Audience 本教程是为希望学习使用Hadoop框架进行大数据分析的基础知识并成为Hadoop开发人员的专业人士准备的。软件专业人员、分析专业人员和ETL开发人员是本课程的主要受益者。 Prerequisites 在开始学习本教程之前，我们假设您已经接触过核心Java、数据库概念和任何Linux操作系统风格。 原文链接：https://www.tutorialspoint.com/hadoop/index.htm]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的String、StringBuffer、StringBuilder有什么区别？]]></title>
    <url>%2Fstring-stringbuffer-and-stringbuilder-in-java%2F</url>
    <content type="text"><![CDATA[String、StringBuffer、StringBuilder有什么区别？这个问题在面试中经常碰到，今天主要讲解一下如何理解Java中的String、StringBuffer、StringBuilder。 典型回答String 是 Java 语言非常基础和重要的类，提供了构造和管理字符串的各种基本逻辑。它是典型的不可变类（ Immutable ），被声明成为 final class，所有属性也都是 final 的。也由于它的不可变性，类似拼接、裁剪字符串等动作，都会产生新的 String 对象。由于字符串操作的普遍性，所以相关操作的效率往往对应用性能有明显影响。 StringBuffer 是为解决上面提到拼接产生太多中间对象的问题而提供的一个类，我们可以用 append 或者 add 方法，把字符串添加到已有序列的末尾或者指定位置。StringBuffer 本质是一个线程安全的可修改字符序列，它保证了线程安全，也随之带来了额外的性能开销，所以除非有线程安全的需要，不然还是推荐使用它的后继者，也就是 StringBuilder。 StringBuilder 是 Java 1.5 中新增的，在能力上和 StringBuffer 没有本质区别，但是它去掉了线程安全的部分，有效减小了开销，是绝大部分情况下进行字符串拼接的首选。 考点分析几乎所有的应用开发都离不开操作字符串，理解字符串的设计和实现以及相关工具如拼接类的使用，对写出高质量代码是非常有帮助的。关于这个问题，前面的回答是一个通常的概要性回答，至少你要知道 String 是 Immutable 的，字符串操作不当可能会产生大量临时字符串，以及线程安全方面的区别。 如果继续深入，面试官可以从各种不同的角度考察，比如可以： 通过 String 和相关类，考察基本的线程安全设计与实现，各种基础编程实践。 考察 JVM 对象缓存机制的理解以及如何良好地使用。 考察 JVM 优化 Java 代码的一些技巧。 String 相关类的演进，比如 Java 9 中实现的巨大变化。 … 知识扩展字符串设计和实现考量String 是 Immutable 类的典型实现，原生的保证了基础线程安全，因为你无法对它内部数据进行任何修改，这种便利甚至体现在拷贝构造函数中，由于不可变，Immutable 对象在拷贝时不需要额外复制数据。 我们再来看看 StringBuffer 实现的一些细节，它的线程安全是通过把各种修改数据的方法都加上 synchronized 关键字实现的，非常直白。其实，这种简单粗暴的实现方式，非常适合我们常见的线程安全类实现，不必纠结于 synchronized 性能之类的，有人说“过早优化是万恶之源”，考虑可靠性、正确性和代码可读性才是大多数应用开发最重要的因素。 为了实现修改字符序列的目的，StringBuffer 和 StringBuilder 底层都是利用可修改的（char，JDK 9 以后是 byte）数组，二者都继承了 AbstractStringBuilder，里面包含了基本操作，区别仅在于最终的方法是否加了 synchronized。 在具体的代码书写中，应该如何选择呢？ 在没有线程安全问题的情况下，全部拼接操作是应该都用 StringBuilder 实现吗？毕竟这样书写的代码，还是要多敲很多字的，可读性也不理想，下面的对比非常明显。 12345String strByBuilder = newStringBuilder().append("aa").append("bb").append("cc").append ("dd").toString(); String strByConcat = "aa" + "bb" + "cc" + "dd"; 其实，在通常情况下，没有必要过于担心，要相信 Java 还是非常智能的。 我们来做个实验，把下面一段代码，利用不同版本的 JDK 编译，然后再反编译，例如： 123456public class StringConcat &#123; public static void main(String[] args) &#123; String myStr = "aa" + "bb" + "cc" + "dd"; System.out.println("My String:" + myStr); &#125; &#125; 先编译再反编译 12$&#123;JAVA_HOME&#125;/bin/javac StringConcat.java$&#123;JAVA_HOME&#125;/bin/javap -v StringConcat.class JDK 8 的输出片段是： 12345678910111213 0: ldc #2 // String hellojava! 2: astore_1 3: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 6: new #4 // class java/lang/StringBuilder 9: dup10: invokespecial #5 // Method java/lang/StringBuilder."&lt;init&gt;":()V13: ldc #6 // String Concat String:15: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;18: aload_119: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;22: invokevirtual #8 // Method java/lang/StringBuilder.toString:()Ljava/lang/String;25: invokevirtual #9 // Method java/io/PrintStream.println:(Ljava/lang/String;)V28: return 你可以看到，在 JDK 8 中，字符串拼接操作会自动被 javac 转换为 StringBuilder 操作，而在 JDK 9 里面则是因为 Java 9 为了更加统一字符串操作优化，提供了 StringConcatFactory，作为一个统一的入口。javac 自动生成的代码，虽然未必是最优化的，但普通场景也足够了，你可以酌情选择。 参考自极客时间：Java核心技术36讲 感谢原作者：杨晓峰老师]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>string</tag>
        <tag>stringbuffer</tag>
        <tag>stringbuilder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 344.反转字符串]]></title>
    <url>%2Fleetcode-344-reverse-string%2F</url>
    <content type="text"><![CDATA[题目描述 编写一个函数，其作用是将输入的字符串反转过来。 示例 1: 123&gt; 输入: &quot;hello&quot;&gt; 输出: &quot;olleh&quot;&gt; 示例 2: 123&gt; 输入: &quot;A man, a plan, a canal: Panama&quot;&gt; 输出: &quot;amanaP :lanac a ,nalp a ,nam A&quot;&gt; 思路 使用StringBuider或StringBuffer修改字符序列，关于StringBuilder和Stringbuffer的区别可以参考我的零一篇文章Java中的String、StringBuffer、StringBuilder有什么区别？ 将字符串转化为字符串数组，对换首尾字符位置即可。 Java AC 使用StringBuilder 123456class Solution &#123; public String reverseString(String s) &#123; String res = new StringBuilder(s).reverse().toString(); return res; &#125;&#125; 转换为数组 123456789101112131415class Solution &#123; public String reverseString(String s) &#123; char[] arr = s.toCharArray(); int i = 0, j = arr.length - 1; while (i &lt; j) &#123; char tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; i++; j--; &#125; String str = String.valueOf(arr); return str; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle比赛top2%特征探索之featexp]]></title>
    <url>%2Fsecret-to-be-in-top-2-of-kaggle%2F</url>
    <content type="text"><![CDATA[在数值数据上构建任意监督学习模型的一个重要方面是理解特征。查看模型的部分依赖图可帮助理解任意特征对模型输出的影响。 但是，部分依赖图存在一个问题，即它们是使用训练好的模型创建的。如果我们可以从训练数据中直接创建部分依赖图，那么它将帮助我们更好地理解底层数据。事实上，它能够帮助你做好以下事情： 特征理解 识别带噪声的特征 (the most interesting part!) 特征工程 特征重要性 特征 debug 泄露检测和理解 模型监控 为了使其更加易于使用，作者将这些技术封装进一个 Python 包 featexp 中，本文将介绍如何使用它进行特征探索。本文使用的是 Kaggle Home Credit Default Risk 竞赛的应用数据集。该竞赛的任务是使用给定数据预测违约者。 featexp：https://github.com/abhayspawar/… 1. 特征理解 如果依赖变量（目标）是二元的，则散点图无效，因为所有点要么是 0 要么是 1。对于连续目标来说，数据点太多会造成难以理解目标 vs 特征趋势。featexp 创建了更好的图，可帮助解决该问题。我们来试一下！ 12345from featexp import get_univariate_plots# Plots drawn for all features if nothing is passed in feature_list parameter.get_univariate_plots(data=data_train, target_col='target', features_list=['DAYS_BIRTH'], bins=10) featexp 为数值特征创建了同等人口数量的 bin（x 轴），然后计算每个 bin 的目标平均值，再绘制出来（如上图左）。在我们的案例中，目标平均值是违约率。该图告诉我们年龄越大的客户违约率越低。这些图帮助我们理解特征表达的意义，及其对模型的影响。右图显示了每个 bin 中客户的数量。 2. 识别带噪声的特征 带噪声的特征导致过拟合，识别它们并非易事。在 featexp 中，你可以输出一个测试集（或者验证集），对比训练／测试集中的特征趋势来确定带噪声的特征。 1get_univariate_plots(data=data_train, target_col='target', data_test=data_test, features_list=['DAYS_EMPLOYED']) featexp 计算两个指标（如上图所示），来帮助测量噪声： 趋势相关度（见测试图）：如果某个特征未体现目标在训练集和测试集中的同样趋势，它会导致过拟合，因为模型会学习一些在测试数据中并不使用的东西。趋势相关度有助于理解训练／测试趋势的相似度，如何利用训练和测试集的 bin 的平均目标值来计算趋势相关度。上图中的特征相关度为 99%，几乎没有噪声。 趋势变化：趋势方向中突然和重复的变化可能表明有噪声。但是，此类趋势变化也会在 bin 的人口数量与其它特征不同时，导致其违约率无法与其它 bin 进行对比。 下图中的特征没有展现同样的趋势，因为趋势相关度为 85%。这两个指标可用于删除带噪声的特征。 当特征很多且相互关联时，删除低趋势相关度特征的效果很好。它会带来更少的过拟合，其它相关特征可以避免信息损失。同时需要注意不要删除太多重要特征，因为这可能导致性能下降。此外，你无法利用特征重要性来判断特征是否带噪声，因为重要的特征也会带噪声！ 使用不同时间段的测试数据效果更好，因为你可以借此确定特征趋势是否一直如此。 featexp 中的 get_trend_stats() 函数返回展示趋势相关度的数据帧，并随着特征而改变。 12from featexp import get_trend_statsstats = get_trend_stats(data=data_train, target_col='target', data_test=data_test) 下面我们就试着删除数据中低趋势相关度的特征，然后看结果是否有所改进。 我们可以看到，趋势相关度阈值越高，排行榜（LB）AUC 越高。不删除重要的特征进一步将 LB AUC 提高到 0.74。测试 AUC 的变化与 LB AUC 不同，这一点也很有趣。完整代码详见 featexp_demo notebook：https://github.com/abhayspawar/…。 3. 特征工程 通过查看这些图所获取的见解可以帮助你创建更好的特征。更好地理解数据将带来更好的特征工程。此外，它还可以帮助你改善现有特征。下面我们来看另一个特征 EXT_SOURCE_1： 具备高 EXT_SOURCE_1 的客户具备较低的违约率。但是，第一个 bin（违约率约 8%）没有遵循该特征趋势（向上升后下降）。它的负值是-99.985，而且人口数量较多。这可能表明这些是特殊的值，因此不遵循特征趋势。幸运的是，非线性模型在学习该关系方面不会有问题。而对于线性模型（如 logistic 回归），此类特殊值和空缺值应该采用类似样本的默认值进行估计，而不是特征平均值。 4. 特征重要性 featexp 还可以帮助衡量特征重要性。DAYS_BIRTH 和 EXT_SOURCE_1 都具备很好的趋势。但是 EXT_SOURCE_1 的人口数量集中于特殊值 bin，这表明其重要性可能不如 DAYS_BIRTH。基于 XGBoost 模型的特征重要性，DAYS_BIRTH 的重要性高于 EXT_SOURCE_1。 5. 特征 debug 查看 featexp 图可以帮助你捕捉复杂特征工程中的 bug： 检查一下特征的人数分布看起来是否正确。由于存在一些小 bug，我个人经常遭遇上述极端情况。 在看这些图之前，一定要假设特征趋势会是什么样子。如果特征趋势看起来不像你期望的那样，可能表示其中存在一些问题。坦率地说，这种假设趋势的过程使得构建 ML 模型更加有趣！ 6 泄露检测 从目标到特征的数据泄露会导致过拟合。泄露特征具有很高的特征重要性，但很难理解为什么特征会发生泄露。查看下列 featexp 图可以帮助你理解。 下面的特征在「Null」bin 中违约率为 0%，在其它 bin 中为 100%。很明显，这是极端的泄露案例。该特征只有在客户违约时才有价值。根据特征是什么，这可能是因为 bug 或者该特征只为违约者填充（在这种情况下它会下降）。弄清楚特征泄露的原因可以加速 debug。 7 模型监控 由于 featexp 计算两个数据集之间的趋势相关性，它可以很轻易地用于模型监控。每次模型被重新训练之后，就可以把新的训练数据与测试好的训练数据（通常从第一次构建模型开始训练数据）进行对比。趋势相关性能够帮助你监控特征信息与目标的关系是否发生任何变化。 感谢原作者：Abhay Pawar 原文链接：https://towardsdatascience.com/…]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
        <tag>featexp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 网站优化之SEO]]></title>
    <url>%2FHexo-website-seo%2F</url>
    <content type="text"><![CDATA[搭建好了个人博客，如果想让Google和百度等搜索引擎搜索到自己的博客，增加博客的访问量，那么对网站做一些SEO(Search Engine Optimization)，即搜索引擎优化，是非常有必要的，下面介绍一些简单的SEO优化方法。 添加sitemapSitemap即网站地图，它的作用在于便于搜索引擎更加智能地抓取网站。最简单和常见的sitemap形式，是XML文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新时间、更新的频率及相对其他网址重要程度等）。 Setp 1: 安装sitemap站点地图自动生成插件 12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save Setp 2: 配置站点跟目录下的_config.yml，添加 12345# hexo sitemap网站地图sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 添加robots.txtobots.txt是一种存放于网站根目录下的ASCII编码的文本文件，它的作用是告诉搜索引擎此网站中哪些内容是可以被爬取的，哪些是禁止爬取的。robots.txt应该放在站点目录下的source文件中，网站生成后在网站的根目录(站点目录/public/)下。 我的robots.txt内容如下 12345678910111213141516# hexo robots.txtUser-agent: * Allow: /Allow: /archives/Allow: /categories/Allow: /about/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: https://tangguangen.com/sitemap.xmlSitemap: https://tangguangen.com/baidusitemap.xml 提交百度站长Setp 1: 添加网站，百度提交网址入口点这里 这里按提示操作即可，网站验证方式建议使用CNAME验证。 Setp2: robots验证 Setp3: 抓取诊断 之所以要进行抓取诊断，是因为githubpage是不允许百度爬虫抓取的，如果你的网站是部署在GitHub上面的话就要选择主动推送的方式，如果抓取诊断成功的话就可以直接提交站点地图baidusitemap 方式一：主动推送Setp 1: 安装主动推送插件： 1npm install hexo-baidu-url-submit --save Setp 2: 配置站点跟目录下的_config.yml，添加一下内容 12345baidu_url_submit: count: 1 ## 提交最新的 1 个链接 host: tangguangen.com ## 在百度站长平台中注册的域名 token: your token ## 请注意这是您的秘钥，所以请不要把博客源代码发布在公众仓库里! path: baidu_urls.txt ## 文本文档的地址， 新链接会保存在此文本文档里 Setp 3: 配置站点跟目录下的_config.yml，添加deploy类型 123456deploy: - type: git repo: git@github.com:xguojing/xguojing.github.io.git branch: master - type: baidu_url_submitter # 这里是新添加的 repo: 最后记得把主题配置文件中的baidu_push设置为true，重新渲染部署即可。 方式二：提交sitemap 提交Google站长Setp 1: 添加网站 进入Google Search Console，相信大家都有Google账号吧。没有的话注册个账号吧，然后登录进去即可。 按提示操作添加网址，然后Google会让你验证你对网站的所有权，我选择的是HTML标记的验证方式 配置主题配置文件_config.yml，填写google_site_verification，之后重新渲染部署点击验证即可 1google_site_verification: your google_site_verification Setp 2: 添加站点地图sitemap 优化title编辑站点目录下的themes/layout/index.swig文件， 将下面的代码 1&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; &#123;% endlock %&#125; 改成 1&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; - &#123;&#123; theme.description &#125;&#125; &#123;% endlock %&#125; 这时将网站的描述及关键词加入了网站的title中，更有利于详细地描述网站。 后记开博客主要是为了记录和分享学习经历、技术笔记等，通过记录总结的方式提高学习效率，在互联网中和更多的人交流学习。学无止境，不忘初心。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 292. Nim游戏]]></title>
    <url>%2FLeetCode-292-Nim%E6%B8%B8%E6%88%8F%2F</url>
    <content type="text"><![CDATA[题目描述： 你和你的朋友，两个人一起玩 Nim游戏：桌子上有一堆石头，每次你们轮流拿掉 1 - 3 块石头。 拿掉最后一块石头的人就是获胜者。你作为先手。 你们是聪明人，每一步都是最优解。 编写一个函数，来判断你是否可以在给定石头数量的情况下赢得游戏。 示例: 12345&gt; 输入: 4&gt; 输出: false &gt; 解释: 如果堆中有 4 块石头，那么你永远不会赢得比赛；&gt; 因为无论你拿走 1 块、2 块 还是 3 块石头，最后一块石头总是会被你的朋友拿走。&gt; 思路简单题，巴什博弈。 显然，如果n=m+1，那么由于一次最多只能取m个，所以，无论先取者拿走多少个，后取者都能够一次拿走剩余的物品，后者取胜。因此我们发现了如何取胜的法则：如果n=（m+1）r+s，（r为任意自然数，s≤m),那么先取者要拿走s个物品，如果后取者拿走k（≤m)个，那么先取者再拿走m+1-k个，结果剩下（m+1）（r-1）个，以后保持这样的取法，那么先取者肯定获胜。总之，要保持给对手留下（m+1）的倍数，就能最后获胜。 对于巴什博弈，那么我们规定，如果最后取光者输，那么又会如何呢？ （n-1）%（m+1）==0则后手胜利 先手会重新决定策略，所以不是简单的相反行的 JAVA SOLUTION12345class Solution &#123; public boolean canWinNim(int n) &#123; return n % 4 == 0 ? false : true; &#125;&#125; 扩展威佐夫博奕威佐夫博弈（Wythoff’s game）：有两堆各若干个物品，两个人轮流从任一堆取至少一个或同时从两堆中取同样多的物品，规定每次至少取一个，多者不限，最后取光者得胜。 两个人如果都采用正确操作，那么面对非奇异局势，先拿者必胜；反之，则后拿者取胜。 那么任给一个局势， (a，b)，怎样判断它是不是奇异局势呢？我们有如下公式：$$ak =\lfloor \frac{k}{2}(1+\sqrt{5}) \rfloor ，$$ $$bk= ak + k \space \space\space\space（k=0，1，2，…n)$$ 尼姆博奕指的是这样的一个博弈游戏，目前有任意堆石子，每堆石子个数也是任意的，双方轮流从中取出石子，规则如下：1)每一步应取走至少一枚石子；每一步只能从某一堆中取走部分或全部石子；2)如果谁取到最后一枚石子就胜。 判断当前局势是否为必胜（必败）局势：把所有堆的石子数目用二进制数表示出来，当全部这些数按位异或结果为0时当前局面为必败局面，否则为必胜局面； 123456789101112131415161718192021#include&lt;iostream&gt; using namespace std; int temp[ 20 ]; //火柴的堆数 int main() &#123; int i, n, min; while( cin &gt;&gt; n ) &#123; for( i = 0; i &lt; n; i++ ) cin &gt;&gt; temp[ i ]; //第i个火柴堆的数量 min = temp[ 0 ]; for( i = 1; i &lt; n ; i++ ) min = min^temp[ i ]; //按位异或 if( min == 0 ) cout &lt;&lt; "Lose" &lt;&lt; endl; //输 else cout &lt;&lt; "Win" &lt;&lt; endl; //赢 &#125; return 0; &#125; 斐波那契博弈有一堆个数为n的石子，游戏双方轮流取石子，满足：1)先手不能在第一次把所有的石子取完；2)之后每次可以取的石子数介于1到对手刚取的石子数的2倍之间（包含1和对手刚取的石子数的2倍）。约定取走最后一个石子的人为赢家，求必败态。 这个游戏叫做斐波那契博弈，肯定和斐波那契数列：$f[n]：1,2,3,5,8,13,21,34,55,89,… $有密切的关系。如果试验一番之后，可以猜测：先手胜当且仅当n不是斐波那契数。换句话说，必败态构成斐波那契数列。]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转载]原码, 反码, 补码 详解]]></title>
    <url>%2F%E8%BD%AC%E8%BD%BD-%E5%8E%9F%E7%A0%81-%E5%8F%8D%E7%A0%81-%E8%A1%A5%E7%A0%81-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[作者：张子秋出处：http://www.cnblogs.com/zhangziqiu/ 本篇文章讲解了计算机的原码, 反码和补码. 并且进行了深入探求了为何要使用反码和补码, 以及更进一步的论证了为何可以用反码, 补码的加法计算原码的减法. 论证部分如有不对的地方请各位牛人帮忙指正! 希望本文对大家学习计算机基础有所帮助! 一. 机器数和真值在学习原码, 反码和补码之前, 需要先了解机器数和真值的概念. 1、机器数一个数在计算机中的二进制表示形式, 叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号, 正数为0, 负数为1. 比如，十进制中的数 +3 ，计算机字长为8位，转换成二进制就是00000011。如果是 -3 ，就是 10000011 。 那么，这里的 00000011 和 10000011 就是机器数。 2、真值 因为第一位是符号位，所以机器数的形式值就不等于真正的数值。例如上面的有符号数 10000011，其最高位1代表负，其真正数值是 -3 而不是形式值131（10000011转换成十进制等于131）。所以，为区别起见，将带符号位的机器数对应的真正数值称为机器数的真值。 例：0000 0001的真值 = +000 0001 = +1，1000 0001的真值 = –000 0001 = –1 二. 原码, 反码, 补码的基础概念和计算方法.在探求为何机器要使用补码之前, 让我们先了解原码, 反码和补码的概念.对于一个数, 计算机要使用一定的编码方式进行存储. 原码, 反码, 补码是机器存储一个具体数字的编码方式. 1. 原码原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是8位二进制: [+1]原 = 0000 0001 [-1]原 = 1000 0001 第一位是符号位. 因为第一位是符号位, 所以8位二进制数的取值范围就是: [1111 1111 , 0111 1111] 即 [-127 , 127] 原码是人脑最容易理解和计算的表示方式. 2. 反码反码的表示方法是: 正数的反码是其本身 负数的反码是在其原码的基础上, 符号位不变，其余各个位取反. [+1] = [00000001]原 = [00000001]反 [-1] = [10000001]原 = [11111110]反 可见如果一个反码表示的是负数, 人脑无法直观的看出来它的数值. 通常要将其转换成原码再计算. 3. 补码补码的表示方法是: 正数的补码就是其本身 负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1) [+1] = [00000001]原 = [00000001]反 = [00000001]补 [-1] = [10000001]原 = [11111110]反 = [11111111]补 对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值. 三. 为何要使用原码, 反码和补码在开始深入学习前, 我的学习建议是先”死记硬背”上面的原码, 反码和补码的表示方式以及计算方法. 现在我们知道了计算机可以有三种编码方式表示一个数. 对于正数因为三种编码方式的结果都相同: [+1] = [00000001]原 = [00000001]反 = [00000001]补 所以不需要过多解释. 但是对于负数: [-1] = [10000001]原 = [11111110]反 = [11111111]补 可见原码, 反码和补码是完全不同的. 既然原码才是被人脑直接识别并用于计算表示方式, 为何还会有反码和补码呢? 首先, 因为人脑可以知道第一位是符号位, 在计算的时候我们会根据符号位, 选择对真值区域的加减. (真值的概念在本文最开头). 但是对于计算机, 加减乘数已经是最基础的运算, 要设计的尽量简单. 计算机辨别”符号位”显然会让计算机的基础电路设计变得十分复杂! 于是人们想出了将符号位也参与运算的方法. 我们知道, 根据运算法则减去一个正数等于加上一个负数, 即: 1-1 = 1 + (-1) = 0 , 所以机器可以只有加法而没有减法, 这样计算机运算的设计就更简单了. 于是人们开始探索 将符号位参与运算, 并且只保留加法的方法. 首先来看原码: 计算十进制的表达式: 1-1=0 1 - 1 = 1 + (-1) = [00000001]原 + [10000001]原 = [10000010]原 = -2 如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的.这也就是为何计算机内部不使用原码表示一个数. 为了解决原码做减法的问题, 出现了反码: 计算十进制的表达式: 1-1=0 1 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原= [0000 0001]反 + [1111 1110]反 = [1111 1111]反 = [1000 0000]原 = -0 发现用反码计算减法, 结果的真值部分是正确的. 而唯一的问题其实就出现在”0”这个特殊的数值上. 虽然人们理解上+0和-0是一样的, 但是0带符号是没有任何意义的. 而且会有[0000 0000]原和[1000 0000]原两个编码表示0. 于是补码的出现, 解决了0的符号以及两个编码的问题: 1-1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]补 + [1111 1111]补 = [0000 0000]补=[0000 0000]原 这样0用[0000 0000]表示, 而以前出现问题的-0则不存在了.而且可以用[1000 0000]表示-128: (-1) + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补 -1-127的结果应该是-128, 在用补码运算的结果中, [1000 0000]补 就是-128. 但是注意因为实际上是使用以前的-0的补码来表示-128, 所以-128并没有原码和反码表示.(对-128的补码表示[1000 0000]补算出来的原码是[0000 0000]原, 这是不正确的) 使用补码, 不仅仅修复了0的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么8位二进制, 使用原码或反码表示的范围为[-127, +127], 而使用补码表示的范围为[-128, 127]. 因为机器使用补码, 所以对于编程中常用到的32位int类型, 可以表示范围是: [-231, 231-1] 因为第一位表示的是符号位.而使用补码表示时又可以多保存一个最小值. 四 原码, 反码, 补码 再深入计算机巧妙地把符号位参与运算, 并且将减法变成了加法, 背后蕴含了怎样的数学原理呢? 将钟表想象成是一个1位的12进制数. 如果当前时间是6点, 我希望将时间设置成4点, 需要怎么做呢?我们可以: \1. 往回拨2个小时: 6 - 2 = 4 \2. 往前拨10个小时: (6 + 10) mod 12 = 4 \3. 往前拨10+12=22个小时: (6+22) mod 12 =4 2,3方法中的mod是指取模操作, 16 mod 12 =4 即用16除以12后的余数是4. 所以钟表往回拨(减法)的结果可以用往前拨(加法)替代! 现在的焦点就落在了如何用一个正数, 来替代一个负数. 上面的例子我们能感觉出来一些端倪, 发现一些规律. 但是数学是严谨的. 不能靠感觉. 首先介绍一个数学中相关的概念: 同余 同余的概念两个整数a，b，若它们除以整数m所得的余数相等，则称a，b对于模m同余 记作 a ≡ b (mod m) 读作 a 与 b 关于模 m 同余。 举例说明: 4 mod 12 = 4 16 mod 12 = 4 28 mod 12 = 4 所以4, 16, 28关于模 12 同余. 负数取模正数进行mod运算是很简单的. 但是负数呢? 下面是关于mod运算的数学定义: 上面是截图, “取下界”符号找不到如何输入(word中粘贴过来后乱码). 下面是使用”L”和”J”替换上图的”取下界”符号: x mod y = x - y L x / y J 上面公式的意思是: x mod y等于 x 减去 y 乘上 x与y的商的下界. 以 -3 mod 2 举例: -3 mod 2 = -3 - 2xL -3/2 J = -3 - 2xL-1.5J = -3 - 2x(-2) = -3 + 4 = 1 所以: (-2) mod 12 = 12-2=10 (-4) mod 12 = 12-4 = 8 (-5) mod 12 = 12 - 5 = 7 开始证明再回到时钟的问题上: 回拨2小时 = 前拨10小时 回拨4小时 = 前拨8小时 回拨5小时= 前拨7小时 注意, 这里发现的规律! 结合上面学到的同余的概念.实际上: (-2) mod 12 = 10 10 mod 12 = 10 -2与10是同余的. (-4) mod 12 = 8 8 mod 12 = 8 -4与8是同余的. 距离成功越来越近了. 要实现用正数替代负数, 只需要运用同余数的两个定理: 反身性: a ≡ a (mod m) 这个定理是很显而易见的. 线性运算定理: 如果a ≡ b (mod m)，c ≡ d (mod m) 那么: (1)a ± c ≡ b ± d (mod m) (2)a c ≡ b d (mod m) 如果想看这个定理的证明, 请看:http://baike.baidu.com/view/79282.htm 所以: 7 ≡ 7 (mod 12) (-2) ≡ 10 (mod 12) 7 -2 ≡ 7 + 10 (mod 12) 现在我们为一个负数, 找到了它的正数同余数. 但是并不是7-2 = 7+10, 而是 7 -2 ≡ 7 + 10 (mod 12) , 即计算结果的余数相等. 接下来回到二进制的问题上, 看一下: 2-1=1的问题. 2-1=2+(-1) = [0000 0010]原 + [1000 0001]原= [0000 0010]反 + [1111 1110]反 先到这一步, -1的反码表示是1111 1110. 如果这里将[1111 1110]认为是原码, 则[1111 1110]原 = -126, 这里将符号位除去, 即认为是126. 发现有如下规律: (-1) mod 127 = 126 126 mod 127 = 126 即: (-1) ≡ 126 (mod 127) 2-1 ≡ 2+126 (mod 127) 2-1 与 2+126的余数结果是相同的! 而这个余数, 正式我们的期望的计算结果: 2-1=1 所以说一个数的反码, 实际上是这个数对于一个膜的同余数. 而这个膜并不是我们的二进制, 而是所能表示的最大值! 这就和钟表一样, 转了一圈后总能找到在可表示范围内的一个正确的数值! 而2+126很显然相当于钟表转过了一轮, 而因为符号位是参与计算的, 正好和溢出的最高位形成正确的运算结果. 既然反码可以将减法变成加法, 那么现在计算机使用的补码呢? 为什么在反码的基础上加1, 还能得到正确的结果? 2-1=2+(-1) = [0000 0010]原 + [1000 0001]原 = [0000 0010]补 + [1111 1111]补 如果把[1111 1111]当成原码, 去除符号位, 则: [0111 1111]原 = 127 其实, 在反码的基础上+1, 只是相当于增加了膜的值: (-1) mod 128 = 127 127 mod 128 = 127 2-1 ≡ 2+127 (mod 128) 此时, 表盘相当于每128个刻度转一轮. 所以用补码表示的运算结果最小值和最大值应该是[-128, 128]. 但是由于0的特殊情况, 没有办法表示128, 所以补码的取值范围是[-128, 127] 本人一直不善于数学, 所以如果文中有不对的地方请大家多多包含, 多多指点!]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop兼容性]]></title>
    <url>%2FHadoop%E5%85%BC%E5%AE%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[目的本文档介绍了Apache Hadoop项目的兼容性目标。枚举了影响Hadoop开发人员，下游项目和最终用户的Hadoop版本之间的不同类型的兼容性。对于每种类型的兼容性，我们： 描述对下游项目或最终用户的影响 在适用的情况下，当允许不兼容的更改时，请调用Hadoop开发人员采用的策略。 兼容性类型Java APIHadoop接口和类被注释为描述目标受众和稳定性，以保持与先前版本的兼容性。有关详细信息，请参阅Hadoop接口分类。 InterfaceAudience：捕获目标受众，可能的值是Public（对于最终用户和外部项目），LimitedPrivate（对于其他Hadoop组件，以及密切相关的项目，如YARN，MapReduce，HBase等）和Private（用于组件内部）。 InterfaceStability：描述允许哪些类型的接口更改。可能的值为Stable，Evolving，Unstable和Deprecated。 用例 需要公共稳定的API兼容性，以确保最终用户程序和下游项目继续工作而无需修改。 需要LimitedPrivate-Stable API兼容性，以允许跨次要版本升级单个组件。 滚动升级需要Private-Stable API兼容性。 政策 在主要版本中删除之前，必须至少弃用一个主要版本的Public-Stable API。 LimitedPrivate-Stable API可以在主要版本中更改，但不能在主要版本中更改。 Private-Stable API可以在主要版本中更改，但不能在主要版本中更改。 未注释的类是隐式“私有”。未注释的类成员继承封闭类的注释。 注意：从proto文件生成的API需要与滚动升级兼容。有关详细信息，请参阅有关电线兼容性的部分。API和有线通信的兼容性策略需要齐头并进，以解决这个问题。 语义兼容性Apache Hadoop努力确保API的行为在版本上保持一致，但正确性的更改可能会导致行为更改。测试和javadoc指定API的行为。社区正在更严格地指定一些API，并增强测试套件以验证是否符合规范，有效地为易于测试的行为子集创建了正式的规范。 政策可以更改API的行为以修复不正确的行为，这种更改伴随着更新现有错误测试或在更改之前没有测试的情况下添加测试。 电线兼容性线路兼容性涉及在Hadoop进程之间通过线路传输的数据。Hadoop使用Protocol Buffers进行大多数RPC通信。保持兼容性需要禁止如下所述的修改。还应考虑非RPC通信，例如使用HTTP传输HDFS映像作为快照或传输MapTask输出的一部分。潜在的沟通可以分类如下： 客户端 - 服务器：Hadoop客户端和服务器之间的通信（例如，HDFS客户端到NameNode协议，或YARN客户端到ResourceManager协议）。 客户端 - 服务器（管理员）：值得区分仅由管理命令（例如，HAAdmin协议）使用的客户端 - 服务器协议的子集，因为这些协议仅影响能够容忍最终用户（使用通用客户端）的更改的管理员服务器协议）不能。 服务器 - 服务器：服务器之间的通信（例如，DataNode和NameNode之间的协议，或NodeManager和ResourceManager） 用例 即使在将服务器（群集）升级到更高版本（或反之亦然）之后，也需要客户端 - 服务器兼容性以允许用户继续使用旧客户端。例如，Hadoop 2.1.0客户端与Hadoop 2.3.0集群通信。 还需要客户端 - 服务器兼容性，以允许用户在升级服务器（群集）之前升级客户端。例如，Hadoop 2.4.0客户端与Hadoop 2.3.0集群通信。这允许在完全集群升级之前部署客户端错误修复。请注意，新客户端API或shell命令调用的新群集功能将无法使用。尝试使用尚未部署到群集的新API（包括数据结构中的新字段）的YARN应用程序可能会出现链接异常。 还需要客户端 - 服务器兼容性，以允许升级单个组件而不升级其他组件。例如，在不升级MapReduce的情况下将HDFS从版本2.1.0升级到2.2.0。 需要服务器 - 服务器兼容性以允许活动集群中的混合版本，以便可以在不停机的情况下以滚动方式升级集群。 政策 Client-Server和Server-Server兼容性都保留在主要版本中。（不同类别的不同政策尚待考虑。） 兼容性只能在主要版本中打破，但即使在主要版本中破坏兼容性也会产生严重后果，应在Hadoop社区中进行讨论。 Hadoop协议在.proto（ProtocolBuffers）文件中定义。客户端 - 服务器协议和服务器 - 服务器协议.proto文件标记为稳定。当.proto文件标记为稳定时，意味着应以兼容的方式进行更改，如下所述： 以下更改是兼容的，并且随时允许： 添加一个可选字段，期望代码处理由于与旧版本代码的通信而丢失的字段。 向服务添加新的rpc /方法 向Message添加新的可选请求 重命名字段 重命名.proto文件 更改影响代码生成的.proto注释（例如java包的名称） 以下更改不兼容，但只能在主要版本中考虑 更改rpc /方法名称 更改rpc / method参数类型或返回类型 删除rpc /方法 更改服务名称 更改消息的名称 以不兼容的方式修改字段类型（以递归方式定义） 将可选字段更改为必需 添加或删除必填字段 只要可选字段具有允许删除的合理默认值，就删除可选字段 以下更改不兼容，因此从不允许 更改字段ID 重用以前删除的旧字段。 字段数字很便宜，改变和重用并不是一个好主意。 最终用户应用程序的Java二进制兼容性，即Apache Hadoop ABI随着Apache Hadoop修订版的升级，最终用户合理地期望他们的应用程序在没有任何修改的情况下继续工作。这是通过支持API兼容性，语义兼容性和线路兼容性来实现的。 但是，Apache Hadoop是一个非常复杂的分布式系统，可以为各种各样的用例提供服务。特别是，Apache Hadoop MapReduce是一个非常非常广泛的API; 从某种意义上说，最终用户可以做出广泛的假设，例如当他们的map / reduce任务执行时本地磁盘的布局，他们的任务的环境变量等。在这种情况下，很难完全指定和支持，绝对兼容性。 用例 现有的MapReduce应用程序，包括现有打包的最终用户应用程序罐和Apache Pig，Apache Hive，Cascading等项目，在指向主要版本中升级的Apache Hadoop集群时，应该不加修改。 现有的YARN应用程序，包括现有打包的最终用户应用程序罐和Apache Tez等项目，在指向主要版本中升级的Apache Hadoop集群时，应该不加修改。 将数据传入/传出HDFS的现有应用程序（包括现有打包的最终用户应用程序罐和Apache Flume等框架）在指向主要版本中的升级后的Apache Hadoop集群时应该不加修改。 政策 现有的MapReduce，YARN和HDFS应用程序和框架应该在主要版本中不加修改，即支持Apache Hadoop ABI。 很少一部分应用程序可能会受到磁盘布局等变化的影响，开发人员社区将尽力减少这些变化，并且不会将它们放在次要版本中。在更加严重的情况下，我们将考虑强烈恢复这些重大变更，并在必要时使违规版本无效。 特别是对于MapReduce应用程序，开发人员社区将尽力支持跨主要版本提供二进制兼容性，例如使用org.apache.hadoop.mapred的应用程序。 在hadoop-1.x和hadoop-2.x之间兼容支持API。有关详细信息，请参阅hadoop-1.x和hadoop-2.x之间的MapReduce应用程序的兼容性。 REST APIREST API兼容性对应于请求（URL）和对每个请求的响应（内容，可能包含其他URL）。Hadoop REST API专门用于发布版本（甚至是主要版本）的客户端的稳定使用。以下是公开的REST API： WebHDFS - 稳定 的ResourceManager 节点管理器 MR Application Master 历史服务器 时间轴服务器v1 REST API 时间轴服务v2 REST API 政策上面文本中注释稳定的API保留了至少一个主要版本的兼容性，并且可能在主要版本中由较新版本的REST API弃用。、 度量/ JMX虽然Metrics API兼容性受Java API兼容性的约束，但Hadoop公开的实际度量标准需要兼容，以便用户能够自动使用它们（脚本等）。添加其他指标是兼容的。修改（例如，更改单位或测量）或删除现有指标会破坏兼容性。同样，对JMX MBean对象名称的更改也会破坏兼容性。 政策度量标准应保留主要版本中的兼容性。 文件格式和元数据用户和系统级数据（包括元数据）存储在不同格式的文件中。对元数据或用于存储数据/元数据的文件格式的更改可能导致版本之间不兼容。 用户级文件格式对最终用户用于存储其数据的格式的更改可能会阻止他们在以后的版本中访问数据，因此保持这些文件格式兼容非常重要。人们总是可以添加一种改进现有格式的“新”格式。这些格式的示例包括har，war，SequenceFileFormat等。 政策 非正向兼容的用户文件格式更改仅限于主要版本。当用户文件格式发生变化时，预计新版本将读取现有格式，但可能会以与先前版本不兼容的格式写入数据。此外，社区应优先创建程序必须选择的新格式，而不是对现有格式进行不兼容的更改。 系统内部文件格式Hadoop内部数据也存储在文件中，再次更改这些格式可能会导致不兼容。虽然此类更改不像用户级文件格式那样具有破坏性，但是关于何时可以破坏兼容性的策略很重要。 MapReduce的MapReduce使用I-File等格式来存储特定于MapReduce的数据。 政策MapReduce内部格式（如IFile）在主要版本中保持兼容性。对这些格式的更改可能导致正在进行的作业失败，因此我们应该确保较新的客户端能够以兼容的方式从旧服务器获取随机数据。 HDFS元数据HDFS以特定格式保存元数据（图像和编辑日志）。对格式或元数据的不兼容更改会阻止后续版本读取旧元数据。此类不兼容的更改可能需要HDFS“升级”才能转换元数据以使其可访问。某些更改可能需要多个此类“升级”。 根据更改中的不兼容程度，可能会出现以下可能的情况： 自动：图像自动升级，无需显式“升级”。 直接：图像可升级，但可能需要一个显式版本“升级”。 间接：图像可升级，但可能需要先升级到中间版本。 无法升级：图像无法升级。 政策 版本升级必须允许群集回滚到旧版本及其旧磁盘格式。回滚需要还原原始数据，但不需要还原更新的数据。 HDFS元数据更改必须可通过任何升级路径进行升级 - 自动，直接或间接。 尚未考虑基于升级类型的更详细的策略。 命令行界面（CLI）Hadoop命令行程序可以直接通过系统shell或shell脚本使用。更改命令的路径，删除或重命名命令行选项，参数的顺序，或命令返回代码和输出中断兼容性，并可能对用户产生负面影响。 政策在将一个主要版本删除或在后续主要版本中进行不兼容修改之前，将弃用CLI命令（使用时发出警告）。 Web UIWeb UI，尤其是网页的内容和布局，更改可能会干扰屏幕抓取网页信息的尝试。 政策网页并不意味着被删除，因此允许随时对它们进行不兼容的更改。期望用户使用REST API来获取任何信息。 Hadoop配置文件用户使用（1）Hadoop定义的属性来配置和提供Hadoop的提示，以及（2）自定义属性以将信息传递给作业。因此，配置属性的兼容性是双重的： 修改Hadoop定义的属性的键名，值单位和默认值。 自定义配置属性键不应与Hadoop定义的属性的命名空间冲突。通常，用户应避免使用Hadoop使用的前缀：hadoop，io，ipc，fs，net，file，ftp，s3，kfs，ha，file，dfs，mapred，mapreduce，yarn。 政策 Hadoop定义的属性至少在一个主要版本中被弃用，然后才会被删除。不允许修改现有属性的单位。 Hadoop定义的属性的默认值可以在次要/主要版本中更改，但在次要版本中的点版本中保持不变。 目前，没有明确的政策可以添加/删除新的前缀，以及自定义配置属性要避免的前缀列表。但是，如上所述，用户应避免使用Hadoop使用的前缀：hadoop，io，ipc，fs，net，file，ftp，s3，kfs，ha，file，dfs，mapred，mapreduce，yarn。 目录结构源代码，工件（源和测试），用户日志，配置文件，输出和作业历史记录都存储在本地文件系统或HDFS的磁盘上。更改这些用户可访问文件的目录结构会破坏兼容性，即使在通过符号链接保留原始路径的情况下（例如，如果路径由配置为不遵循符号链接的servlet访问）。 政策 源代码和构建工件的布局可以随时更改，尤其是在主要版本中。在主要版本中，开发人员将尝试（不保证）保留目录结构; 但是，可以添加/移动/删除单个文件。确保补丁与代码保持同步的最佳方法是将它们提交到Apache源代码树。 配置文件，用户日志和作业历史记录的目录结构将在主要版本中的次要版本和点版本中保留。 Java Classpath针对Hadoop构建的用户应用程序可能会将所有Hadoop jar（包括Hadoop的库依赖项）添加到应用程序的类路径中。添加新依赖项或更新现有依赖项的版本可能会干扰应用程序类路径中的依赖项。 政策目前，没有关于Hadoop的依赖关系何时可以改变的政策。 环境变量用户和相关项目通常使用导出的环境变量（例如HADOOP_CONF_DIR），因此删除或重命名环境变量是一种不兼容的更改。 政策目前，没有关于环境变量何时可以改变的政策。开发人员尝试限制对主要版本的更改。 构建工件Hadoop使用maven进行项目管理，更改工件可能会影响现有的用户工作流程。 政策 测试工件：生成的测试jar严格用于内部使用，预计不会在Hadoop之外使用，类似于注释@Private，@ Unstable的API。 构建工件：hadoop-client工件（maven groupId：artifactId）在主要版本中保持兼容，而其他工件可以以不兼容的方式更改。 硬件/软件要求为了跟上硬件，操作系统，JVM和其他软件的最新进展，新的Hadoop版本或其某些功能可能需要更高版本的版本。对于特定环境，升级Hadoop可能需要升级其他相关软件组件。 政策 硬件 架构：社区没有计划将Hadoop限制为特定架构，但可以进行特定于系列的优化。 最小资源：虽然无法保证Hadoop守护程序所需的最低资源，但社区尝试不在次要版本中增加要求。 操作系统：社区将尝试在次要版本中维护相同的操作系统要求（操作系统内核版本）。目前，GNU / Linux和Microsoft Windows是社区正式支持的操作系统，而Apache Hadoop在其他操作系统（如Apple MacOSX，Solaris等）上运行良好。 除非所讨论的JVM版本不受支持，否则JVM要求不会在同一次要版本中的点版本之间发生更改。次要/主要版本可能需要更高版本的JVM用于部分/全部受支持的操作系统。 其他软件：社区尝试维护Hadoop所需的其他软件的最低版本。例如，ssh，kerberos等。 参考以下是与该主题相关的一些相关JIRA和页面： 该文件的演变 - HADOOP-9517 hadoop-1.x和hadoop-2.x之间MapReduce最终用户应用程序的二进制兼容性 - MapReduce hadoop-1.x和hadoop-2.x之间的兼容性 根据接口分类计划的接口注释 - HADOOP-7391 Hadoop接口分类 兼容Hadoop 1.x版本 - HADOOP-5071 在Hadoop的路线图，捕捉其他发行政策页]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop: FileSystem Shell]]></title>
    <url>%2FHadoop-FileSystem-Shell%2F</url>
    <content type="text"><![CDATA[概览文件系统（FS）shell包括各种类似shell的命令，这些命令直接与Hadoop分布式文件系统（HDFS）以及Hadoop支持的其他文件系统交互，例如本地FS，HFTP FS，S3 FS等。FS shell由以下方式调用： 1bin/hadoop fs &lt;args&gt; 所有FS shell命令都将路径URI作为参数。URI格式为scheme：// authority / path。对于HDFS，方案是hdfs，对于本地FS，方案是文件。该计划和权限是可选的。如果未指定，则使用配置中指定的默认方案。可以将HDFS文件或目录（例如/ parent / child）指定为hdfs：// namenodehost / parent / child或简单地指定为/ parent / child（假设您的配置设置为指向hdfs：// namenodehost）。 FS shell中的大多数命令都表现得像对应的Unix命令。使用每个命令描述差异。错误信息发送到stderr，输出发送到stdout。 如果正在使用HDFS，则hdfs dfs是同义词。 可以使用相对路径。对于HDFS，当前工作目录是HDFS主目录/ user / &lt;username&gt;，通常必须手动创建。也可以隐式访问HDFS主目录，例如，当使用HDFS垃圾文件夹时，主目录中的.Trash目录。 有关通用shell选项，请参阅命令手册。 appendToFile用法：hadoop fs -appendToFile &lt;localsrc&gt; ... &lt;dst&gt; 将单个src或多个srcs从本地文件系统附加到目标文件系统。还从stdin读取输入并附加到目标文件系统。 hadoop fs -appendToFile localfile /user/hadoop/hadoopfile hadoop fs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile hadoop fs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile hadoop fs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin. 退出代码： 成功时返回0，错误时返回1。 cat用法：hadoop fs -cat [-ignoreCrc] URI [URI ...] 将源路径复制到stdout。 选项 该-ignoreCrc选项禁用checkshum验证。 例： hadoop fs -cat hdfs：//nn1.example.com/file1 hdfs：//nn2.example.com/file2 hadoop fs -cat file：/// file3 / user / hadoop / file4 退出代码： 成功时返回0，错误时返回-1。 checksum用法：hadoop fs -checksum URI 返回文件的校验和信息。 例： hadoop fs -checksum hdfs：//nn1.example.com/file1 hadoop fs -checksum file：/// etc / hosts chgrp用法：hadoop fs -chgrp [-R] GROUP URI [URI ...] 更改文件的组关联。用户必须是文件的所有者，否则必须是超级用户。其他信息在“ 权限指南”中。 选项 -R选项将通过目录结构递归地进行更改。 CHMOD用法：hadoop fs -chmod [-R] &lt;MODE [，MODE] ... | OCTALMODE&gt; URI [URI ...] 更改文件的权限。使用-R，通过目录结构递归更改。用户必须是文件的所有者，否则必须是超级用户。其他信息在“ 权限指南”中。 选项 -R选项将通过目录结构递归地进行更改。 CHOWN用法：hadoop fs -chown [-R] [OWNER] [：[GROUP]] URI [URI] 更改文件的所有者。用户必须是超级用户。其他信息在“ 权限指南”中。 选项 -R选项将通过目录结构递归地进行更改。 copyFromLocal用法：hadoop fs -copyFromLocal &lt;localsrc&gt; URI 与fs -put命令类似，但源仅限于本地文件引用。 选项： -p：保留访问和修改时间，所有权和权限。（假设权限可以跨文件系统传播） -f：覆盖目标（如果已存在）。 -l：允许DataNode懒惰地将文件持久保存到磁盘，强制复制因子为1.此标志将导致持久性降低。小心使用。 -d：使用后缀._COPYING_跳过创建临时文件。 copyToLocal用法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt; 与get命令类似，但目标仅限于本地文件引用。 count用法：hadoop fs -count [-q] [-h] [-v] [-x] [-t [&lt;存储类型&gt;]] [-u] &lt;路径&gt; 计算与指定文件模式匹配的路径下的目录，文件和字节数。获取配额和使用情况。-count的输出列为：DIR_COUNT，FILE_COUNT，CONTENT_SIZE，PATHNAME -u和-q选项控制输出包含的列。-q表示显示配额，-u限制输出以仅显示配额和使用情况。 -count -q的输出列为：QUOTA，REMAINING_QUOTA，SPACE_QUOTA，REMAINING_SPACE_QUOTA，DIR_COUNT，FILE_COUNT，CONTENT_SIZE，PATHNAME -count -u的输出列为：QUOTA，REMAINING_QUOTA，SPACE_QUOTA，REMAINING_SPACE_QUOTA，PATHNAME -t选项显示每种存储类型的配额和使用情况。如果未给出-u或-q选项，则忽略-t选项。可以在-t选项中使用的可能参数列表（除参数“”之外不区分大小写）：“”，“all”，“ram_disk”，“ssd”，“disk”或“archive”。 -h选项以人类可读格式显示大小。 -v选项显示标题行。 -x选项从结果计算中排除快照。如果没有-x选项（默认），则始终从所有INode计算结果，包括给定路径下的所有快照。如果给出-u或-q选项，则忽略-x选项。 例： hadoop fs -count hdfs：//nn1.example.com/file1 hdfs：//nn2.example.com/file2 hadoop fs -count -q hdfs：//nn1.example.com/file1 hadoop fs -count -q -h hdfs：//nn1.example.com/file1 hadoop fs -count -q -h -v hdfs：//nn1.example.com/file1 hadoop fs -count -u hdfs：//nn1.example.com/file1 hadoop fs -count -u -h hdfs：//nn1.example.com/file1 hadoop fs -count -u -h -v hdfs：//nn1.example.com/file1 退出代码： 成功时返回0，错误时返回-1 CP用法：hadoop fs -cp [-f] [-p | -p [topax]] URI [URI ...] &lt;dest&gt; 将文件从源复制到目标。此命令也允许多个源，在这种情况下，目标必须是目录。 如果（1）源文件系统和目标文件系统支持它们（仅限HDFS），并且（2）所有源和目标路径名都在/.reserved/raw层次结构中，则保留’raw。‘命名空间扩展属性。是否保留raw。 namespace xattrs的确定与-p（保留）标志无关。 选项： 如果目标已存在，则-f选项将覆盖目标。 -p选项将保留文件属性[topx]（时间戳，所有权，权限，ACL，XAttr）。如果指定了-p而没有arg，则保留时间戳，所有权和权限。如果指定了-pa，则还保留权限，因为ACL是一组超级权限。确定是否保留原始命名空间扩展属性与-p标志无关。 例： hadoop fs -cp / user / hadoop / file1 / user / hadoop / file2 hadoop fs -cp / user / hadoop / file1 / user / hadoop / file2 / user / hadoop / dir 退出代码： 成功时返回0，错误时返回-1。 createSnapshot请参阅“ HDFS快照指南”。 deleteSnapshot请参阅“ HDFS快照指南”。 df用法：hadoop fs -df [-h] URI [URI ...] 显示可用空间。 选项： -h选项将以“人类可读”的方式格式化文件大小（例如64.0m而不是67108864） 例： hadoop dfs -df / user / hadoop / dir1 du用法：hadoop fs -du [-s] [-h] [-x] URI [URI ...] 显示给定目录中包含的文件和目录的大小或文件的长度，以防它只是一个文件。 选项： -s选项将导致显示文件长度的汇总摘要，而不是单个文件。如果没有-s选项，则通过从给定路径向上移动1级来完成计算。 -h选项将以“人类可读”的方式格式化文件大小（例如64.0m而不是67108864） -x选项将从结果计算中排除快照。如果没有-x选项（默认），则始终从所有INode计算结果，包括给定路径下的所有快照。 du返回三列，格式如下： 1size disk_space_consumed_with_all_replicas full_path_name 例： hadoop fs -du / user / hadoop / dir1 / user / hadoop / file1 hdfs：//nn.example.com/user/hadoop/dir1 退出代码：成功时返回0，错误时返回-1。 dus用法：hadoop fs -dus &lt;args&gt; 显示文件长度的摘要。 注意：不推荐使用此命令。而是使用hadoop fs -du -s。 expunge用法：hadoop fs -expunge 从trash目录中永久删除早于保留阈值的检查点中的文件，并创建新的检查点。 创建检查点时，垃圾箱中最近删除的文件将移动到检查点下。早于fs.trash.interval的检查点中的文件将在下次调用-expunge命令时被永久删除。 如果文件系统支持该功能，则用户可以配置为通过存储为fs.trash.checkpoint.interval的参数（在core-site.xml中）定期创建和删除检查点。该值应小于或等于fs.trash.interval。 有关HDFS垃圾功能的更多信息，请参阅HDFS体系结构指南。 find用法：hadoop fs -find &lt;path&gt; ... &lt;expression&gt; ... 查找与指定表达式匹配的所有文件，并将选定的操作应用于它们。如果未指定路径，则默认为当前工作目录。如果未指定表达式，则默认为-print。 识别以下主要表达式： -name pattern-iname pattern 如果文件的基名与使用标准文件系统通配符的模式匹配，则求值为true。如果使用-iname，则匹配不区分大小写。 -print-print0 始终评估为true。导致将当前路径名写入标准输出。如果使用-print0表达式，则附加ASCII NULL字符。 识别以下运算符： 表达式-a表达式表达式和表达式表达式表达式 用于连接两个表达式的逻辑AND运算符。如果两个子表达式都返回true，则返回true。由两个表达式的并置所暗示，因此不需要明确指定。如果第一个表达式失败，则不会应用第二个表达式。 例： 1hadoop fs -find / -name test -print 退出代码： 成功时返回0，错误时返回-1。 get用法：hadoop fs -get [-ignorecrc] [-crc] [-p] [-f] &lt;src&gt; &lt;localdst&gt; 将文件复制到本地文件系统。可以使用-ignorecrc选项复制CRC校验失败的文件。可以使用-crc选项复制文件和CRC。 例： hadoop fs -get / user / hadoop / file localfile hadoop fs -get hdfs：//nn.example.com/user/hadoop/file localfile 退出代码： 成功时返回0，错误时返回-1。 选项： -p：保留访问和修改时间，所有权和权限。（假设权限可以跨文件系统传播） -f：覆盖目标（如果已存在）。 -ignorecrc：对下载的文件进行跳过CRC校验。 -crc：为下载的文件写入CRC校验和。 getfacl用法：hadoop fs -getfacl [-R] &lt;path&gt; 显示文件和目录的访问控制列表（ACL）。如果目录具有默认ACL，则getfacl还会显示默认ACL。 选项： -R：递归列出所有文件和目录的ACL。 path：要列出的文件或目录。 例子： hadoop fs -getfacl / file hadoop fs -getfacl -R / dir 退出代码： 成功时返回0，错误时返回非零。 getfattr用法：hadoop fs -getfattr [-R] -n name | -d [-e en] &lt;path&gt; 显示文件或目录的扩展属性名称和值（如果有）。 选项： -R：递归列出所有文件和目录的属性。 -n name：转储指定的扩展属性值。 -d：转储与pathname关联的所有扩展属性值。 -e encoding：检索后对代码值进行编码。有效编码为“text”，“hex”和“base64”。编码为文本字符串的值用双引号（“）括起来，编码为十六进制和base64的值分别以0x和0为前缀。 path：文件或目录。 例子： hadoop fs -getfattr -d / file hadoop fs -getfattr -R -n user.myAttr / dir 退出代码： 成功时返回0，错误时返回非零。 getmerge用法：hadoop fs -getmerge [-nl] &lt;src&gt; &lt;localdst&gt; 将源目录和目标文件作为输入，并将src中的文件连接到目标本地文件。可选地，-nl可以设置为在每个文件的末尾添加换行符（LF）。-skip-empty-file可用于在空文件的情况下避免不需要的换行符。 例子： hadoop fs -getmerge -nl / src /opt/output.txt hadoop fs -getmerge -nl /src/file1.txt /src/file2.txt /output.txt 退出代码： 成功时返回0，错误时返回非零。 help用法：hadoop fs -help 返回使用输出。 ls用法：hadoop fs -ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] &lt;args&gt; 选项： -C：仅显示文件和目录的路径。 -d：目录列为纯文件。 -h：以人类可读的方式格式化文件大小（例如64.0m而不是67108864）。 -q：打印？而不是不可打印的字符。 -R：递归列出遇到的子目录。 -t：按修改时间排序输出（最近的第一个）。 -S：按文件大小排序输出。 -r：反转排序顺序。 -u：使用访问时间而不是修改时间进行显示和排序。 对于文件，ls使用以下格式返回文件的stat： 1permissions number_of_replicas userid groupid filesize modification_date modification_time filename 对于目录，它返回其直接子节点的列表，如在Unix中。目录列为： 1permissions userid groupid modification_date modification_time dirname 默认情况下，目录中的文件按文件名排序。 例： hadoop fs -ls / user / hadoop / file1 退出代码： 成功时返回0，错误时返回-1。 lsr用法：hadoop fs -lsr &lt;args&gt; ls的递归版本。 注意：不推荐使用此命令。而是使用hadoop fs -ls -R mkdir用法：hadoop fs -mkdir [-p] &lt;paths&gt; 将路径uri作为参数并创建目录。 选项： -p选项行为与Unix mkdir -p非常相似，沿路径创建父目录。 例： hadoop fs -mkdir / user / hadoop / dir1 / user / hadoop / dir2 hadoop fs -mkdir hdfs：//nn1.example.com/user/hadoop/dir hdfs：//nn2.example.com/user/hadoop/dir 退出代码： 成功时返回0，错误时返回-1 moveFromLocal用法：hadoop fs -moveFromLocal &lt;localsrc&gt; &lt;dst&gt; 与put命令类似，只是在复制后删除了源localsrc。 moveToLocal用法：hadoop fs -moveToLocal [-crc] &lt;src&gt; &lt;dst&gt; 显示“尚未实现”消息。 mv用法：hadoop fs -mv URI [URI ...] &lt;dest&gt; 将文件从源移动到目标。此命令也允许多个源，在这种情况下，目标需要是目录。不允许跨文件系统移动文件。 例： hadoop fs -mv / user / hadoop / file1 / user / hadoop / file2 hadoop fs -mv hdfs：//nn.example.com/file1 hdfs：//nn.example.com/file2 hdfs：//nn.example.com/file3 hdfs：//nn.example.com/dir1 退出代码： 成功时返回0，错误时返回-1。 put用法：hadoop fs -put [-f] [-p] [-l] [-d] [ - | &lt;localsrc1&gt; ..]。&lt;DST&gt; 将单个src或多个srcs从本地文件系统复制到目标文件系统。如果源设置为“ - ”，还从stdin读取输入并写入目标文件系统 如果文件已存在，则复制失败，除非给出-f标志。 选项： -p：保留访问和修改时间，所有权和权限。（假设权限可以跨文件系统传播） -f：覆盖目标（如果已存在）。 -l：允许DataNode懒惰地将文件持久保存到磁盘，强制复制因子为1.此标志将导致持久性降低。小心使用。 -d：使用后缀._COPYING_跳过创建临时文件。 例子： hadoop fs -put localfile / user / hadoop / hadoopfile hadoop fs -put -f localfile1 localfile2 / user / hadoop / hadoopdir hadoop fs -put -d localfile hdfs：//nn.example.com/hadoop/hadoopfile hadoop fs -put - hdfs：//nn.example.com/hadoop/hadoopfile从stdin读取输入。 退出代码： 成功时返回0，错误时返回-1。 renameSnapshot请参阅“ HDFS快照指南”。 rm用法：hadoop fs -rm [-f] [-r | -R] [-skipTrash] [-safely] URI [URI ...] 删除指定为args的文件。 如果启用了垃圾箱，则文件系统会将已删除的文件移动到垃圾箱目录（由FileSystem＃getTrashRoot提供）。 目前，默认情况下禁用垃圾箱功能。用户可以通过为参数fs.trash.interval（在core-site.xml中）设置大于零的值来启用垃圾。 请参阅删除有关删除垃圾箱中文件的信息。 选项： 如果文件不存在，-f选项将不显示诊断消息或修改退出状态以反映错误。 -R选项以递归方式删除目录及其下的任何内容。 -r选项等效于-R。 -skipTrash选项将绕过垃圾桶（如果已启用），并立即删除指定的文件。当需要从超配额目录中删除文件时，这非常有用。 -safely选项在删除目录之前需要安全确认，文件总数大于hadoop.shell.delete.limit.num.files（在core-site.xml中，默认值：100）。它可以与-skipTrash一起使用，以防止意外删除大目录。当递归地遍历大目录以计算在确认之前要删除的文件的数量时，预期延迟。 例： hadoop fs -rm hdfs：//nn.example.com/file / user / hadoop / emptydir 退出代码： 成功时返回0，错误时返回-1。 rmdir用法：hadoop fs -rmdir [--ignore-fail-on-non-empty] URI [URI ...] 删除目录。 选项： --ignore-fail-on-non-empty：使用通配符时，如果目录仍包含文件，请不要失败。 例： hadoop fs -rmdir / user / hadoop / emptydir rmr用法：hadoop fs -rmr [-skipTrash] URI [URI ...] 删除的递归版本。 注意：不推荐使用此命令。而是使用hadoop fs -rm -r setfacl用法：hadoop fs -setfacl [-R] [-b | -k -m | -x &lt;acl_spec&gt; &lt;path&gt;] | [ - set &lt;acl_spec&gt; &lt;path&gt;] 设置文件和目录的访问控制列表（ACL）。 选项： -b：删除除基本ACL条目之外的所有条目。保留用户，组和其他条目以与权限位兼容。 -k：删除默认ACL。 -R：递归地对所有文件和目录应用操作。 -m：修改ACL。新条目将添加到ACL，并保留现有条目。 -x：删除指定的ACL条目。保留其他ACL条目。 --set：完全替换ACL，丢弃所有现有条目。所述acl_spec必须包括用户，组条目和其他用于与权限位兼容性。 acl_spec：以逗号分隔的ACL条目列表。 path：要修改的文件或目录。 例子： hadoop fs -setfacl -m user：hadoop：rw- / file hadoop fs -setfacl -x user：hadoop / file hadoop fs -setfacl -b / file hadoop fs -setfacl -k / dir hadoop fs -setfacl --set user :: rw-，user：hadoop：rw-，group :: r - ，other :: r-- / file hadoop fs -setfacl -R -m user：hadoop：rx / dir hadoop fs -setfacl -m default：user：hadoop：rx / dir 退出代码： 成功时返回0，错误时返回非零。 setfattr用法：hadoop fs -setfattr -n name [-v value] | -x name &lt;path&gt; 设置文件或目录的扩展属性名称和值。 选项： -n name：扩展属性名称。 -v value：扩展属性值。该值有三种不同的编码方法。如果参数用双引号括起来，那么值就是引号内的字符串。如果参数的前缀为0x或0X，则将其视为十六进制数。如果参数以0或0S开头，则将其视为base64编码。 -x name：删除扩展属性。 path：文件或目录。 例子： hadoop fs -setfattr -n user.myAttr -v myValue / file hadoop fs -setfattr -n user.noValue / file hadoop fs -setfattr -x user.myAttr / file 退出代码： 成功时返回0，错误时返回非零。 setrep用法：hadoop fs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt; 更改文件的复制因子。如果path是目录，则命令以递归方式更改以path为根的目录树下的所有文件的复制因子。 选项： -w标志请求命令等待复制完成。这可能需要很长时间。 接受-R标志是为了向后兼容。它没有效果。 例： hadoop fs -setrep -w 3 / user / hadoop / dir1 退出代码： 成功时返回0，错误时返回-1。 stat用法：hadoop fs -stat [格式] &lt;路径&gt; ... 以指定格式打印有关的文件/目录的统计信息。格式接受八进制（％a）和符号（％A），文件大小（字节）（％b），类型（％F），所有者组名（％g），名称（％n），块大小（％o）的权限），复制（％r），所有者的用户名（％u），访问日期（％x，％X）和修改日期（％y，％Y）。％x和％y将UTC日期显示为“yyyy-MM-dd HH：mm：ss”，％X和％Y显示自1970年1月1日UTC以来的毫秒数。如果未指定格式，则默认使用％y。 例： hadoop fs -stat“type：％F perm：％a％u：％g size：％b mtime：％y atime：％x name：％n”/ file 退出代码：成功时返回0，错误时返回-1。 tail用法：hadoop fs -tail [-f] URI 显示文件的最后一千字节到stdout。 选项： -f选项将在文件增长时输出附加数据，如在Unix中一样。 例： hadoop fs -tail路径名 退出代码：成功时返回0，错误时返回-1。 test用法：hadoop fs -test - [defsz] URI 选项： -d：f路径是目录，返回0。 -e：如果路径存在，则返回0。 -f：如果路径是文件，则返回0。 -s：如果路径不为空，则返回0。 -r：如果路径存在且授予读权限，则返回0。 -w：如果路径存在且授予写入权限，则返回0。 -z：如果文件长度为零，则返回0。 例： hadoop fs -test -e filename text用法：hadoop fs -text &lt;src&gt; 获取源文件并以文本格式输出文件。允许的格式是zip和TextRecordInputStream。 touchz用法：hadoop fs -touchz URI [URI ...] 创建一个零长度的文件。如果文件存在非零长度，则返回错误。 例： hadoop fs -touchz pathname 退出代码：成功时返回0，错误时返回-1。 truncate用法：hadoop fs -truncate [-w] &lt;length&gt; &lt;paths&gt; 将与指定文件模式匹配的所有文件截断为指定的长度。 选项： 该-w标志的要求，对块恢复命令如有必要，等待完成。如果没有-w标志，则在恢复过程中文件可能会保持一段时间不闭合。在此期间，无法重新打开文件以进行追加。 例： hadoop fs -truncate 55 / user / hadoop / file1 / user / hadoop / file2 hadoop fs -truncate -w 127 hdfs：//nn1.example.com/user/hadoop/file1 usage用法：hadoop fs -usage命令 返回单个命令的帮助。 使用对象存储Hadoop FileSystem shell可与Object Stores（如Amazon S3，Azure WASB和OpenStack Swift）配合使用。 12345678＃创建一个目录hadoop fs -mkdir s3a：// bucket / datasets /＃从群集文件系统上传文件hadoop fs -put /datasets/example.orc s3a：// bucket / datasets /＃触摸文件hadoop fs -touchz wasb：//yourcontainer@youraccount.blob.core.windows.net/touched 与普通文件系统不同，重命名对象存储中的文件和目录通常需要与被操作对象的大小成比例的时间。由于许多文件系统shell操作使用重命名作为操作的最后阶段，因此跳过该阶段可以避免长时间的延迟。 特别是，put和copyFromLocal命令都应该为直接上载设置-d选项。 12345678910# Upload a file from the cluster filesystemhadoop fs -put -d /datasets/example.orc s3a://bucket/datasets/# Upload a file from under the user's home directory in the local filesystem.# Note it is the shell expanding the "~", not the hadoop fs commandhadoop fs -copyFromLocal -d -f ~/datasets/devices.orc s3a://bucket/datasets/# create a file from stdin# the special "-" source means "use stdin"echo "hello" | hadoop fs -put -d -f - wasb://yourcontainer@youraccount.blob.core.windows.net/hello.txt 可以下载和查看对象： 1234567891011121314＃将目录复制到本地文件系统hadoop fs -copyToLocal s3a：// bucket / datasets /＃将文件从对象库复制到集群文件系统。hadoop fs -get wasb：//yourcontainer@youraccount.blob.core.windows.net/hello.txt / examples＃打印对象hadoop fs -cat wasb：//yourcontainer@youraccount.blob.core.windows.net/hello.txt＃打印对象，必要时解压缩hadoop fs -text wasb：//yourcontainer@youraccount.blob.core.windows.net/hello.txt##将日志文件下载到本地文件中hadoop fs -getmerge wasb：//yourcontainer@youraccount.blob.core.windows.net/logs \ * log.txt 列出许多文件的命令往往比使用HDFS或其他文件系统时慢得多 12hadoop fs -count s3a://bucket/hadoop fs -du s3a://bucket/ 其他慢速命令包括find，mv，cp和rm。 Find在提供路径下有许多目录的大型商店中，这可能会非常慢。 12345# enumerate all files in the object store's container.hadoop fs -find s3a://bucket/ -print# remember to escape the wildcards to stop the shell trying to expand them firsthadoop fs -find s3a://bucket/datasets/ -name \*.txt -print Rename重命名文件的时间取决于其大小。 重命名目录的时间取决于该目录下所有文件的数量和大小。 1hadoop fs -mv s3a：// bucket / datasets s3a：// bucket / historical 如果操作中断，则对象存储将处于未定义状态。 Copy1hadoop fs -cp s3a://bucket/datasets s3a://bucket/historical 复制操作读取每个文件，然后将其写回对象存储区; 完成的时间取决于要复制的数据量，以及本地计算机和对象存储库之间双向带宽。 计算机离对象存储器越远，复制所用的时间越长 删除对象该RM命令删除对象和目录满对象。如果对象存储最终是一致的，则fs ls命令和其他访问器可能会暂时返回现在删除的对象的详细信息; 这是对象存储的工件，无法避免。 如果文件系统客户端配置为将文件复制到废纸篓目录，则该文件系统将位于存储桶中; 然后，rm操作将花费与数据大小成比例的时间。此外，删除的文件将继续产生存储成本。 要避免这种情况，请使用-skipTrash选项。 1hadoop fs -rm -skipTrash s3a：// bucket / dataset 可以使用expunge命令清除移动到.Trash目录的数据。由于此命令仅适用于默认文件系统，因此必须将其配置为使默认文件系统成为目标对象库。 1hadoop fs -expunge -D fs.defaultFS = s3a：// bucket / 覆盖对象如果对象存储最终是一致的，则任何覆盖现有对象的操作可能不会立即对所有客户端/查询可见。即：稍后查询相同对象的状态或内容的操作可以获得前一个对象。在读取单个对象时，这有时可以在同一客户端中显示。 避免使用一系列覆盖对象的命令，然后立即处理更新的数据; 存在以下风险：将使用先前的数据。 时间戳对象存储中对象和目录的时间戳可能不遵循HDFS中文件和目录的行为。 对象的创建和初始修改时间将是它在对象库上创建的时间; 这将是写入过程的结束，而不是开始。 时间戳将取自对象存储基础架构的时钟，而不是客户端的时钟。 如果覆盖了对象，则将更新修改时间。 目录可能有也可能没有有效的时间戳。当更新下面的对象时，他们不太可能更新修改时间。 该atime的访问时间特征不被任何在Apache Hadoop的代码库中找到的对象存储的支持。 有关这可能如何影响distcp -update操作的详细信息，请参阅DistCp文档。 安全模型和操作对象存储的安全性和权限模型通常与Unix风格的文件系统非常不同; 查询或操纵权限的操作通常不受支持。 适用的操作包括：chgrp，chmod，chown，getfacl和setfacl。相关属性命令getfattr和setfattr通常也不可用。 列出权限和用户/组详细信息的文件系统命令通常模拟这些详细信息。 尝试保留权限的操作（例如fs -put -p）不会因此原因保留权限。（特例：wasb：//，它保留权限但不强制执行）。 当与只读对象存储交互时，“list”和“stat”命令中的权限可以指示用户具有写访问权限，而实际上他们没有。 对象存储通常具有自己的权限模型，模型可以通过特定于商店的工具进行操作。请注意，对象存储可能提供的某些权限（例如只写路径或根路径上的不同权限）可能与Hadoop文件系统客户端不兼容。这些往往需要对它们写入数据的整个对象存储桶/容器进行完全读写访问。 作为如何模拟权限的示例，这里是亚马逊的公共，只读桶Landsat图像的列表： 123456789101112$ hadoop fs -ls s3a://landsat-pds/Found 10 itemsdrwxrwxrwx - mapred 0 2016-09-26 12:16 s3a://landsat-pds/L8-rw-rw-rw- 1 mapred 23764 2015-01-28 18:13 s3a://landsat-pds/index.htmldrwxrwxrwx - mapred 0 2016-09-26 12:16 s3a://landsat-pds/landsat-pds_stats-rw-rw-rw- 1 mapred 105 2016-08-19 18:12 s3a://landsat-pds/robots.txt-rw-rw-rw- 1 mapred 38 2016-09-26 12:16 s3a://landsat-pds/run_info.jsondrwxrwxrwx - mapred 0 2016-09-26 12:16 s3a://landsat-pds/runs-rw-rw-rw- 1 mapred 27458808 2016-09-26 12:16 s3a://landsat-pds/scene_list.gzdrwxrwxrwx - mapred 0 2016-09-26 12:16 s3a://landsat-pds/tarqdrwxrwxrwx - mapred 0 2016-09-26 12:16 s3a://landsat-pds/tarq_corruptdrwxrwxrwx - mapred 0 2016-09-26 12:16 s3a://landsat-pds/test 所有文件都列为具有完全读/写权限。 所有目录似乎都具有完整的rwx权限。 所有文件的复制计数为“1”。 所有文件和目录的所有者被声明为当前用户（mapred）。 所有目录的时间戳实际上是执行-ls操作的时间戳。这是因为这些目录不是商店中的实际对象; 它们是基于路径下对象存在的模拟目录。 当尝试删除其中一个文件时，操作失败 - 尽管ls命令显示的权限： 12345$ hadoop fs -rm s3a://landsat-pds/scene_list.gzrm: s3a://landsat-pds/scene_list.gz: delete on s3a://landsat-pds/scene_list.gz: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 1EF98D5957BCAB3D), S3 Extended Request ID: wi3veOXFuFqWBUCJgV3Z+NQVj9gWgZVdXlPU4KBbYMsw/gA+hyhRXcaQ+PogOsDgHh31HlTCebQ= 这表明列出的权限不能作为写访问的证据; 只有对象操作才能确定这一点。 请注意，Microsoft Azure WASB文件系统允许设置和检查权限，但实际上并未强制实施权限。此功能提供了使用DistCp备份HDFS目录树的功能，其权限得以保留，权限可在将目录复制回HDFS时恢复。但是，为了保护对对象存储中数据的访问，必须使用 Azure 自己的模型和工具。 Commands of limited value以下是通常没有效果的shell命令列表 - 实际上可能会失败。 command limitations appendToFile generally unsupported checksum the usual checksum is “NONE” chgrp generally unsupported permissions model; no-op chmod generally unsupported permissions model; no-op chown generally unsupported permissions model; no-op createSnapshot generally unsupported deleteSnapshot generally unsupported df default values are normally displayed getfacl may or may not be supported getfattr generally supported renameSnapshot generally unsupported setfacl generally unsupported permissions model setfattr generally unsupported permissions model setrep has no effect truncate generally unsupported 不同的对象存储客户端可能支持这些命令：请查阅文档并针对目标存储进行测试。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群设置]]></title>
    <url>%2FHadoop%E9%9B%86%E7%BE%A4%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[目的本文档描述了如何安装和配置Hadoop集群，范围从几个节点到具有数千个节点的极大集群。要使用Hadoop，您可能首先要将其安装在一台计算机上（请参阅单节点设置）。 本文档不包括安全性或高可用性等高级主题。 先决条件 安装Java。有关已知的好版本，请参阅Hadoop Wiki。 从Apache镜像下载稳定版本的Hadoop。 安装安装Hadoop集群通常涉及在集群中的所有计算机上解压缩软件，或者通过适合您的操作系统的打包系统进行安装。将硬件划分为几个功能非常重要。 通常，群集中的一台计算机被指定为NameNode，而一台计算机则被指定为ResourceManager。这些都是masters。其他服务（例如Web App Proxy Server和MapReduce Job History server）通常在专用硬件或共享基础架构上运行，具体取决于负载。 集群中的其余计算机充当DataNode和NodeManager。这些是slaves。 在非安全模式下配置HadoopHadoop的 Java 配置由两种类型的重要配置文件驱动： 只读默认配置 - core-default.xml, hdfs-default.xml, yarn-default.xml 和mapred-default.xml. 具体站点的配置- etc/hadoop/core-site.xml, etc/hadoop/hdfs-site.xml, etc/hadoop/yarn-site.xml 和etc/hadoop/mapred-site.xml. 此外，您可以通过etc/hadoop/hadoop-env.sh和etc/hadoop/yarn-env.sh设置特定于站点的值来控制分发的 bin/ 目录中的Hadoop脚本。 要配置Hadoop集群，您需要配置Hadoop后台进程执行的environment以及Hadoop后台进程的configuration parameters。 HDFS后台进程是 NameNode，SecondaryNameNode 和 DataNode。YARN后台进程是ResourceManager，NodeManager和WebAppProxy。如果要使用MapReduce，则MapReduce Job History Server也将运行。对于大型安装，这些通常运行在不同的主机上。 Hadoop的环境配置管理员应该使用etc/hadoop/hadoop-env.sh和etc/hadoop/mapred-env.sh以及etc/hadoop/yarn-env.sh脚本来对Hadoop环境进行特定于站点的自定义。 至少，您必须指定JAVA_HOME，以便在每个远程节点上正确定义它。 管理员可以使用下表中显示的配置选项配置各个后台进程： Daemon Environment Variable NameNode HADOOP_NAMENODE_OPTS DataNode HADOOP_DATANODE_OPTS Secondary NameNode HADOOP_SECONDARYNAMENODE_OPTS ResourceManager YARN_RESOURCEMANAGER_OPTS NodeManager YARN_NODEMANAGER_OPTS WebAppProxy YARN_PROXYSERVER_OPTS Map Reduce Job History Server HADOOP_JOB_HISTORYSERVER_OPTS 例如，要将NameNode配置为使用parallelGC，应在hadoop-env.sh中添加以下语句： 1export HADOOP_NAMENODE_OPTS="-XX:+UseParallelGC" 有关其他示例，请参阅 etc/hadoop/hadoop-env.sh 您可以自定义的其他有用配置参数包括： HADOOP_PID_DIR - 存储后台进程的进程标识文件的目录。 HADOOP_LOG_DIR - 存储后台程序日志文件的目录。如果日志文件不存在，则会自动创建日志文件。 HADOOP_HEAPSIZE / YARN_HEAPSIZE - 要使用的最大堆大小，以MB为单位，例如，如果varibale设置为1000，则堆将设置为1000MB。这用于配置守护程序的堆大小。默认情况下，该值为1000.如果要为每个可以使用的守护程序单独配置值。 在大多数情况下，您应该指定HADOOP_PID_DIR和HADOOP_LOG_DIR目录，以便它们只能由将要运行hadoop守护程序的用户写入。否则就有可能发生符号链接攻击。 在系统范围的shell环境配置中配置HADOOP_PREFIX也是传统的。例如，在/etc/profile.d中有一个简单的脚本： 12HADOOP_PREFIX=/path/to/hadoopexport HADOOP_PREFIX Daemon Environment Variable ResourceManager YARN_RESOURCEMANAGER_HEAPSIZE NodeManager YARN_NODEMANAGER_HEAPSIZE WebAppProxy YARN_PROXYSERVER_HEAPSIZE Map Reduce Job History Server HADOOP_JOB_HISTORYSERVER_HEAPSIZE 配置Hadoop本节介绍在给定配置文件中指定的重要参数： etc/hadoop/core-site.xml Parameter Value Notes fs.defaultFS NameNode URI hdfs://host:port/ io.file.buffer.size 131072 SequenceFiles中使用的读/写缓冲区的大小。 etc/hadoop/hdfs-site.xml NameNode的配置： Parameter Value Notes dfs.namenode.name.dir NameNode存储名称空间和持续事务日志的本地文件系统上的路径。 如果这是一个以逗号分隔的目录列表，那么名称表将复制到所有目录中，以实现冗余。 dfs.hosts / dfs.hosts.exclude List of permitted/excluded DataNodes. If necessary, use these files to control the list of allowable datanodes. dfs.blocksize 268435456 HDFS blocksize of 256MB for large file-systems. dfs.namenode.handler.count 100 More NameNode server threads to handle RPCs from large number of DataNodes. Configurations for DataNode: Parameter Value Notes dfs.datanode.data.dir Comma separated list of paths on the local filesystem of a DataNode where it should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. etc/hadoop/yarn-site.xml ResourceManager和NodeManager的配置： Parameter Value Notes yarn.acl.enable true / false Enable ACLs? Defaults to false. yarn.admin.acl Admin ACL ACL to set admins on the cluster. ACLs are of for comma-separated-usersspacecomma-separated-groups. Defaults to special value of * which means anyone. Special value of just space means no one has access. yarn.log-aggregation-enable false Configuration to enable or disable log aggregation ResourceManager的配置： Parameter Value Notes yarn.resourcemanager.address ResourceManagerhost:port for clients to submit jobs. host:port If set, overrides the hostname set in yarn.resourcemanager.hostname. yarn.resourcemanager.scheduler.address ResourceManagerhost:port for ApplicationMasters to talk to Scheduler to obtain resources. host:port If set, overrides the hostname set in yarn.resourcemanager.hostname. yarn.resourcemanager.resource-tracker.address ResourceManagerhost:port for NodeManagers. host:port If set, overrides the hostname set in yarn.resourcemanager.hostname. yarn.resourcemanager.admin.address ResourceManagerhost:port for administrative commands. host:port If set, overrides the hostname set in yarn.resourcemanager.hostname. yarn.resourcemanager.webapp.address ResourceManagerweb-ui host:port. host:port If set, overrides the hostname set in yarn.resourcemanager.hostname. yarn.resourcemanager.hostname ResourceManagerhost. host Single hostname that can be set in place of setting all yarn.resourcemanager*address resources. Results in default ports for ResourceManager components. yarn.resourcemanager.scheduler.class ResourceManagerScheduler class. CapacityScheduler (recommended), FairScheduler (also recommended), or FifoScheduler. Use a fully qualified class name, e.g., org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler. yarn.scheduler.minimum-allocation-mb Minimum limit of memory to allocate to each container request at the Resource Manager. In MBs yarn.scheduler.maximum-allocation-mb Maximum limit of memory to allocate to each container request at the Resource Manager. In MBs yarn.resourcemanager.nodes.include-path / yarn.resourcemanager.nodes.exclude-path List of permitted/excluded NodeManagers. If necessary, use these files to control the list of allowable NodeManagers. NodeManager的配置： Parameter Value Notes yarn.nodemanager.resource.memory-mb Resource i.e. available physical memory, in MB, for given NodeManager Defines total available resources on the NodeManager to be made available to running containers yarn.nodemanager.vmem-pmem-ratio Maximum ratio by which virtual memory usage of tasks may exceed physical memory The virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio. yarn.nodemanager.local-dirs Comma-separated list of paths on the local filesystem where intermediate data is written. Multiple paths help spread disk i/o. yarn.nodemanager.log-dirs Comma-separated list of paths on the local filesystem where logs are written. Multiple paths help spread disk i/o. yarn.nodemanager.log.retain-seconds 10800 Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled. yarn.nodemanager.remote-app-log-dir /logs HDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled. yarn.nodemanager.remote-app-log-dir-suffix logs Suffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled. yarn.nodemanager.aux-services mapreduce_shuffle Shuffle service that needs to be set for Map Reduce applications. Configurations for History Server (Needs to be moved elsewhere): Parameter Value Notes yarn.log-aggregation.retain-seconds -1 How long to keep aggregation logs before deleting them. -1 disables. Be careful, set this too small and you will spam the name node. yarn.log-aggregation.retain-check-interval-seconds -1 Time between checks for aggregated log retention. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful, set this too small and you will spam the name node. etc/hadoop/mapred-site.xml MapReduce应用程序的配置： Parameter Value Notes mapreduce.framework.name yarn Execution framework set to Hadoop YARN. mapreduce.map.memory.mb 1536 Larger resource limit for maps. mapreduce.map.java.opts -Xmx1024M Larger heap-size for child jvms of maps. mapreduce.reduce.memory.mb 3072 Larger resource limit for reduces. mapreduce.reduce.java.opts -Xmx2560M Larger heap-size for child jvms of reduces. mapreduce.task.io.sort.mb 512 Higher memory-limit while sorting data for efficiency. mapreduce.task.io.sort.factor 100 More streams merged at once while sorting files. mapreduce.reduce.shuffle.parallelcopies 50 Higher number of parallel copies run by reduces to fetch outputs from very large number of maps. Configurations for MapReduce JobHistory Server: Parameter Value Notes mapreduce.jobhistory.address MapReduce JobHistory Server host:port Default port is 10020. mapreduce.jobhistory.webapp.address MapReduce JobHistory Server Web UI host:port Default port is 19888. mapreduce.jobhistory.intermediate-done-dir /mr-history/tmp Directory where history files are written by MapReduce jobs. mapreduce.jobhistory.done-dir /mr-history/done Directory where history files are managed by the MR JobHistory Server. 监控NodeManager的健康状况Hadoop提供了一种机制，管理员可以通过该机制定期运行管理员提供的脚本以确定节点是否健康。 管理员可以通过在脚本中执行对其选择的任何检查来确定节点是否处于正常状态。如果脚本检测到节点处于不健康状态，则必须以字符串ERROR开头的标准输出行。NodeManager定期生成脚本并检查其输出。如果脚本的输出包含字符串ERROR，如上所述，节点的状态将报告为运行状况不佳并且ResourceManager将节点列入黑名单。不会为此节点分配其他任务。但是，NodeManager继续运行脚本，因此如果节点再次变得健康，它将自动从ResourceManager上的黑名单节点中删除。如果节点不健康，则可以在ResourceManager Web界面中为管理员提供节点的运行状况以及脚本的输出。自节点健康以来的时间也显示在Web界面上。 以下参数可用于控制etc / hadoop / yarn-site.xml中的节点运行状况监视脚本。 Parameter Value Notes yarn.nodemanager.health-checker.script.path Node health script Script to check for node’s health status. yarn.nodemanager.health-checker.script.opts Node health script options Options for script to check for node’s health status. yarn.nodemanager.health-checker.interval-ms Node health script interval Time interval for running health script. yarn.nodemanager.health-checker.script.timeout-ms Node health script timeout interval Timeout for health script execution. 如果只有部分本地磁盘变坏，则运行状况检查程序脚本不应该给出错误。NodeManager能够定期检查本地磁盘的运行状况（具体检查nodemanager-local-dirs和nodemanager-log-dirs），并在达到配置属性yarn.nodemanager设置的错误目录数阈值后.disk-health-checker.min-healthy-disks，整个节点被标记为运行状况不佳，此信息也会发送给资源管理器。引导磁盘被突袭或健康检查程序脚本识别引导磁盘中的故障。 Slaves 文件列出etc / hadoop / slaves文件中的所有从属主机名或IP地址，每行一个。Helper脚本（如下所述）将使用etc / hadoop / slaves文件一次在多个主机上运行命令。它不用于任何基于Java的Hadoop配置。为了使用此功能，必须为用于运行Hadoop的帐户建立ssh信任（通过无密码ssh或其他方式，如Kerberos）。 Hadoop机架意识许多Hadoop组件都具有机架感知功能，并利用网络拓扑结构提高性能和安全性。Hadoop守护程序通过调用管理员配置的模块来获取集群中从站的机架信息。有关更多具体信息，请参阅Rack Awareness文档。 强烈建议在启动HDFS之前配置机架感知。 记录Hadoop 通过Apache Commons Logging框架使用Apache log4j进行日志记录。编辑etc / hadoop / log4j.properties文件以自定义Hadoop守护程序的日志记录配置（日志格式等）。 操作Hadoop集群完成所有必要的配置后，将文件分发到所有计算机上的HADOOP_CONF_DIR目录。这应该是所有计算机上的相同目录。 通常，建议HDFS和YARN作为单独的用户运行。在大多数安装中，HDFS进程以’hdfs’的形式执行。YARN通常使用’yarn’帐户。、 Hadoop启动要启动Hadoop集群，您需要启动HDFS和YARN集群。 第一次启动HDFS时，必须对其进行格式化。将新的分布式文件系统格式化为hdfs： 1[hdfs]$ $HADOOP_PREFIX/bin/hdfs namenode -format &lt;cluster_name&gt; 在指定节点上使用以下命令以hdfs启动HDFS NameNode ： 1[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode 使用以下命令在每个指定节点上以hdfs启动HDFS DataNode ： 1[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs start datanode 如果配置了etc/hadoop/slaves和ssh trusted access（请参阅单节点设置），则可以使用实用程序脚本启动所有HDFS进程。作为hdfs： 1[hdfs]$ $HADOOP_PREFIX/sbin/start-dfs.sh 使用以下命令启动YARN，在指定的ResourceManager上以yarn形式运行： 1[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager 运行脚本以在每个指定的主机上启动NodeManager作为yarn： 1[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR start nodemanager 启动独立的WebAppProxy服务器。以纱线形式在WebAppProxy服务器上运行。如果使用多个服务器进行负载平衡，则应在每个服务器上运行： 1[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start proxyserver 如果配置了etc/hadoop/slaves和ssh trusted access（请参阅单节点设置），则可以使用实用程序脚本启动所有YARN进程。作为 As yarn: 1[yarn]$ $HADOOP_PREFIX/sbin/start-yarn.sh 使用以下命令启动MapReduce JobHistory Server，在指定的服务器上以mapred运行： 1[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver Hadoop关闭使用以下命令停止NameNode，在指定的NameNode上运行为hdfs： 1[hdfs] $ $ HADOOP_PREFIX / sbin / hadoop-daemon.sh --config $ HADOOP_CONF_DIR --script hdfs stop namenode 运行脚本以将DataNode作为hdfs停止： 1[hdfs] $ $ HADOOP_PREFIX / sbin / hadoop-daemons.sh --config $ HADOOP_CONF_DIR --script hdfs stop datanode 如果配置了etc / hadoop / slaves和ssh trusted access（请参阅单节点设置），则可以使用实用程序脚本停止所有HDFS进程。作为hdfs： 1[hdfs] $ $ HADOOP_PREFIX / sbin / stop-dfs.sh 使用以下命令停止ResourceManager，在指定的ResourceManager上作为yarn运行： 1[yarn] $ $ HADOOP_YARN_HOME / sbin / yarn-daemon.sh --config $ HADOOP_CONF_DIR stop resourcemanager 运行脚本以将从站上的NodeManager作为yarn停止： 1[yarn] $ $ HADOOP_YARN_HOME / sbin / yarn-daemons.sh --config $ HADOOP_CONF_DIR stop nodemanager 如果配置了etc / hadoop / slaves和ssh trusted access（请参阅单节点设置），则可以使用实用程序脚本停止所有YARN进程。As hdfs: 1[yarn] $ $ HADOOP_PREFIX / sbin / stop-yarn.sh 停止WebAppProxy服务器。以纱线形式在WebAppProxy服务器上运行。如果使用多个服务器进行负载平衡，则应在每个服务器上运行： 1[yarn] $ $ HADOOP_YARN_HOME / sbin / yarn-daemon.sh --config $ HADOOP_CONF_DIR stop proxyserver 使用以下命令停止MapReduce JobHistory Server，在指定的服务器上以mapred运行： 1[mapred] $ $ HADOOP_PREFIX / sbin / mr-jobhistory-daemon.sh --config $ HADOOP_CONF_DIR stop historyserver Web界面一旦Hadoop集群启动并运行，请检查组件的web-ui，如下所述： Daemon Web Interface Notes NameNode http://nn_host:port/ Default HTTP port is 50070. ResourceManager http://rm_host:port/ Default HTTP port is 8088. MapReduce JobHistory Server http://jhs_host:port/ Default HTTP port is 19888.]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop: 设置单个节点集群]]></title>
    <url>%2FHadoop-%E8%AE%BE%E7%BD%AE%E5%8D%95%E4%B8%AA%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目的本文档介绍如何设置和配置单节点Hadoop安装，以便您可以使用Hadoop MapReduce和Hadoop分布式文件系统（HDFS）快速执行简单操作。 先决条件支持的平台 支持GNU / Linux作为开发和生产平台。已经在具有2000个节点的GNU / Linux集群上演示了Hadoop。 Windows也是受支持的平台，但以下步骤仅适用于Linux。要在Windows上设置Hadoop，请参阅Wiki页面。 必备软件Linux所需的软件包括： 必须安装Java™。HadoopJavaVersions描述了推荐的Java版本。 必须安装ssh并且必须运行sshd才能使用管理远程Hadoop守护程序的Hadoop脚本。 安装软件如果您的群集没有必需的软件，则需要安装它。 例如在Ubuntu Linux上： 12$ sudo apt-get install ssh$ sudo apt-get install rsync 下载要获得Hadoop发行版，请从其中一个Apache下载镜像下载最新的稳定版本。 准备启动Hadoop集群解压缩下载的Hadoop发行版。在分发中，编辑文件etc / hadoop / hadoop-env.sh以定义一些参数，如下所示： 12＃设置为Java安装的根目录export JAVA_HOME = / usr / java / latest 请尝试以下命令： 1$ bin / hadoop 这将显示hadoop脚本的使用文档。 现在，您已准备好以三种支持模式之一启动Hadoop集群： 本地（独立）模式 伪分布式模式 完全分布式模式 独立操作默认情况下，Hadoop配置为以非分布式模式运行，作为单个Java进程。这对调试很有用。 以下示例复制解压缩的conf目录以用作输入，然后查找并显示给定正则表达式的每个匹配项。输出将写入给定的输出目录。 1234$ mkdir input$ cp etc/hadoop/*.xml input$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar grep input output 'dfs[a-z.]+'$ cat output/* 伪分布式操作Hadoop还可以在伪分布式模式下在单节点上运行，其中每个Hadoop后台进程在单独的Java进程中运行。 配置文件如下： etc/hadoop/core-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 设置passphraseless ssh现在检查您是否可以在没有密码的情况下ssh到localhost： 1$ ssh localhost 如果在没有密码短语的情况下无法ssh到localhost，请执行以下命令： 123$ ssh-keygen -t rsa -P'' - f~ / .ssh / id_rsa$ cat~ / .ssh / id_rsa.pub &gt;&gt;〜/ .ssh / authorized_keys$ chmod 0600~ / .ssh / authorized_keys 执行以下说明是在本地运行MapReduce作业。如果要在YARN上执行作业，请参阅单节点上的YARN 。 格式化文件系统： 1$ bin/hdfs namenode -format 启动NameNode守护程序和DataNode守护程序： 1$ sbin/start-dfs.sh hadoop后台进程日志输出将写入$ HADOOP_LOG_DIR目录（默认为$ HADOOP_HOME / logs）。 浏览NameNode的Web界面; 默认情况下，它可用于： NameNode - http：// localhost：50070 / 创建执行MapReduce作业所需的HDFS目录： 12$ bin/hdfs dfs -mkdir /user$ bin/hdfs dfs -mkdir /user/&lt;username&gt; 将输入文件复制到分布式文件系统中： 1$ bin/hdfs dfs -put etc/hadoop input 运行一些提供的示例： 1$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar grep input output 'dfs[a-z.]+' 检查输出文件：将输出文件从分布式文件系统复制到本地文件系统并检查它们： 12$ bin/hdfs dfs -get output output$ cat output/* 或者 查看分布式文件系统上的输出文件： 1$ bin/hdfs dfs -cat output/* 完成后，停止守护进程： 1$ sbin/stop-dfs.sh YARN在单个节点上您可以通过设置一些参数并运行ResourceManager守护程序和NodeManager守护程序，以伪分布式模式在YARN上运行MapReduce作业。 以下说明假定已执行上述指令的 1.~4步骤。 配置参数如下:etc/hadoop/mapred-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/yarn-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动ResourceManager守护程序和NodeManager守护程序： 1$ sbin/start-yarn.sh 浏览ResourceManager的Web界面; 默认情况下，它可用于： ResourceManager - http：// localhost：8088 / 运行MapReduce作业。 完成后，停止后台进程： 1$ sbin/stop-yarn.sh 全分布式操作有关设置完全分布式，非平凡群集的信息，请参阅群集设置。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop命令指南]]></title>
    <url>%2FHadoop%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[概述所有hadoop命令都由bin / hadoop脚本调用。不带任何参数运行hadoop脚本会打印所有命令的描述。 用法: hadoop [--config confdir] [--loglevel loglevel] [COMMAND] [GENERIC_OPTIONS] [COMMAND_OPTIONS] FIELD Description --config confdir Overwrites the default Configuration directory. Default is ${HADOOP_HOME}/conf. --loglevel loglevel Overwrites the log level. Valid log levels are FATAL, ERROR, WARN, INFO, DEBUG, and TRACE. Default is INFO. GENERIC_OPTIONS The common set of options supported by multiple commands. COMMAND_OPTIONS Various commands with their options are described in this documention for the Hadoop common sub-project. HDFS and YARN are covered in other documents. 通用选项许多子命令都支持一组通用的配置选项来改变它们的行为： GENERIC_OPTION Description -archives &lt;comma separated list of archives&gt; Specify comma separated archives to be unarchived on the compute machines. Applies only to job. -conf &lt;configuration file&gt; Specify an application configuration file. -D &lt;property&gt;=&lt;value&gt; Use value for given property. -files &lt;comma separated list of files&gt; Specify comma separated files to be copied to the map reduce cluster. Applies only to job. -fs &lt;file:///&gt; or &lt;hdfs://namenode:port&gt; Specify default filesystem URL to use. Overrides ‘fs.defaultFS’ property from configurations. -jt &lt;local&gt; or &lt;resourcemanager:port&gt; Specify a ResourceManager. Applies only to job. -libjars &lt;comma seperated list of jars&gt; Specify comma separated jar files to include in the classpath. Applies only to job. Hadoop常用命令所有这些命令都是从hadoop shell命令执行的。它们已分解为用户命令和管理命令。 用户命令对hadoop集群的用户有用的命令。 档案创建一个hadoop存档。有关更多信息，请参阅Hadoop Archives Guide。 checknative用法：hadoop checknative [-a] [-h] COMMAND_OPTION Description -a Check all libraries are available. -h print help 此命令检查Hadoop本机代码的可用性。有关更多信息，请参阅Native Libaries。默认情况下，此命令仅检查libhadoop的可用性。 classpathUsage: hadoop classpath [--glob |--jar &lt;path&gt; |-h |--help] COMMAND_OPTION Description --glob expand wildcards --jar path write classpath as manifest in jar named path -h, --help print help 打印获取Hadoop jar和所需库所需的类路径。如果不带参数调用，则打印由命令脚本设置的类路径，该脚本可能在类路径条目中包含通配符。其他选项在通配符扩展后打印类路径，或将类路径写入jar文件的清单中。后者在无法使用通配符且扩展类路径超过支持的最大命令行长度的环境中非常有用。 凭据用法: hadoop credential &lt;subcommand&gt; [options] COMMAND_OPTION Description create alias [-provider provider-path] [-strict] [-value credential-value] 提示用户将凭据存储为给定别名。所述hadoop.security.credential.provider.path芯site.xml文件内将被使用，除非一个-provider被指示。该-strict标志将导致如果提供商使用默认密码的命令失败。使用-value标志提供凭据值（也就是别名密码）而不是提示 delete alias [-provider provider-path] [-strict] [-f] 使用提供的别名删除凭据。所述hadoop.security.credential.provider.path芯site.xml文件内将被使用，除非一个-provider被指示。该-strict标志将导致如果提供商使用默认密码的命令失败。除非指定了-f，否则该命令会要求确认 list [-provider provider-path] [-strict] 列出所有的凭证的别名hadoop.security.credential.provider.path芯site.xml文件内将被使用，除非一个-provider被指示。该-strict标志将导致如果提供商使用默认密码的命令失败。 用于管理凭据提供程序中的凭据，密码和机密的命令。 Hadoop中的CredentialProvider API允许分离应用程序以及它们如何存储所需的密码/秘密。为了指示特定的提供程序类型和位置，用户必须在core-site.xml中提供hadoop.security.credential.provider.path配置元素，或者对以下每个命令使用命令行选项-provider。此提供程序路径是以逗号分隔的URL列表，用于指示应查阅的提供程序列表的类型和位置。例如，以下路径：user：///，jceks：//file/tmp/test.jceks,jceks：//hdfs@nn1.example.com/my/path/test.jceks 表示应通过用户提供程序查询当前用户的凭证文件，位于/tmp/test.jceks的本地文件是Java密钥库提供程序，该文件位于HDFS中的nn1.example.com/my/path/ test.jceks也是Java Keystore Provider的商店。 当使用凭证命令时，它通常用于向特定凭证存储提供商提供密码或秘密。为了明确指出要使用哪个提供者存储，应该使用-provider选项。否则，给定多个提供者的路径，将使用第一个非瞬态提供者。这可能是也可能不是你想要的那个。 提供商经常要求提供密码或其他秘密。如果提供程序需要密码而无法找到密码，则它将使用默认密码并发出警告消息，指出正在使用默认密码。如果提供了-strict标志，则警告消息将成为错误消息，并且该命令会立即返回错误状态。 示例：hadoop凭证列表-provider jceks：//file/tmp/test.jceks distcp递归复制文件或目录。有关详细信息，请参阅Hadoop DistCp指南。 fs“ 文件系统Shell指南”中介绍了此命令。当HDFS正在使用时，它是hdfs dfs的同义词。 jarUsage: hadoop jar &lt;jar&gt; [mainClass] args... Runs a jar file. Use yarn jar to launch YARN applications instead. keyUsage: hadoop key &lt;subcommand&gt; [options] COMMAND_OPTION Description create keyname [-cipher cipher] [-size size] [-description description] [-attr attribute=value] [-provider provider] [-strict] [-help] 创建由指定的名称一个新的密钥键名由指定的提供者中的说法-provider说法。该-strict标志将导致如果提供商使用默认密码的命令失败。您可以使用-cipher参数指定密码。默认密码当前为“AES / CTR / NoPadding”。默认密钥大小为128.您可以使用-size参数指定请求的密钥长度。可以使用-attr参数指定任意属性=值样式属性。-attr可以多次指定，每个属性一次。 roll keyname [-provider provider] [-strict] [-help] 使用-provider参数为指示的提供程序中的指定键创建新版本。该-strict标志将导致如果提供商使用默认密码的命令失败。 delete keyname [-provider provider] [-strict] [-f] [-help] 从-provider指定的提供程序中删除keyname参数指定的所有密钥版本。该-strict标志将导致如果提供商使用默认密码的命令失败。除非指定了-f，否则该命令会要求用户确认。 list [-provider provider] [-strict] [-metadata] [-help] 显示在core-site.xml中配置或使用-provider参数指定的特定提供程序中包含的键名。该-strict标志将导致如果提供商使用默认密码的命令失败。-metadata显示元数据。 -help 打印此命令的用法 通过KeyProvider管理密钥。有关KeyProviders的详细信息，请参阅“ 透明加密指南”。 提供商经常要求提供密码或其他秘密。如果提供程序需要密码而无法找到密码，则它将使用默认密码并发出警告消息，指出正在使用默认密码。如果提供了-strict标志，则警告消息将成为错误消息，并且该命令会立即返回错误状态。 注意：某些KeyProviders（例如org.apache.hadoop.crypto.key.JavaKeyStoreProvider）不支持大写键名称。 注意：某些KeyProviders不直接执行密钥删除（例如，执行软删除，或延迟实际删除，以防止错误）。在这些情况下，删除后创建/删除具有相同名称的密钥时可能会遇到错误。有关详细信息，请查看基础KeyProvider。 trace查看和修改Hadoop跟踪设置。请参阅“ 跟踪指南”。 version用法：hadoop版本 打印版本。 CLASSNAME用法：hadoop CLASSNAME 运行名为CLASSNAME的类。 envvars用法：hadoop envvars 显示计算的Hadoop环境变量。 管理命令对hadoop集群的管理员有用的命令。 daemonlogUsage: 12hadoop daemonlog -getlevel &lt;host:port&gt; &lt;classname&gt; [-protocol (http|https)]hdoop daemonlog -setlevel &lt;host:port&gt; &lt;classname&gt; &lt;level&gt; [-protocol (http|https)] COMMAND_OPTION Description -getlevel host:port classname[-protocol (http\ https)] 在host：port运行的守护程序中打印由限定类名标识的日志的日志级别。所述-protocol标志指定用于连接的协议。 -setlevel host:port classname**level [-protocol (http\ https)] 设置在host：port运行的守护程序中由限定类名标识的日志的日志级别。所述-protocol标志指定用于连接的协议。 获取/设置守护程序中由限定类名称标识的日志的日志级别。默认情况下，该命令发送HTTP请求，但可以使用参数-protocol https来覆盖此请求以发送HTTPS请求。 例： 12$ bin/hadoop daemonlog -setlevel 127.0.0.1:50070 org.apache.hadoop.hdfs.server.namenode.NameNode DEBUG$ bin/hadoop daemonlog -getlevel 127.0.0.1:50470 org.apache.hadoop.hdfs.server.namenode.NameNode DEBUG -protocol https 请注意，该设置不是永久性的，并且会在重新启动守护程序时重置。此命令通过向守护程序的内部Jetty servlet发送HTTP / HTTPS请求来工作，因此它支持以下守护程序： HDFS 名称节点 辅助名称节点 数据节点 期刊节点 YARN 资源经理 节点管理员 时间线服务器 但是，该命令不支持KMS服务器，因为它的Web界面基于Tomcat，后者不支持servlet。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS22n笔记01：Introduction to NLP and Deep Learning]]></title>
    <url>%2FCS22n%E7%AC%94%E8%AE%B001-Introduction-to-NLP-and-Deep-Learning%2F</url>
    <content type="text"><![CDATA[主要参考自http://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html 什么是自然语言处理自然语言处理是计算机科学、人工智能和语言学的交叉学科。虽然语言只是人工智能的一部分（人工智能还包括计算机视觉等），但它是非常独特的一部分。这个星球上有许多生物拥有超过人类的视觉系统，但只有人类才拥有这么高级的语言。 自然语言处理的目标是让计算机处理或说“理解”自然语言，以完成有意义的任务，比如订机票购物或QA等。完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。 自然语言处理涉及的几个层次 作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词（事实上，跳过分词虽然理所当然地不能做句法分析，但字符级也可以直接做不少应用）。接下来是形态学，援引《统计自然语言处理》中的定义： 形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科［Matthews,2000］。 下面的是句法分析和语义分析，最后面的在中文中似乎翻译做“语篇处理”，需要根据上文语境理解下文。 这门课主要关注画圈的三个部分，其中中间的两个是重中之重，虽然深度学习在语音识别上的发力最大。 NLP应用NLP中有不同级别的任务，从语音处理（speech processing）到语义解释（semantic interpretation）和语篇处理（ discourse processing）。 NLP的目标是设让计算机“理解”自然语言以执行某些任务的算法。 示例任务从简单到复杂有： Easy Spell Checking Keyword Search Finding Synonyms Medium Parsing information from websites, documents, etc. Hard Machine Translation (e.g. Translate Chinese text to English) Semantic Analysis (What is the meaning of query statement?) Coreference (e.g. What does “he” or “it” refer to given a document?) Question Answering (e.g. Answering Jeopardy questions). 在工业界从搜索到广告投放、自动\辅助翻译、情感舆情分析、语音识别、聊天机器人\管家等等五花八门。 人类语言的特殊之处人类（自然）语言有什么特别之处？ 人类语言是专门用于传达有意义的信息的，这种传输连小孩子都能很快学会（amazingly!）。人类语言是离散的、明确的符号系统。但又允许出现各种变种，比如颜文字，随意的错误拼写“I loooove it”。这种自由性可能是因为语言的可靠性。所以说语言文字绝对不是形式逻辑或传统AI的产物。 语言符号有多种形式（声音、手势、书写），在这些不同的形式中，其意义保持不变： 虽然人类语言是明确的符号系统，但符号传输到大脑的过程是通过连续的声学光学信号，大脑编码似乎是连续的激活值上的模式。另外巨大的词表也导致数据稀疏，不利于机器学习。这构成一种动机，是不是应该用连续的信号而不是离散的符号去处理语言。 什么是深度学习深度学习是机器学习的一个子集。传统机器学习中，人类需要对专业问题理解非常透彻，才能手工设计特征。比如地名和机构名识别的特征模板： 然后把特征交给某个机器学习算法，比如线性分类器。机器为这些特征调整找到合适的权值，将误差优化到最小。 下面这张图很好地展示了这个过程中的比例： 而深度学习是表示学习的一部分，用来学习原始输入的多层特征表示，输入的元数据可能是声音、字符、单词…… “深度学习”的历史虽然这个术语大部分时候指代利用各种各样多层的神经网络进行表示学习，有时候也有一些概率图模型参与。统计学家会说，哦，不过是一些逻辑斯谛回归单元的堆砌而已。也许的确如此，但这还是以偏概全的说法（电子计算机还是一堆半导体的堆砌呢，大脑还是一堆神经元的堆砌呢）。这门课不会回顾历史（像Hinton老爷子那样博古通今），而只会专注当前在NLP领域大放异彩的方法。 为什么需要研究深度学习 手工特征耗时耗力，还不易拓展 自动特征学习快，方便拓展 深度学习提供了一种通用的学习框架，可用来表示世界、视觉和语言学信息 深度学习既可以无监督学习，也可以监督学习 深度学习可追溯到八九十年代，但在2010年左右才崛起（最先是语音与图像，后来才是NLP），那之前为什么没有呢？ 与Hinton介绍的一样，无非是以前数据量不够，计算力太弱。当然，最近也的确有许多新模型，新算法。 Large amounts of training data favor deep learning Faster machines and multicore CPU/GPUs favor Deep Learning New models, algorithms, ideas• Better, more flexible learning of intermediate representations• Effective end-to-end joint system learning• Effective learning methods for using contexts and transferring between tasks 语音识别中的深度学习 深度学习上突破性的研究发生在语音识别领域，来自Hinton老爷子的学生，具体参考：http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html#h3-11 计算机视觉中的深度学习大多数深度学习的研究都集中在计算机视觉（至少到两年前） 突破性进展还是来自Hinton的学生。 ImageNet Classification with Deep Convolutional Neural Networks by Krizhevsky, Sutskever, &amp; Hinton, 2012, U. Toronto. 课程相关有4次编程练习，会用到TensorFlow。 为什么NLP难人类语言是充满歧义的，不像编程语言那样明确。编程语言中有各种变量名，但人类语言中只有少数几个代词可以用，你得思考到底指代的是谁…… 人类语言的解读依赖于现实世界、常识以及上下文。由于说话速度书写速度阅读速度的限制，人类语言非常简练，省略了大量背景知识。 接下来是几个英文的歧义例子，对native speaker而言很有趣。为了完整性只看一个： The Pope’s baby steps on gays 主要歧义发生在baby上面，可以理解为“教皇的孩子踩了基佬”，也可以理解为“教皇在同性恋问题上裹足不前”。 旧版CS224d里面还有个更直观的例子，推特上关于电影明星“海瑟薇”的评论影响了保险公司哈撒韦的股价，因为两者拼写是一样的。 说明某些“舆情系统”没做好命名实体识别。 Deep NLP = Deep Learning + NLP将自然语言处理的思想与表示学习结合起来，用深度学习的手法解决NLP目标。这提高了许多方面的效果： 层次：语音、词汇、语法、语义 工具：词性标注、命名实体识别、句法\语义分析 应用：机器翻译、情感分析、客服系统、问答系统 Word Vectors所有NLP任务的第一个也是最重要的共同点是我们如何将单词表示为模型的输入。 为了在大多数NLP任务上表现良好，我们首先需要有一些相似性和单词之间差异的概念。 使用词向量，我们可以很容易地在向量本身中编码这种能力（如Jaccard，Cosine，Euclidean等距离测量方法）。 NLP表示层次：形态级别传统方法在形态级别的表示是词素： 深度学习中把词素也作为向量，多个词素向量构成相同纬度语义更丰富的词向量。 NLP工具：句法分析 我在《基于神经网络的高性能依存句法分析器》中分析并移植的LTP句法分析器，参考的就是这里介绍的Danqi Chen的A Fast and Accurate Dependency Parser using Neural Networks.pdf。原来她是这门课的TA： NLP语义层面的表示传统方法是手写大量的规则函数，叫做Lambda calculus： • Carefully engineered functions• Take as inputs specific other functions• No notion of similarity or fuzziness of language 在深度学习中，每个句子、短语和逻辑表述都是向量。神经网络负责它们的合并。 情感分析传统方法是请一两百个工人，手工搜集“情感极性词典”在词袋模型上做分类器。 深度学习复用了RNN来解决这个问题，它可以识别“反话”的情感极性： 注意这只是为了方便理解的示意图，并不是RNN的工作流程。私以为这张图放在这里不合适，可能会误导一部分人，以为神经网络就是这样的基于规则的“决策树”模型。 问答系统传统方法是手工编写大量的逻辑规则，比如正则表达式之类： 深度学习依然使用了类似的学习框架，把事实储存在向量里： 客服系统最著名的例子得数GMail的自动回复： 这是Neural Language Models的又一次成功应用，Neural Language Models是基于RNN的： 机器翻译传统方法在许多层级上做了尝试，词语、语法、语义之类。这类方法试图找到一种世界通用的“国际语”（Interlingua）来作为原文和译文的桥梁。 而Neural Machine Translation将原文映射为向量，由向量构建译文。也许可以说Neural Machine Translation的“国际语”是向量。 结论：所有层级的表示都是向量 这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高阶的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。 下面两次课会详细地讲解向量表示，希望能带来新的体会。 这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高阶的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。 旧版视频中Socher还顺便广告了一下他的创业公司MetaMind（已被收购，人生赢家）： 这个demo让我非常惊讶，因为普通NLP演示页面都是让人手工选择要执行的任务的。而这个demo竟然支持用一句话表示自己要执行的意图。不光可以执行情感分析、句法分析之类的常规任务，还可以输入一段话做推理任务。更让我惊讶的是，据说后台所有任务用的都是同一种模型，真乃神机也。据说这种模型是Dynamic Memory Network。另外，他们又发了篇A Joint Many-Task Model:Growing a Neural Network for Multiple NLP Tasks，不知道两者有什么联系没有。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>CS224n</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实体和关系的联合抽取研究]]></title>
    <url>%2F%E5%AE%9E%E4%BD%93%E5%92%8C%E5%85%B3%E7%B3%BB%E7%9A%84%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Abstract实体关系联合抽取旨在识别出非结构化文本中的实体, 并同时提取出实体之间隐含的语义关系。它是信息抽取任务中一个非常重要研究方向。本文总结了该领域的主要研究方向和最新进展。 Introduction实体关系联合抽取旨在识别出非结构化文本中的实体, 并同时提取出实体之间隐含的语义关系。如图一所示 与开放信息抽取 (Open IE) (Banko et al., 2007) 不同的是，实体关系联合抽取任务中的关系词是从预定义的关系集合中抽取出来的，它可能不会出现在给定的句子当中。这是知识提取（knowledge extraction）和知识库（knowledge base）自动构建中的一个重要问题。 实体关系抽取任务的传统方法采用流水线方式（pipelined manner），例如：首先提取出实体(Nadeau and Sekine, 2007)，然后对实体间的关系进行分类(Rink, 2010)。这种框架将实体的识别和关系的分类分为两个子任务，使得实体和关系的抽取任务变得简单，每个组件都很灵活。由于每一个子任务都是一个独立的模型，忽略了这两个子任务之间的相关性，实体识别的结果会对关系分类的性能造成影响，产生误差传递(Li and Ji, 2014)。 联合学习方法是使用一个模型同时抽取实体和关系。 Reference[1]张晓斌,陈福才,黄瑞阳.基于CNN和双向LSTM融合的实体关系抽取[J].网络与信息安全学报,2018(09):44-51. [2]Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJCAI. volume 7, pages 2670–2676.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云服务器安装远程ipython notebook]]></title>
    <url>%2F%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85%E8%BF%9C%E7%A8%8Bipython-notebook%2F</url>
    <content type="text"><![CDATA[Ipython Notebook非常适合用来做数据分析，在windows下安装anaconda就可以直接使用Ipython Notebook了。但是如果是要在云服务器的Linux环境下安装Ipython Notebook，并且在本地远程访问，就需要自己动手配置了。 环境服务器：阿里云ECS，Ubuntu16.04 64位 Xshell5 1.安装anacondaAnaconda指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。 首先在清华大学开源软件镜像站下载Anaconda3，下载最新版本就行了 12sudo apt-get update #执行这条命令更新一下sudo wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Linux-x86_64.sh #这里我选择的版本是3-5.2.0，64位。 下载好后，进入下载的anaconda 安装包的目录下进行安装 1bash Anaconda3-5.2.0-Linux-x86_64.sh 最后一部是询问是否将python加入环境变量，选择yes，当然你也可以在安装完后手动添加到bash中。 安装完成之后要重启终端，anaconda才能生效。 输入python进行测试安装是否成功，显示python版本这表示安装正确。 2. 配置ipython jupyter2.1.生成配置文件123456789101112131415# 生成配置文件jupyter notebook --generate-config# 此时生成配置文件：# Writing default config to: /home/python/.jupyter/jupyter_notebook_config.py# 这里 /home/python/... 中的python指的是用户名是python，将其改成自己的用户名就好了，下面的情况也是一样的。# 创建登录密码# 打开ipython，生成密钥，这一步也可以不做，如果不需要设置密码的话。$ ipythonfrom notebook.auth import passwdpasswd()Enter password:Verify password:Out[2]: 'sha1:6f6193fcfbd5:614c4ba185334868fc8bbce2e9890b3ef7d1a79b' # 我这里创建的密码是123456，对应的密钥是sha1xxxx的那一串# 然后退出ipython 2.2.创建自签名的证书这一步使用openssl创建一个自签名证书，如果不担心安全问题，不使用ssl速度会快一些。 1234567891011# # 在linux下执行，遇到询问的地方一路回车即可openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem# 会在当前文件夹下生成 mycert.pem，我将它移到.jupyter/secret文件夹下面，方便管理# 先创建.secret文件夹cd .jupytermkdir secret # 移动cd ~mv mycert.pem .jupyter/secret/ 2.3.修改配置文件123456789101112131415161718192021# 打开刚才创建的.jupyter/jupyter_notebook_config.py，先备份源文件，然后再修改# 备份$ cp .jupyter/jupyter_notebook_config.py .jupyter/jupyter_notebook_config.py_bak# 修改如下，可以先删除里面的内容添加，也可以修改，或者直接在头部添加，反正里面的原先的内容都是注释掉的：vi /home/python/.jupyter/jupyter_notebook_config.pyc = get_config()# Kernel configc.IPKernelApp.pylab = 'inline' # if you want plotting support alwaysc.NotebookApp.ip = '*' # 就是设置所有ip皆可访问，在144行c.NotebookApp.open_browser = False # 禁止自动打开浏览器# 密钥，在194行。该密钥就是2.1步生成的c.NotebookApp.password = 'sha1:74d233d59da1:50d7ef60a58456e2016dc427547fb42cdd971cea'c.NotebookApp.port = 6868 # 访问端口，在197行# 自签名证书位置，如果不使用ssl，可以不设置c.NotebookNotary.secret_file = '/home/python/.jupyter/secret/mycert.pem'c.NotebookApp.keyfile = '/home/python/.jupyter/.secret/mykey.key'# 设置目录，存放创建的ipython notebook文件c.NotebookApp.notebook_dir = '/home/python/ipython' 3.XShell配置远程访问1234567891011121314# 启动ipython jupyter，不使用ssljupyter notebook# 或者开启ssl# jupyter notebook --certfile=mycert.pem --keyfile mykey.keyjupyter notebook --certfile=/home/zhenyu/.jupyter/secret/mycert.pem# 输出，看倒数第二行，显示的是IP地址和端口号，记下来[I 15:56:59.916 NotebookApp] JupyterLab beta preview extension loaded from /home/python/anaconda3/lib/python3.6/site-packages/jupyterlab[I 15:56:59.916 NotebookApp] JupyterLab application directory is /home/python/anaconda3/share/jupyter/lab[I 15:56:59.921 NotebookApp] Serving notebooks from local directory: /home/python/ipython[I 15:56:59.921 NotebookApp] 0 active kernels[I 15:56:59.921 NotebookApp] The Jupyter Notebook is running at:[I 15:56:59.921 NotebookApp] http://127.0.0.1:6868/[I 15:56:59.921 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). 这时候退出XShell连接，再打开XShell，进入会话连接属性，添加一条转义规则，就可以在本地window机器上远程访问jupyter notebook使用python进行数据分析了。 打开浏览器，在地址栏输入http://localhost:5858就能远程访问云服务器中的6868端口了。 linux 下安装anacondahttps://blog.csdn.net/u010414589/article/details/51303502 Linux 远程Ipython Notebookhttps://blog.csdn.net/suzyu12345/article/details/51037905 XShell–SSH端口转发https://blog.csdn.net/qq_34039315/article/details/77510923]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>anaconda</tag>
        <tag>云服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop在云服务器上的安装教程]]></title>
    <url>%2FHadoop%E5%9C%A8%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[XShell–SSH端口转发https://blog.csdn.net/qq_34039315/article/details/77510923 修改Linux终端命令行中的用户名和主机名https://blog.csdn.net/qingdu007/article/details/51434172 本教程转载自厦门大学数据库实验室 / 给力星 环境本教程使用 Ubuntu 14.04 64位 作为系统环境（Ubuntu 12.04，Ubuntu16.04 也行，32位、64位均可），请自行安装系统（可参考使用VirtualBox安装Ubuntu）。 如果用的是 CentOS/RedHat 系统，请查看相应的CentOS安装Hadoop教程_单机伪分布式配置。 本教程基于原生 Hadoop 2，在 Hadoop 2.6.0 (stable) 版本下验证通过，可适合任何 Hadoop 2.x.y 版本，如 Hadoop 2.7.1、2.6.3、2.4.1等。 使用本教程请确保系统处于联网状态下，部分高校使用星网锐捷连接网络，可能导致虚拟机无法联网，那么建议您使用双系统安装ubuntu,然后再使用本教程！ Hadoop版本 Hadoop 有两个主要版本，Hadoop 1.x.y 和 Hadoop 2.x.y 系列，比较老的教材上用的可能是 0.20 这样的版本。Hadoop 2.x 版本在不断更新，本教程均可适用。如果需安装 0.20，1.2.1这样的版本，本教程也可以作为参考，主要差别在于配置项，配置请参考官网教程或其他教程。 新版是兼容旧版的，书上旧版本的代码应该能够正常运行（我自己没验证，欢迎验证反馈）。 装好了 Ubuntu 系统之后，在安装 Hadoop 前还需要做一些必备工作。 创建hadoop用户如果你安装 Ubuntu 的时候不是用的 “hadoop” 用户，那么需要增加一个名为 hadoop 的用户。 首先按 ctrl+alt+t 打开终端窗口，输入如下命令创建新用户 : 1sudo useradd -m hadoop -s /bin/bash Shell 命令 这条命令创建了可以登陆的 hadoop 用户，并使用 /bin/bash 作为 shell。 sudo命令 本文中会大量使用到sudo命令。sudo是ubuntu中一种权限管理机制，管理员可以授权给一些普通用户去执行一些需要root权限执行的操作。当使用sudo命令时，就需要输入您当前用户的密码. 密码 在Linux的终端中输入密码，终端是不会显示任何你当前输入的密码，也不会提示你已经输入了多少字符密码。而在windows系统中,输入密码一般都会以“*”表示你输入的密码字符 输入法中英文切换 ubuntu中终端输入的命令一般都是使用英文输入。linux中英文的切换方式是使用键盘“shift”键来切换，也可以点击顶部菜单的输入法按钮进行切换。ubuntu自带的Sunpinyin中文输入法已经足够读者使用。 Ubuntu终端复制粘贴快捷键 在Ubuntu终端窗口中，复制粘贴的快捷键需要加上 shift，即粘贴是 ctrl+shift+v。 接着使用如下命令设置密码，可简单设置为 hadoop，按提示输入两次密码： 1sudo passwd hadoop Shell 命令 可为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较棘手的权限问题： 1sudo adduser hadoop sudo Shell 命令 最后注销当前用户（点击屏幕右上角的齿轮，选择注销），返回登陆界面。在登陆界面中选择刚创建的 hadoop 用户进行登陆。 更新apt用 hadoop 用户登录后，我们先更新一下 apt，后续我们使用 apt 安装软件，如果没更新可能有一些软件安装不了。按 ctrl+alt+t 打开终端窗口，执行如下命令： 1sudo apt-get update Shell 命令 若出现如下 “Hash校验和不符” 的提示，可通过更改软件源来解决。若没有该问题，则不需要更改。从软件源下载某些软件的过程中，可能由于网络方面的原因出现没法下载的情况，那么建议更改软件源。在学习Hadoop过程中，即使出现“Hash校验和不符”的提示，也不会影响Hadoop的安装。 Ubuntu更新软件源时遇到Hash校验和不符的问题 点击查看：如何更改软件源 后续需要更改一些配置文件，我比较喜欢用的是 vim（vi增强版，基本用法相同），建议安装一下（如果你实在还不会用 vi/vim 的，请将后面用到 vim 的地方改为 gedit，这样可以使用文本编辑器进行修改，并且每次文件更改完成后请关闭整个 gedit 程序，否则会占用终端）： 1sudo apt-get install vim Shell 命令 安装软件时若需要确认，在提示处输入 y 即可。 通过命令行安装软件 点击查看：vim简单操作指南 安装SSH、配置SSH无密码登陆集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server： 1sudo apt-get install openssh-server Shell 命令 安装后，可以使用如下命令登陆本机： 1ssh localhost Shell 命令 此时会有如下提示(SSH首次登陆提示)，输入 yes 。然后按提示输入密码 hadoop，这样就登陆到本机了。 SSH首次登陆提示 但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。 首先退出刚才的 ssh，就回到了我们原先的终端窗口，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中： 1exit # 退出刚才的 ssh localhostcd ~/.ssh/ # 若没有该目录，请先执行一次ssh localhostssh-keygen -t rsa # 会有提示，都按回车就可以cat ./id_rsa.pub &gt;&gt; ./authorized_keys # 加入授权 Shell 命令 ~的含义 在 Linux 系统中，~ 代表的是用户的主文件夹，即 “/home/用户名” 这个目录，如你的用户名为 hadoop，则 ~ 就代表 “/home/hadoop/”。 此外，命令中的 # 后面的文字是注释，只需要输入前面命令即可。 此时再用 ssh localhost 命令，无需输入密码就可以直接登陆了，如下图所示。 SSH无密码登录 安装Java环境Java环境可选择 Oracle 的 JDK，或是 OpenJDK，按中说的，新版本在 OpenJDK 1.7 下是没问题的。为图方便，这边直接通过命令安装 OpenJDK 7。下面有两种安装JDK的方式，可以任选一种，如果第1种失败，就选择第2种。推荐直接使用第2种安装方式。（1）第1种安装JDK方式： 1sudo apt-get install openjdk-7-jre openjdk-7-jdk Shell 命令 安装好 OpenJDK 后，需要找到相应的安装路径，这个路径是用于配置 JAVA_HOME 环境变量的。执行如下命令： 1dpkg -L openjdk-7-jdk | grep '/bin/javac' Shell 命令 该命令会输出一个路径，除去路径末尾的 “/bin/javac”，剩下的就是正确的路径了。如输出路径为 /usr/lib/jvm/java-7-openjdk-amd64/bin/javac，则我们需要的路径为 /usr/lib/jvm/java-7-openjdk-amd64。 接着需要配置一下 JAVA_HOME 环境变量，为方便，我们在 ~/.bashrc 中进行设置（扩展阅读: 设置Linux环境变量的方法和区别）： 1vim ~/.bashrc Shell 命令 在文件最前面添加如下单独一行（注意 = 号前后不能有空格），将“JDK安装路径”改为上述命令得到的路径，并保存： 1export JAVA_HOME=JDK安装路径 Shell 如下图所示（该文件原本可能不存在，内容为空，这不影响）： 配置JAVA_HOME变量 接着还需要让该环境变量生效，执行如下代码： 1source ~/.bashrc # 使变量设置生效 Shell 命令 设置好后我们来检验一下是否设置正确： 1echo $JAVA_HOME # 检验变量值java -version$JAVA_HOME/bin/java -version # 与直接执行 java -version 一样 Shell 命令 如果设置正确的话，$JAVA_HOME/bin/java -version 会输出 java 的版本信息，且和 java -version 的输出结果一样，如下图所示： 成功配置JAVA_HOME变量 这样，Hadoop 所需的 Java 运行环境就安装好了。 （2）第2种安装JDK方式根据大量电脑安装Java环境的情况我们发现，部分电脑按照上述的第一种安装方式会出现安装失败的情况，这时，可以采用这里介绍的另外一种安装方式，命令如下： 1sudo apt-get install default-jre default-jdk Shell 命令 上述安装过程需要访问网络下载相关文件，请保持联网状态。安装结束以后，需要配置JAVA_HOME环境变量，请在Linux终端中输入下面命令打开当前登录用户的环境变量配置文件.bashrc： 1vim ~/.bashrc Shell 命令 在文件最前面添加如下单独一行（注意，等号“=”前后不能有空格），然后保存退出： 1export JAVA_HOME=/usr/lib/jvm/default-java 接下来，要让环境变量立即生效，请执行如下代码： 1source ~/.bashrc # 使变量设置生效 Shell 命令 执行上述命令后，可以检验一下是否设置正确： 1echo $JAVA_HOME # 检验变量值java -version$JAVA_HOME/bin/java -version # 与直接执行java -version一样 Shell 命令 至此，就成功安装了Java环境。下面就可以进入Hadoop的安装。 安装 Hadoop 2Hadoop 2 可以通过 http://mirror.bit.edu.cn/apache/hadoop/common/ 或者 http://mirrors.cnnic.cn/apache/hadoop/common/ 下载，一般选择下载最新的稳定版本，即下载 “stable” 下的 hadoop-2.x.y.tar.gz 这个格式的文件，这是编译好的，另一个包含 src 的则是 Hadoop 源代码，需要进行编译才可使用。 截止到2015年12月9日，Hadoop官方网站已经更新到2.7.1版本。对于2.6.0以上版本的Hadoop，仍可以参照此教程学习，可放心下载官网最新版本的Hadoop。 如果读者是使用虚拟机方式安装Ubuntu系统的用户，请用虚拟机中的Ubuntu自带firefox浏览器访问本指南，再点击下面的地址，才能把hadoop文件下载虚拟机ubuntu中。请不要使用Windows系统下的浏览器下载，文件会被下载到Windows系统中，虚拟机中的Ubuntu无法访问外部Windows系统的文件，造成不必要的麻烦。 如果读者是使用双系统方式安装Ubuntu系统的用户，请进去Ubuntu系统，在Ubuntu系统打开firefox浏览器访问本指南，再点击下面的地址下载：hadoop-2.7.1下载地址 下载完 Hadoop 文件后一般就可以直接使用。但是如果网络不好，可能会导致下载的文件缺失，可以使用 md5 等检测工具可以校验文件是否完整。 点击查看：如何校验下载的文件是否完整 我们选择将 Hadoop 安装至 /usr/local/ 中： 1sudo tar -zxf ~/下载/hadoop-2.6.0.tar.gz -C /usr/local # 解压到/usr/local中cd /usr/local/sudo mv ./hadoop-2.6.0/ ./hadoop # 将文件夹名改为hadoopsudo chown -R hadoop ./hadoop # 修改文件权限 Shell 命令 Hadoop 解压后即可使用。输入如下命令来检查 Hadoop 是否可用，成功则会显示 Hadoop 版本信息： 1cd /usr/local/hadoop./bin/hadoop version Shell 命令 相对路径与绝对路径 请务必注意命令中的相对路径与绝对路径，本文后续出现的 ./bin/...，./etc/... 等包含 ./ 的路径，均为相对路径，以 /usr/local/hadoop 为当前目录。例如在 /usr/local/hadoop 目录中执行 ./bin/hadoop version 等同于执行 /usr/local/hadoop/bin/hadoop version。可以将相对路径改成绝对路径来执行，但如果你是在主文件夹 ~ 中执行 ./bin/hadoop version，执行的会是 /home/hadoop/bin/hadoop version，就不是我们所想要的了。 Hadoop单机配置(非分布式)Hadoop 默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。 现在我们可以执行例子来感受下 Hadoop 的运行。Hadoop 附带了丰富的例子（运行 ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar 可以看到所有例子），包括 wordcount、terasort、join、grep 等。 在此我们选择运行 grep 例子，我们将 input 文件夹中的所有文件作为输入，筛选当中符合正则表达式 dfs[a-z.]+ 的单词并统计出现的次数，最后输出结果到 output 文件夹中。 1cd /usr/local/hadoopmkdir ./inputcp ./etc/hadoop/*.xml ./input # 将配置文件作为输入文件./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output 'dfs[a-z.]+'cat ./output/* # 查看运行结果 Shell 命令 执行成功后如下所示，输出了作业的相关信息，输出的结果是符合正则的单词 dfsadmin 出现了1次 Hadoop单机模式运行grep的输出结果 注意，Hadoop 默认不会覆盖结果文件，因此再次运行上面实例会提示出错，需要先将 ./output 删除。 1rm -r ./output Shell 命令 Hadoop伪分布式配置Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。 Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。 修改配置文件 core-site.xml (通过 gedit 编辑会比较方便: gedit ./etc/hadoop/core-site.xml)，将当中的 1&lt;configuration&gt;&lt;/configuration&gt; XML 修改为下面配置： 1&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; XML 同样的，修改配置文件 hdfs-site.xml： 1&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; XML Hadoop配置文件说明 Hadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。 此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 /tmp/hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。 配置完成后，执行 NameNode 的格式化: 1./bin/hdfs namenode -format Shell 命令 成功的话，会看到 “successfully formatted” 和 “Exitting with status 0” 的提示，若为 “Exitting with status 1” 则是出错。 执行namenode格式化 如果在这一步时提示 Error: JAVA_HOME is not set and could not be found. 的错误，则说明之前设置 JAVA_HOME 环境变量那边就没设置好，请按教程先设置好 JAVA_HOME 变量，否则后面的过程都是进行不下去的。如果已经按照前面教程在.bashrc文件中设置了JAVA_HOME，还是出现 Error: JAVA_HOME is not set and could not be found. 的错误，那么，请到hadoop的安装目录修改配置文件“/usr/local/hadoop/etc/hadoop/hadoop-env.sh”，在里面找到“export JAVA_HOME=${JAVA_HOME}”这行，然后，把它修改成JAVA安装路径的具体地址，比如，“export JAVA_HOME=/usr/lib/jvm/default-java”，然后，再次启动Hadoop。 接着开启 NameNode 和 DataNode 守护进程。 1./sbin/start-dfs.sh #start-dfs.sh是个完整的可执行文件，中间没有空格 Shell 命令 若出现如下SSH提示，输入yes即可。 启动Hadoop时的SSH提示 启动时可能会出现如下 WARN 提示：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable WARN 提示可以忽略，并不会影响正常使用。 启动 Hadoop 时提示 Could not resolve hostname 如果启动 Hadoop 时遇到输出非常多“ssh: Could not resolve hostname xxx”的异常情况，如下图所示： 启动Hadoop时的异常提示 这个并不是 ssh 的问题，可通过设置 Hadoop 环境变量来解决。首先按键盘的 ctrl + c 中断启动，然后在 ~/.bashrc 中，增加如下两行内容（设置过程与 JAVA_HOME 变量一样，其中 HADOOP_HOME 为 Hadoop 的安装目录）： 1export HADOOP_HOME=/usr/local/hadoopexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native Shell 保存后，务必执行 source ~/.bashrc 使变量设置生效，然后再次执行 ./sbin/start-dfs.sh 启动 Hadoop。 启动完成后，可以通过命令 jps 来判断是否成功启动，若成功启动则会列出如下进程: “NameNode”、”DataNode” 和 “SecondaryNameNode”（如果 SecondaryNameNode 没有启动，请运行 sbin/stop-dfs.sh 关闭进程，然后再次尝试启动尝试）。如果没有 NameNode 或 DataNode ，那就是配置不成功，请仔细检查之前步骤，或通过查看启动日志排查原因。 通过jps查看启动的Hadoop进程 Hadoop无法正常启动的解决方法 一般可以查看启动日志来排查原因，注意几点： 启动时会提示形如 “DBLab-XMU: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-DBLab-XMU.out”，其中 DBLab-XMU 对应你的机器名，但其实启动日志信息是记录在 /usr/local/hadoop/logs/hadoop-hadoop-namenode-DBLab-XMU.log 中，所以应该查看这个后缀为 .log 的文件； 每一次的启动日志都是追加在日志文件之后，所以得拉到最后面看，对比下记录的时间就知道了。 一般出错的提示在最后面，通常是写着 Fatal、Error、Warning 或者 Java Exception 的地方。 可以在网上搜索一下出错信息，看能否找到一些相关的解决方法。 此外，若是 DataNode 没有启动，可尝试如下的方法（注意这会删除 HDFS 中原有的所有数据，如果原有的数据很重要请不要这样做）： 1# 针对 DataNode 没法启动的解决方法./sbin/stop-dfs.sh # 关闭rm -r ./tmp # 删除 tmp 文件，注意这会删除 HDFS 中原有的所有数据./bin/hdfs namenode -format # 重新格式化 NameNode./sbin/start-dfs.sh # 重启 Shell 命令 成功启动后，可以访问 Web 界面 http://localhost:50070 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。 Hadoop的Web界面 运行Hadoop伪分布式实例上面的单机模式，grep 例子读取的是本地数据，伪分布式读取的则是 HDFS 上的数据。要使用 HDFS，首先需要在 HDFS 中创建用户目录： 1./bin/hdfs dfs -mkdir -p /user/hadoop Shell 命令 注意 教材《大数据技术原理与应用》的命令是以”./bin/hadoop dfs”开头的Shell命令方式，实际上有三种shell命令方式。\1. hadoop fs\2. hadoop dfs\3. hdfs dfs hadoop fs适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统hadoop dfs只能适用于HDFS文件系统hdfs dfs跟hadoop dfs的命令作用一样，也只能适用于HDFS文件系统 接着将 ./etc/hadoop 中的 xml 文件作为输入文件复制到分布式文件系统中，即将 /usr/local/hadoop/etc/hadoop 复制到分布式文件系统中的 /user/hadoop/input 中。我们使用的是 hadoop 用户，并且已创建相应的用户目录 /user/hadoop ，因此在命令中就可以使用相对路径如 input，其对应的绝对路径就是 /user/hadoop/input: 1./bin/hdfs dfs -mkdir input./bin/hdfs dfs -put ./etc/hadoop/*.xml input Shell 命令 复制完成后，可以通过如下命令查看文件列表： 1./bin/hdfs dfs -ls input Shell 命令 伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件（可以将单机步骤中创建的本地 input 文件夹，输出结果 output 文件夹都删掉来验证这一点）。 1./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output 'dfs[a-z.]+' Shell 命令 查看运行结果的命令（查看的是位于 HDFS 中的输出结果）： 1./bin/hdfs dfs -cat output/* Shell 命令 结果如下，注意到刚才我们已经更改了配置文件，所以运行结果不同。 Hadoop伪分布式运行grep结果 我们也可以将运行结果取回到本地： 1rm -r ./output # 先删除本地的 output 文件夹（如果存在）./bin/hdfs dfs -get output ./output # 将 HDFS 上的 output 文件夹拷贝到本机cat ./output/* Shell 命令 Hadoop 运行程序时，输出目录不能存在，否则会提示错误 “org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/hadoop/output already exists” ，因此若要再次执行，需要执行如下命令删除 output 文件夹: 1./bin/hdfs dfs -rm -r output # 删除 output 文件夹 Shell 命令 运行程序时，输出目录不能存在 运行 Hadoop 程序时，为了防止覆盖结果，程序指定的输出目录（如 output）不能存在，否则会提示错误，因此运行前需要先删除输出目录。在实际开发应用程序时，可考虑在程序中加上如下代码，能在每次运行时自动删除输出目录，避免繁琐的命令行操作： 1Configuration conf = new Configuration();Job job = new Job(conf); /* 删除输出目录 */Path outputPath = new Path(args[1]);outputPath.getFileSystem(conf).delete(outputPath, true); Java 若要关闭 Hadoop，则运行 1./sbin/stop-dfs.sh Shell 命令 注意 下次启动 hadoop 时，无需进行 NameNode 的初始化，只需要运行 ./sbin/start-dfs.sh 就可以！ YARNYARN 是 Hadoop 2.x 中的内容，使用林子雨编写的大数据技术原理与应用教材的读者，可不用学习YARN内容。 如果对这方便的内容感兴趣，可点击下方查看。 点击查看：启动YARN 自此，你已经掌握 Hadoop 的配置和基本使用了。安装好的Hadoop项目中已经包含了第三章的HDFS，继续学习第3章HDFS文件系统，请参考如下学习指南：大数据技术原理与应用 第三章 学习指南 附加教程: 配置PATH环境变量在这里额外讲一下 PATH 这个环境变量（可执行 echo $PATH 查看，当中包含了多个目录）。例如我们在主文件夹 ~ 中执行 ls 这个命令时，实际执行的是 /bin/ls 这个程序，而不是 ~/ls 这个程序。系统是根据 PATH 这个环境变量中包含的目录位置，逐一进行查找，直至在这些目录位置下找到匹配的程序（若没有匹配的则提示该命令不存在）。 上面的教程中，我们都是先进入到 /usr/local/hadoop 目录中，再执行 sbin/hadoop，实际上等同于运行 /usr/local/hadoop/sbin/hadoop。我们可以将 Hadoop 命令的相关目录加入到 PATH 环境变量中，这样就可以直接通过 start-dfs.sh 开启 Hadoop，也可以直接通过 hdfs 访问 HDFS 的内容，方便平时的操作。 同样我们选择在 ~/.bashrc 中进行设置（vim ~/.bashrc，与 JAVA_HOME 的设置相似），在文件最前面加入如下单独一行: 1export PATH=$PATH:/usr/local/hadoop/sbin:/usr/local/hadoop/bin 添加后执行 source ~/.bashrc 使设置生效，生效后，在任意目录中，都可以直接使用 hdfs 等命令了，读者不妨现在就执行 hdfs dfs -ls input 查看 HDFS 文件试试看。 安装Hadoop集群在平时的学习中，我们使用伪分布式就足够了。如果需要安装 Hadoop 集群，请查看Hadoop集群安装配置教程。 相关教程 使用Eclipse编译运行MapReduce程序: 使用 Eclipse 可以方便的开发、运行 MapReduce 程序，还可以直接管理 HDFS 中的文件。 使用命令行编译打包运行自己的MapReduce程序: 有时候需要直接通过命令来编译、打包 MapReduce 程序。 参考资料 http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html http://www.micmiu.com/bigdata/hadoop/hadoop-2x-ubuntu-build/]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是大数据]]></title>
    <url>%2F%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[概述根据IBM前首席执行官郭士纳的观点，IT领域每隔十五年就会迎来一次重大变革。 物联网改变了数据产生的方式，迎来了大数据时代。 大数据时代的技术支撑：存储、计算、网络能力的大幅提升。 大数据的四个特性：4V 数据量大：大数据摩尔定律、人类在最近两年产生的数据量相当于之前产生的全部数据量； 数据类型多：大数据是由结构化数据和非结构化数据组成，90%都是非结构化数据，结构化数据是存储在关系型数据库中的数据。 数据处理速度快 数据价值密度低，商业价值高 大数据的影响Jim Gray博士，98年图灵奖得主，提出了事务理论 四种研究方式： 实验，通过实验解决科学问题。 理论 计算 数据，以数据为驱动的全新的科学研究时代。通过对大量数据的分析找出问题。考数据驱动，发现问题，解决问题。 在思维方式方面大数据完全颠覆了传统的思维方式 全样而非抽样，不需要像以前一样做抽样 效率而非精确，因为以前是做抽样分析，所以要最前高精确度，使误差尽量小。大数据时代是对全样分析，不纯在误差放大问题，就不需要最求精确度，所以这时候追求的是效率。 相关而非因果，大数据时代只关注相关性，而不关注因果性。 大数据关键技术两大核心技术： 分布式存储：解决海量数据的存储问题 分布式处理：解决海量数据的处理问题 大数据技术一谷歌公司技术为代表 分布式数据库Big Table 分布式文件系统GFS 分布式并行处理技术MapReduce 不同的计算模式需要使用不同的产品 大数据产品服务的计算模式是不一样的，有些是用于批处理，有些是用于实时计算，有些是用于交互式计算。 批处理计算： 解决问题：针对大规模数据的批量处理 MapReduce是批处理计算模式的典型代表，适用于批处理，不适用于实时计算 15年异军突起的产品Spark，实时性比MapReduce更好，而且它解决了MapReduce当中的一些缺点，MapReduce无法高效的执行迭代计算，但是Spark可以进行迭代计算。许多应用中需要做迭代计算，比如数据挖掘，这时就不能用MapReduce要用Spark 流计算： 解决问题：针对流数据的实时计算 流计算是专门针对流数据的实时计算，流数据需要实时处理，给出实时响应，否则分析结果就会失去商业价值，只能用流计算框架进行处理，它是实现秒级的针对实时数据流的响应。 流计算代表产品：S4，Storm，Flume 图计算： 解决问题：针对大规模图结构数据的处理 图计算代表软件：Google Pregel 社交网络数据就是图结构数据 查询分析计算： 解决问题：大规模数据的存储管理和查询分析 大数据查询分析软件，满足交互式查询分析、 代表产品：Google Dremel，Hive，Cassandra，Impala 云计算 云计算解决两大核心问题： 分布式存储 分布式处理 云计算电信特征： 虚拟化 多租户 云计算的概念：云计算是通过网络以服务的方式为用户提供非常廉价的IT资源 云计算的优势：企业不需要自建IT基础设施，可以租用云端资源 云计算的三种模式： 公有云：百度云 私有云 混合云 自底向上的三种服务： IaaS 将基础设施（计算资源和存储）作为服务出租，如弹性云计算EC2 面向网络架构师 PaaS 平台即服务，云计算环境的开发平台 面向应用开发者 SaaS 典型案例：云财务软件 云计算的关键技术： 虚拟化， 多租户，云计算同时为多个用户服务 云计算数据中心 物联网 物联网层次架构：感知层，网络层，处理层，应用层 物联网的关键技术： 识别技术 感知技术]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】hexo框架基于next主题定制]]></title>
    <url>%2Fhexo%E6%A1%86%E6%9E%B6%E5%9F%BA%E4%BA%8Enext%E4%B8%BB%E9%A2%98%E5%AE%9A%E5%88%B6%2F</url>
    <content type="text"><![CDATA[hexo框架基于next主题深度定制方案 主要有： 在右上角或者左上角实现fork me on github 添加RSS 背景配置 添加动态背景 实现点击出现桃心效果 修改文章内链接文本样式 修改文章底部的那个带#号的标签 在每篇文章末尾统一添加“本文结束”标记 修改作者头像并旋转 博文压缩 修改“代码块自定义样式 侧边栏社交小图标设置 主页文章添加阴影效果 在网站底部加上访问量 添加热度 网站底部字数统计 添加 README.md 文件 设置网站的图标Favicon 实现统计功能 添加顶部加载条 在文章底部增加版权信息 添加Gitment评论系统 隐藏网页底部powered By Hexo / 强力驱动 修改网页底部的桃心 文章加密访问 添加jiathis分享 博文置顶 修改字体大小 修改打赏字体不闪动 侧边栏推荐阅读 自定义鼠标样式 hexo 添加百度站长推送 hexo NexT主题首页title链接的优化 Hexo NexT主题修改文章标题样式 hexo 添加百度站长推送 在右上角或者左上角实现fork me on github具体实现方法点击这里 挑选自己喜欢的样式，并复制代码。 例如，我是复制如下代码： 然后粘贴刚才复制的代码到themes/next/layout/_layout.swig文件中放在 1&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt; 的下面，并把href改为你的github地址 添加RSS具体实现方法切换到你的blog的路径，例如我是在/Users/Hexo/blog这个路径上，也就是在你的根目录下 然后安装 Hexo 插件：(这个插件会放在node_modules这个文件夹里) 1$ npm install --save hexo-generator-feed 接下来打开配置文件 在里面的末尾添加：(请注意在冒号后面要加一个空格，不然会发生错误！) 123# Extensions## Plugins: http://hexo.io/plugins/plugins: hexo-generate-feed 然后打开next主题文件夹里面的_config.yml,在里面配置为如下样子：(就是在rss:的后面加上/atom.xml,注意在冒号后面要加一个空格) 1234# Set rss to false to disable feed link.# Leave rss as empty to use site&apos;s feed link.# Set rss to specific value if you have burned your feed already.rss: /atom.xml 配置完之后运行：$ hexo g 重新生成一次，你会在./public 文件夹中看到 atom.xml 文件。然后启动服务器查看是否有效，之后再部署到 Github 中。 背景配置背景透明 博客根目录 themes\next\source\css_schemes\Pisces_layout.styl这个文件的第65行background:删除掉 按钮背景 博客根目录 themes\next\source\css_common\components\post\post-button.styl 第七行修 background: ; 站点概况背景 博客根目录 themes\next\source\css_schemes\Pisces_sidebar.styl 菜单栏背景 next\source\css_schemes\Pisces_layout.styl 文件里.header-inner 这个选择器下的background 就是背景色 添加动态背景具体实现方法修改代码 打开next/layout/_layout.swig，在之前添加如下代码： 123&#123;% if theme.canvas_nest %&#125;&lt;script type=&quot;text/javascript&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125; 修改主题配置文件 打开/next/_config.yml，添加以下代码： 123456# --------------------------------------------------------------# background settings# --------------------------------------------------------------# add canvas-nest effect# see detail from https://github.com/hustcc/canvas-nest.jscanvas_nest: true 运行hexo clean 和 hexo g hexo s之后就可以看到效果了 实现点击出现桃心效果具体实现方法点击这里love.js 然后将里面的代码copy一下，新建love.js文件并且将代码复制进去，然后保存。将love.js文件放到路径/themes/next/source/js/src里面，然后打开\themes\next\layout_layout.swig文件,在末尾（在前面引用会出现找不到的bug）添加以下代码： 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt; 修改文章内链接文本样式具体实现方法修改文件 themes\next\source\css_common\components\post\post.styl，在末尾添加如下css样式，： 1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125; 其中选择.post-body 是为了不影响标题，选择 p 是为了不影响首页“阅读全文”的显示样式,颜色可以自己定义。 修改文章底部的那个带#号的标签具体实现方法修改模板/themes/next/layout/_macro/post.swig，搜索 rel=”tag”&gt;#，将 # 换成 1&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 在每篇文章末尾统一添加“本文结束”标记具体实现方法在路径 \themes\next\layout_macro 中新建 passage-end-tag.swig 文件,并添加以下内容： 12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束&lt;i class=&quot;fa fa-paw&quot;&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 接着打开\themes\next\layout_macro\post.swig文件，在post-body 之后， post-footer 之前添加如下画红色部分代码（post-footer之前两个DIV）： 代码如下： 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;passage-end-tag.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 然后打开主题配置文件（_config.yml),在末尾添加： 123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true 完成以上设置之后，在每篇文章之后都会添加如上效果图的样子。 修改作者头像并旋转具体实现方法 打开\themes\next\source\css_common\components\sidebar\sidebar-author.styl，在里面添加如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;&#125;img:hover &#123; /* 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;*/ /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;/* Z 轴旋转动画 */@-webkit-keyframes play &#123; 0% &#123; -webkit-transform: rotateZ(0deg); &#125; 100% &#123; -webkit-transform: rotateZ(-360deg); &#125;&#125;@-moz-keyframes play &#123; 0% &#123; -moz-transform: rotateZ(0deg); &#125; 100% &#123; -moz-transform: rotateZ(-360deg); &#125;&#125;@keyframes play &#123; 0% &#123; transform: rotateZ(0deg); &#125; 100% &#123; transform: rotateZ(-360deg); &#125;&#125; 博文压缩在站点的根目录下执行以下命令： 12$ npm install gulp -g$ npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save 在如下图所示，新建 gulpfile.js ，并填入以下内容： 123456789101112131415161718192021222324252627282930313233var gulp = require(&apos;gulp&apos;);var minifycss = require(&apos;gulp-minify-css&apos;);var uglify = require(&apos;gulp-uglify&apos;);var htmlmin = require(&apos;gulp-htmlmin&apos;);var htmlclean = require(&apos;gulp-htmlclean&apos;);// 压缩 public 目录 cssgulp.task(&apos;minify-css&apos;, function() &#123; return gulp.src(&apos;./public/**/*.css&apos;) .pipe(minifycss()) .pipe(gulp.dest(&apos;./public&apos;));&#125;);// 压缩 public 目录 htmlgulp.task(&apos;minify-html&apos;, function() &#123; return gulp.src(&apos;./public/**/*.html&apos;) .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest(&apos;./public&apos;))&#125;);// 压缩 public/js 目录 jsgulp.task(&apos;minify-js&apos;, function() &#123; return gulp.src(&apos;./public/**/*.js&apos;) .pipe(uglify()) .pipe(gulp.dest(&apos;./public&apos;));&#125;);// 执行 gulp 命令时执行的任务gulp.task(&apos;default&apos;, [ &apos;minify-html&apos;,&apos;minify-css&apos;,&apos;minify-js&apos;]); 生成博文是执行 hexo g &amp;&amp; gulp 就会根据 gulpfile.js 中的配置，对 public 目录中的静态资源文件进行压缩。 修改“代码块自定义样式具体实现方法打开\themes\next\source\css_custom\custom.styl,向里面加入：(颜色可以自己定义) 123456789101112131415// Custom styles.code &#123; color: #ff7600; background: #fbf7f8; margin: 2px;&#125;// 大代码块的自定义样式.highlight, pre &#123; margin: 5px 0; padding: 5px; border-radius: 3px;&#125;.highlight, code, pre &#123; border: 1px solid #d6d6d6;&#125; 侧边栏社交小图标设置具体实现方法打开主题配置文件（_config.yml），搜索social_icons:,在图标库找自己喜欢的小图标，并将名字复制在如下位置，保存即可 主页文章添加阴影效果具体实现方法打开\themes\next\source\css_custom\custom.styl,向里面加入： 12345678// 主页文章添加阴影效果 .post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); &#125; 在网站底部加上访问量具体实现方法打开\themes\next\layout_partials\footer.swig文件,在copyright前加上画红线这句话： 代码如下： 1&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 然后再合适的位置添加显示统计的代码，如图： 代码如下： 12345&lt;div class=&quot;powered-by&quot;&gt;&lt;i class=&quot;fa fa-user-md&quot;&gt;&lt;/i&gt;&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt; 本站访客数:&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt; 在这里有两中不同计算方式的统计代码： pv的方式，单个用户连续点击n篇文章，记录n次访问量 123&lt;span id=&quot;busuanzi_container_site_pv&quot;&gt; 本站总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次&lt;/span&gt; uv的方式，单个用户连续点击n篇文章，只记录1次访客数 123&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt; 本站总访问量&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;次&lt;/span&gt; 添加之后再执行hexo d -g，然后再刷新页面就能看到效果 添加热度具体实现方法next主题集成leanCloud，打开/themes/next/layout/_macro/post.swig,在画红线的区域添加℃： 然后打开，/themes/next/languages/zh-Hans.yml,将画红框的改为热度就可以了 网站底部字数统计具体方法实现切换到根目录下，然后运行如下代码 1$ npm install hexo-wordcount --save 然后在/themes/next/layout/_partials/footer.swig文件尾部加上： 1234&lt;div class=&quot;theme-info&quot;&gt; &lt;div class=&quot;powered-by&quot;&gt;&lt;/div&gt; &lt;span class=&quot;post-count&quot;&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt; 添加 README.md 文件每个项目下一般都有一个 README.md 文件，但是使用 hexo 部署到仓库后，项目下是没有 README.md 文件的。在 Hexo 目录下的 source 根目录下添加一个 README.md 文件，修改站点配置文件 _config.yml，将 skip_render 参数的值设置为skip_render: README.md 保存退出即可。再次使用 hexo d 命令部署博客的时候就不会在渲染 README.md 这个文件了。 设置网站的图标Favicon具体方法实现在EasyIcon中找一张（32*32）的ico图标,或者去别的网站下载或者制作，并将图标名称改为favicon.ico，然后把图标放在/themes/next/source/images里，并且修改主题配置文件：# Put your favicon.ico into hexo-site/source/ directory.favicon: /favicon.ico 实现统计功能具体实现方法在根目录下安装 hexo-wordcount,运行： 1$ npm install hexo-wordcount --save 然后在主题的配置文件中，配置如下： 123456# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true wordcount: true min2read: true 添加顶部加载条具体实现方法打开/themes/next/layout/_partials/head.swig文件，添加红框上的代码 代码如下： 12&lt;script src=&quot;//cdn.bootcss.com/pace/1.0.2/pace.min.js&quot;&gt;&lt;/script&gt;&lt;link href=&quot;//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css&quot; rel=&quot;stylesheet&quot;&gt; 但是，默认的是粉色的，要改变颜色可以在/themes/next/layout/_partials/head.swig文件中添加如下代码（接在刚才link的后面） 12345678910111213&lt;style&gt; .pace .pace-progress &#123; background: #1E92FB; /*进度条颜色*/ height: 3px; &#125; .pace .pace-progress-inner &#123; box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /*阴影颜色*/ &#125; .pace .pace-activity &#123; border-top-color: #1E92FB; /*上边框颜色*/ border-left-color: #1E92FB; /*左边框颜色*/ &#125;&lt;/style&gt; 目前，博主的增加顶部加载条的pull request 已被Merge现在升级最新版的next主题，升级后只需修改主题配置文件(_config.yml)将pace: false改为pace: true就行了，你还可以换不同样式的加载条. 在文章底部增加版权信息在目录 next/layout/_macro/下添加 my-copyright.swig： 1234567891011121314151617181920212223242526272829303132&#123;% if page.copyright %&#125;&lt;div class=&quot;my_post_copyright&quot;&gt; &lt;script src=&quot;//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js&quot;&gt;&lt;/script&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css&quot;&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot;&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=&quot;/&quot; title=&quot;访问 &#123;&#123; theme.author &#125;&#125; 的个人博客&quot;&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title=&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class=&quot;copy-path&quot; title=&quot;点击复制文章链接&quot;&gt;&lt;i class=&quot;fa fa-clipboard&quot; data-clipboard-text=&quot;&#123;&#123; page.permalink &#125;&#125;&quot; aria-label=&quot;复制成功！&quot;&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=&quot;fa fa-creative-commons&quot;&gt;&lt;/i&gt; &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; target=&quot;_blank&quot; title=&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard(&apos;.fa-clipboard&apos;); clipboard.on(&apos;success&apos;, $(function()&#123; $(&quot;.fa-clipboard&quot;).click(function()&#123; swal(&#123; title: &quot;&quot;, text: &apos;复制成功&apos;, html: false, timer: 500, showConfirmButton: false &#125;); &#125;); &#125;)); &lt;/script&gt;&#123;% endif %&#125; 在目录next/source/css/_common/components/post/下添加my-post-copyright.styl： 123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 修改next/layout/_macro/post.swig，在代码 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;wechat-subscriber.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 之前添加增加如下代码： 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;my-copyright.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 如下： 修改next/source/css/_common/components/post/post.styl文件，在最后一行增加代码： 1@import &quot;my-post-copyright&quot; 保存重新生成即可。如果要在该博文下面增加版权信息的显示，需要在 Markdown 中增加 copyright: true 的设置，类似：小技巧：如果你觉得每次都要输入 copyright: true 很麻烦的话,那么在 /scaffolds/post.md 文件中添加： 这样每次hexo new “你的内容”之后，生成的md文件会自动把copyright:加到里面去(注意：如果解析出来之后，你的原始链接有问题：如：http://yoursite.com/前端小项目：使用canvas绘画哆啦A梦.html,那么在根目录下_config.yml中写成类似这样：）就行了。 添加Gitment评论系统具体实现方法见 为 hexo NexT 添加 Gitment 评论插件 隐藏网页底部powered By Hexo / 强力驱动打开themes/next/layout/_partials/footer.swig,使用””隐藏之间的代码即可，或者直接删除。 修改网页底部的桃心还是打开themes/next/layout/_partials/footer.swig，找到：，然后还是在图标库中找到你自己喜欢的图标，然后修改画红线的部分就可以了。 文章加密访问具体实现方法打开themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig文件,在以下位置插入这样一段代码： 代码如下： 12345678910&lt;script&gt; (function()&#123; if(&apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; alert(&apos;密码错误！&apos;); history.back(); &#125; &#125; &#125;)();&lt;/script&gt; 添加jiathis分享在主题配置文件中，jiathis为true，就行了，如下图 默认是这样子的： 如果你想自定义话，打开themes/next/layout/_partials/share/jiathis.swig修改画红线部分就可以了 博文置顶修改 hero-generator-index 插件，把文件：node_modules/hexo-generator-index/lib/generator.js 内的代码替换为： 12345678910111213141516171819202122232425262728&apos;use strict&apos;;var pagination = require(&apos;hexo-pagination&apos;);module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || &apos;page&apos;; return pagination(&apos;&apos;, posts, &#123; perPage: config.index_generator.per_page, layout: [&apos;index&apos;, &apos;archive&apos;], format: paginationDir + &apos;/%d/&apos;, data: &#123; __index: true &#125; &#125;);&#125;; 在文章中添加 top 值，数值越大文章越靠前，如 12345678---title: 解决Charles乱码问题date: 2017-05-22 22:45:48tags: 技巧categories: 技巧copyright: truetop: 100--- 修改字体大小打开\themes\next\source\css_variables\base.styl文件，将 $font-size-base改成16px ，如下所示： 1$font-size-base = 16px 修改打赏字体不闪动修改文件next/source/css/_common/components/post/post-reward.styl，然后注释其中的函数wechat:hover和alipay:hover，如下： 123456789101112/* 注释文字闪动函数 #wechat:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125; #alipay:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;*/ 侧边栏推荐阅读打开主题配置文件修改成这样就行了(links里面写你想要的链接): 1234567891011# Blogrollslinks_title: 推荐阅读#links_layout: blocklinks_layout: inlinelinks: 优设: http://www.uisdc.com/ 张鑫旭: http://www.zhangxinxu.com/ Web前端导航: http://www.alloyteam.com/nav/ 前端书籍资料: http://www.36zhen.com/t?id=3448 百度前端技术学院: http://ife.baidu.com/ google前端开发基础: http://wf.uisdc.com/cn/ 自定义鼠标样式打开themes/next/source/css/_custom/custom.styl,在里面写下如下代码 1234567// 鼠标样式 * &#123; cursor: url(&quot;http://om8u46rmb.bkt.clouddn.com/sword2.ico&quot;),auto!important &#125; :active &#123; cursor: url(&quot;http://om8u46rmb.bkt.clouddn.com/sword1.ico&quot;),auto!important &#125; 其中 url 里面必须是 ico 图片，ico 图片可以上传到网上（我是使用七牛云图床），然后获取外链，复制到 url 里就行了 hexo 添加百度站长推送具体实现方法见 hexo 添加百度站长推送 hexo NexT主题首页title链接的优化具体实现方法见 hexo NexT主题首页title链接的优化 Hexo NexT主题修改文章标题样式进入主题目录 hexo\themes\next\source\css_common\components\post 修改post.styl文件，在配置的后面添加下面的代码。该文件是博文的样式表。 注意如果想把主页标题样式一同修改，可以用把 .page-post-detail 去掉 123456789101112131415161718192021/*添加下面的CSS代码来修改博客标题样式*/.page-post-detail .post-title &#123; font-size: 26px; text-align: center; word-break: break-word; font-weight: $posts-expand-title-font-weight background-color: #b9d3ee; border-radius:.3em; line-height:1em; padding-bottom:.12em; padding-top:.12em; box-shadow:2px 2px 7px #9fb6cd; +mobile() &#123; font-size: 22px; &#125;&#125;/*添加上面的CSS代码来修改博客标题样式*/@import &quot;post-expand&quot;;@import &quot;post-collapse&quot;;@import &quot;post-type&quot;;@import &quot;post-title&quot;; hexo 添加百度站长推送主动推送 自动推送，sitemap推送 本文转载自Hunter-Zack]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1936年9月竺可桢校长在浙大开学典礼上的讲话]]></title>
    <url>%2F1936%E5%B9%B49%E6%9C%88%E7%AB%BA%E5%8F%AF%E6%A1%A2%E6%A0%A1%E9%95%BF%E5%9C%A8%E6%B5%99%E5%A4%A7%E5%BC%80%E5%AD%A6%E5%85%B8%E7%A4%BC%E4%B8%8A%E7%9A%84%E8%AE%B2%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[诸位同学，学校开课已一周，今天训育处召集这个会，能如家人似的在一起谈话，觉得非常愉快。 大学生，是人生最快活的时期，没有直接的经济负担，没有谋生的问题。诸位在中学中，同学大都是同县或同省，可是，来大学后，有从全国各方面来的同学，可以知道全国的情形，时间长了，各人都认识。这样，各人家庭的状况，故乡的风物，都能互相知道，这亦是一种教育。大学比之中学，在经费和设备方面，都来的充实，教师的经验和学识，也远胜于中学，这供给诸位切磋学问的极好机会。同时，国家花在诸位身上的钱，每年有一千五百元，而且，全中国大学生仅四万人，诸位都是这一万分之一的青年，这种机会，万万不能错过。 诸位到这里来，应该明了这里的校风。一校有一校的精神，英文称为College Spirit。至于浙大的精神，可以把“诚”、“勤”两字来表示。浙大的前身是求是书院和高等学堂，一脉相传，都可以诚勤两字代表它的学风，学生不浮夸，做事很勤恳，在社会上的声誉亦很好。有的学校校舍很好，可是毕业生做事，初出去就希望有物质的享受，待遇低一点便不愿做，房屋陋不愿住。浙大的毕业生便无此习惯，校外的人，碰见了，总是称赞浙大的风气朴实。这种风气，希望诸位把它保持。 诸位在校，有两个问题应该自己问问，第一，到浙大来做什么?第二，将来毕业后要做什么样的人?我想诸位中间，一定没有人说为文凭而到浙大来的，或者有的为到这里来是为了求一种技术，以做谋生的工具。但是，谋生之道很多，不一定到大学来，就是讲技术，亦不一定在大学。美国大文豪罗威尔氏说：“大学的目的，不在使学生得到面包，而在使所得到的面包味道更好。”教育不仅使学生谋得求生之道，单学一种技术，尚非教育最重要的目的。 这里我可以讲一个故事。中国古时有一个人求神仙心切，遍走名山大川。吕纯阳发慈悲，知道他诚心，想送给他一点金钱宝贝，向他说道，我的指头能指石为金，或任何物件，你要什么我便给你什么。可是那个人并不要金钱宝贝，而要他那只指头。这故事西洋也有的，英文所谓Wishing Ring，便是这个意思，要想什么就可得什么。世界上万事万物统有他存在的理由，朱子所谓格物致知就是即事而穷其理。要能即事而穷其理，最要紧的是一个清醒的头脑。 清醒的头脑，是事业成功的基础。两三年以后诸位出去，在社会上做一番事业，无论工农商学，都须有清醒的头脑。专精一门技术的人，头脑未必清楚。反之，头脑清楚，做学问办事业统行，我们国家到这步田地，完全靠头脑清醒的人才有救。凡是办一桩事或是研究一个问题，大致可分为以下三个步骤： 第一，以科学的方法来分析，使复杂的变成简单; 第二，以公正的态度来计划; 第三，以果断的决心来执行。 这三点，科学的方法，公正的态度，果断的决心，统应该在小学时代养成和学习的。中国历年来工商业的不振，科学的不进步，都是由于主持者没有清醒的头脑。瘟疫流行，水旱灾荒，连年叠见，仍旧还要靠拜忏求神扶乩种种迷信方法。兴办事业，毫无计划，都是吃了头脑不清楚的亏。风水扶乩算命求神等之为迷信，不但为近世科学家所诟病，即我国古代明理之君子亦早深悉而痛绝之。但到如今，大学毕业生和东西洋留学生中，受了环境的同化，而同流合污的很不少。大的企业如久大公司、永利公司和商务印书馆的成功，要算例外了。 近年来政府对社会所办的棉纱厂、面粉厂、硫酸厂、酒精厂和糖厂等，大多数是失败的。失败的原因或是由于调查的时候不用科学方法。譬如办糖厂，应在事先调查在该厂附近地域产多少甘蔗，出产的糖销至何处，成本的多少，赢利的厚薄，与夫国外倾销竞争的状况。若事先不调查清楚，后来必至蚀本倒闭。这类事在中国司空见惯，如汉口的造纸厂，梧州的硫酸厂，真不胜枚举。还有失败的原因是用人行政重情而不重理，这就是没有公正的态度。用人不完全以人才为标准，而喜欢滥用亲戚。每个机关、公司应该多聘专家，计划决定以后，外界无论如何攻击，都得照着计划做去，这样才能成功。 盲从的习惯，我们应该竭力避免，我们不能因为口号叫得响一点，或是主义新一点，就一唱百和的盲从起来。我们大家要静心平气的来观察口号的目的，主义的背景，凭我们的裁判，扪良心来决定我们的主张。若是对的，我们应竭力奉行。若是不对的我们应尽力排除。依违两可，明哲保身的态度，和盲从是一样要避免的。我们要做有主张有作为的人，这样就非有清醒之头脑不可。 现在，要问第二个问题，便是，离开大学以后，将来做什么样的人?我们的人生观应如何?有人认为中国的人生观很受孔孟的影响，实际影响最大的还是老子。孔孟主张见义勇为，老子主张明哲保身;孔孟主张正是非，老子主张明祸福。孟子说：“天之将降大任于斯人也，必先苦其心志，劳其筋骨”，诸葛亮“鞠躬尽瘁，死而后已”，这才不是享福哲学。老子说：“祸莫大于不知足”，又曰“祸兮福所倚，福兮祸所伏”。 中国一般人的最后目的还是享福。我们羡慕人家说某人福气好，娶媳妇进门，即祝之曰“多福多寿多男子”。就是生子的最大目的，也就是想年老的时候可以享福。中国普通人意想中的天堂，是可以不劳而获的一个世界，茶来开口，饭来伸手，这样享福哲学影响于民生问题很大。 人以享福为人生最大目的，中国民族必遭灭亡，历史上罗马之亡可为殷鉴。现在的世界是竞争的世界，如果一个民族还是一味以享受为目的，不肯以服务为目的，必归失败。我们应该以享福为可耻，只有老弱残废才能享福，而以自食其力为光荣。英国国王在幼年时，必在军舰充当小兵，惟其如此方能知兵士的疾苦。全世界最富的人是煤油大王洛克菲勒(Rockefeller)，他的儿子做事从小伙计做起，所以他们的事业能子孙相传不替。 多年前，中日同时派学生留学欧美，中国的学生，一看见各类机械，便问从何处购买?何处最便宜?而日本的学生，只问如何制造?中国人只知道买，以享受为目的，而日本人则重做，以服务为目的;中国从前学工学农的人，统是只叫工人农夫去推动机器，耕耘田亩，而自己却在一边袖手旁观，这样讲究农工业是不会进步的。中国古代轻视劳力，现在已经完全改变，样样应该自己动手，这种人生观的改造，是极重要的。 以上所说的两点：第一，诸位求学，应不仅在科目本身，而且要训练如何能正确地训练自己的思想;第二，我们人生的目的是在能服务，而不在享受。 ——刊于《国立浙江大学日刊》第20号（1936年9月23日）“讲演”栏]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>感想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[华为云图引擎服务实时推荐（Real-time Recommendation）算法]]></title>
    <url>%2Fhuawei-real-time-recommendation%2F</url>
    <content type="text"><![CDATA[概述图引擎服务提供的实时推荐算法是一种基于随机游走模型的实时推荐算法，能够推荐与输入节点相近程度高、关系或喜好相近的节点。 实时推荐过程主要是基于对图上多起点“source”的随机游走：其根据各节点的重要性给各请求节点分配不同的游走步长， 同时对各请求节点进行随机游走，综合各起点随机游走下候选节点的得分结果，进行推荐。 其中，随机游走过程：从请求节点”source“沿着边，根据一定的倾向性不断地“游走“到相邻的节点，达到一定步数（步长）时返回起点重新游走。 游走步长：CurrSteps = SampleWalkLength(alpha)[1] 多次这样的游走后， 对于一个节点，如果其在随机游走过程被访问到，且被访问到的次数达到“nv”，则该节点将记入候选推荐的节点。若某个source节点的候选推荐节点达到“np”，对于该source节点的随机游走将提前结束，或者达到总的游走步数上限”N“时结束。 随机游走过程中，随着被经过的次数的增加，候选节点被推荐的得分也相应增加；当所有的随机游走结束时，系统将给出各候选节点的综合得分，得分越高被推荐的程度越大。 参数说明sources： 给定节点ID，点击”+”可添加多个节点，最多不超过30个。该参数必填。 alpha：权重系数, 其值越大步长越长, 取值范围（0~1）, 实数，默认值为0.85。 N：总的游走步数, [1,200000], 正整数，默认值10000。 nv：候选推荐节点所需访问次数的最小值, 取值范围(0,10], 正整数，默认值5。 np：游走过程提前结束参数：候选推荐节点个数。取值范围 [1,2000], 正整数，默认值1000。 label：希望输出的点的类型。其值为空时，将不考虑点的类型，输出算法原始计算结果。对其赋值时，将从计算结果中过滤出具有该“label”的点的返回。 directed：是否考虑边的方向。true 或false，布尔型。默认值true。 参数分析 sources 单个source 例如：“当Mike打开某电影平台，向其推荐电影“时，可以输入sources为Mike, label为movie，其他参数如下图（左），点击运行。页面画布上将会出现含有推荐节点的子图，大小反映了推荐程度。 多个sources 例如：“Leo的某个朋友，浏览了Leo主页，添加Steven Spielberg导演动态,为其进行电影推荐”，这时，我们可以输入参数如下图左，综合考虑这两种兴趣（“Leo”、“Steven Spielberg”）进行实时推荐： alpha alpha和N的值决定了总的游走步长。通过实验得知alpha的取值不应过小。当alpha取一个比较小的值（这里取的是0.3）时，推荐的结果中，得分从201过渡到34，如下图所示。而当alpha取一个较大值，例如0.85的时候就不会出现这种情况。 N 总的游走步数，当达到总的游走步数上限”N“时结束。控制程序何时结束。 nv 候选推荐节点所需访问次数的最小值。 对于一个节点，如果其在随机游走过程被访问到，且被访问到的次数达到“nv”，则该节点将记入候选推荐的节点。 np 若某个source节点的候选推荐节点达到“np”，对于该source节点的随机游走将提前结束。这是一个早停参数。如下图所示，当np=1000时，算法总耗时:0.0616 s，当np=10时，算法总耗时:0.0019 s。可知选择一个合适的np值可以减少耗时，提高效率。 label 控制输出的点的类型。 其值为空时，将不考虑点的类型，输出算法原始计算结果。 对其赋值时，将从计算结果中过滤出具有该“label”的点的返回。 directed 是否考虑边的方向。当directed=true时，可以看到结果画布中Leo节点的入度为0。 参考文献[1] Eksombatchai C, Jindal P, Liu J Z, et al. Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time[J]. 2017.]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>华为云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Natural-Language-Processing-Tasks-and-References]]></title>
    <url>%2FNatural-Language-Processing-Tasks-and-References%2F</url>
    <content type="text"><![CDATA[Text Similarity WIKI Semantic similarity PAPER A Survey of Text Similarity Approaches PAPER Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks PAPER Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks CHALLENGE SemEval-2014 Task 3: Cross-Level Semantic Similarity CHALLENGE SemEval-2014 Task 10: Multilingual Semantic Textual Similarity CHALLENGE SemEval-2017 Task 1: Semantic Textual Similarity WIKI Semantic Textual Similarity Wiki Reference[1] https://github.com/Kyubyong/nlp_tasks]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HEXO备忘]]></title>
    <url>%2Fhexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[hexo常用命令预览访问 1hexo server 部署步骤 123hexo cleanhexo generatehexo deploy 为文章添加附件Hexo配置文件的设置确保你的Hexo的配置文件_config.yml里面有个这个选项配置，并将其置为true 1post_asset_folder: true 插入图片在 \source\_posts 建立一个md文件来写博客，同时建立一个同名的文件夹去存放资源。 1![图片描述](/文件夹/图片名.png)]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The State of the Art in Semantic Representation]]></title>
    <url>%2Fthe-state-of-the-art-in-semantic-repesentation%2F</url>
    <content type="text"><![CDATA[作者：Omri Abend and Ari Rappoport 这是一篇ACL2017的文章，是一篇语义表示方面的综述性论文。作者对当前一些最先进的语义表示方案以及资源（比如 AMR, UCCA, GMB, UDS)进行了一个survey。 概述语义表示在过去的几年中越来越受到人们的关注，并且发表了很多语义表示方面的文章（例如：AMR, UCCA, GMB, UDS）。但是这些文章中很少和句法方案进行比较，评估他们之间的优缺点，也没有清楚的阐明语义表示研究的总体目标。这篇文章通过语义表示领域的最先进的论文和技术进行了survey，填补了这项空白。 这篇文章可以分为四部分： 第一部分详细阐述了文本语义表示（SRT）的目的，作者将语义表示定义为将文本的意思映射为语言发言者所理解的语义表示。 第二部分对文本语义表示包含的主要内容进行了总结，语义表示的内容主要包括谓词论元关系（predicate-argument）、话语关系（discourse relations）和逻辑结构（logical structure）等。 第三部分详细介绍了当前最先进的SRT方案和注释资源，并且分别介绍了它们的评估标准以及与句法方案的比较。 第四部分进行了总结。 语义表示的定义（术语 semantics 在不同的上下文中会有不同意思。）在这篇论文中，作者将语义表示定义为将文本的意思映射为语言发言者(language speaker)所理解的语义表示。因此，语义表示可以看成是提取信息的方法，所提取出来的信息可以由语言发言者直接评估。 提取过程应该可靠且计算效率高。 一种方法是通过外部（文本外）标准或应用程序提取信息。比如支持推理的文本语义表示，推理出文本蕴涵或自然逻辑。 其他例子包括根据支持知识库查询定义语义表示等。 另一个SRT的方法是通过向量空间模型（VSM），从单词到短语和句子级别的所有语言元素建模为向量，它避免使用符号结构。 使用这种方法的通常会调用神经网络，在许多任务上获得优秀的结果，包括词汇任务，如跨语言单词相似度，机器翻译等。 语义内容SRTs的内容的基本组成成分是论元（argument）结构（谁对谁做了什么，何地，何时以及为什么）。换句话说就是事件（events），表示参与者和他们之间的关系。 我们将使用以下作为运行示例：(1) Although Ann was leaving, she gave the present to John. Events. 一个事件包括谓词，它是事件的主要决定因素。 事件还包括论元（参与者，核心元素）和次级关系（修饰符，非核心元素）。示例1通常被视为具有两个事件，由谓词“leaving”和“gave”引起。 Predicates and Arguments. 谓词论元关系被普遍认为是语义表示的基础。大多数语义角色标注（SRL）方案涵盖了各种各样的动词谓词，但不同之处在于他们是否涵盖了名词性谓词和形容词谓词。例如，PropBank（Palmer et al。，2005）是SRL的主要资源之一，涵盖动词，并且在其最新版本中也包括偶数名词和多论元形容词。 FrameNet（Ruppenhofer等，2016）涵盖了所有这些，但也包括不引起事件的关系名词，如“president”。 其他工作涉及出现在句子边界之外的语义参数，或者没有明确出现在文本中的任何地方（Gerber和Chai，2010; Roth和Frank，2015）。 Core and Non-core Arguments. 论元类型之间最常见的区别在于核心和非核心论元。虽然可以在必需论元和可选论元之间进行区别，但在这里我们关注语义维度，它区分了其含义是特定谓词的论元，并且是所描述事件（核心）的必要组成，以及一般谓词（非核心）。 Semantic Roles. 语义角色是论元的类别。 多年来已经在NLP中提出并使用了许多不同的语义角色库，最突出的是FrameNet，以及PropBank。 PropBank的角色集由AMR等后续项目扩展。 另一个突出的语义角色库是VerbNet和之后的项目，它们定义了一组封闭的抽象语义角色集合，适用于所有谓词论元。 Co-reference and Anaphora. 共指关系允许从引用相同实体的不同方式中抽象出来，并且通常包括在语义资源中。 Temporal Relations. NLP中的大多数时间语义工作都集中在事件之间的时间关系上，或者通过根据文本中找到的时间表达式对它们进行时间戳，或者通过预测它们在时间上的相对顺序。 重要资源包括TimeML，它是时间关系的规范语言，以及TempEval系列共享任务和注释语料库。与时间关系相关的是事件之间的因果关系，它们在语言中无处不在，并且是各种应用的核心。 Spatial Relations. 空间关系的表示在语义的认知理论中是关键的，并且在诸如地理信息系统或机器人导航的应用领域中是关键的。 该领域的重要任务包括空间角色标记，该任务包括空间元素和空间关系的识别和分类，例如地点，路径，方向和运动，以及它们的相对结构。 Discourse Relations 话语关系包含事件或更大的语义单元之间的任何语义关系。 例如，在（1）中，leaving和giving事件有时通过CONCESSION类型的话语关系相关，由“although”引起。 此类信息非常有用，通常对于各种NLP任务例如摘要，机器翻译和信息提取至关重要，但在开发此类系统时通常会被忽略。 Logical Structure. 逻辑结构是许多理论语言学中语义分析的基石，也在NLP领域引起了广泛关注。常见的表示通常基于谓词演算的变体，对于需要将文本映射到外部（通常是可执行的）形式语言的应用程序非常有用，例如查询语言或机器人指令。逻辑结构对于识别句子之间的蕴涵关系也很有用，因为一些蕴涵可以通过正式证明者从文本的逻辑结构中计算出来。 Inference and Entailment. 许多语义方案的主要动机是它们支持推理和蕴涵的能力。 语义方案和资源本节简要介绍了SRT的不同方案和资源。 Semantic Role Labeling. SRL方案通常被称为“浅层语义分析”，因为它们专注于论元结构，忽略了其他关系，如话语事件，或者谓词和论元是如何在内部构建的。 SRL方案在它们的事件类型，涵盖的谓词类型，粒度，跨语言适用性，组织原则以及它们与句法的关系方面有所不同。大多数SRL方案相对于某些语法结构定义它们的注释，例如在PropBank的情况下PTB的解析树，或者在FrameNet的情况下为SRL目的定义的专用语法类别。除了上面讨论的PropBank，FrameNet和VerbNet之外，其他值得关注的资源包括Semlink（Loper等，2007），它链接不同资源中的相应条目，如Prop-Bank，FrameNet，VerbNet和WordNet，以及Preposition Supersenses项目（Schneider et al。，2015），侧重于介词引发的角色。 AMR （抽象语义表示）涵盖谓词 - 论元关系，包括语义角色，它适用于各种谓词（包括言语，名词和形容词谓词），修饰词，共指关系，命名实体和一些时间表达。AMR目前不支持高于句子级别的关系到一个语义类别。并且是以英语中心，因此，在跨语言时，AMR面临着困难。 UCCA （通用概念认知注释）是一种跨语言适用的语义注释方案，建立在类型学理论的基础上，主要基于基础语言学理论。UCCA的基础层次侧重于各种类型的论元结构和它们之间的关系。其当前状态中，UCCA比上述方案更粗糙（例如，它不包括语义角色信息）。 然而，它的跨语言中得到很好的推广。UCCA另一个优点是支持非专家的注释。 UDS. Universal Decompositional Semantics 通用分解语义是一种多层方案，目前包括语义角色注释，词义和体类。 然而，UDS表示的骨架结构源自句法依赖性，并且仅包括动词论元结构。 The Prague Dependency Treebank (PDT) Tectogrammatical Layer (PDT-TL)涵盖了丰富的功能和语义差异，例如论元结构（包括语义角色）， 时态，省略号，主题/焦点，共指关系，词义消歧和方言信息。 PDT-TL源于对PDT语法层的抽象，其与语法的密切关系是显而易见的。 CCG-based Schemes. CCG是一种词汇化语法（即，几乎所有语义内容都在词典中编码），它定义了词汇信息如何组成以构成短语和句子含义的理论，并且在各种语义任务中被证明是有效的。 HPSG-based Schemes. 与基于CCG的方案相关的是基于Head-driven Phrase Structure Grammar的SRT，其中句法和语义特征被表示为特征束，其通过统一规则迭代地组成以形成复合单元。 基于HPSG的SRT方案通常使用Minimal Recursion Semantics形式化。 注释语料库和手工制作的语法存在多种语言，并且通常关注论元结构和逻辑语义现象。 OntoNotes 是一个有用的资源，具有多个相互关联的注释层，借用于不同的方案。 这些层包括句法，SRL，共指关系和词义消歧内容。 谓词的某些属性，例如哪些名词是偶数，也被编码。 总而言之，虽然SRT方案在它们支持的内容类型上有所不同，但方案不断发展以不断添加新的内容类型，从而使这些差异不那么重要。 这些方案之间的根本区别在于它们从语法中抽象出来的程度。 例如，AMR和UCCA从语法中抽象出来作为其设计的一部分，而在大多数其他方案中，句法法和语义更加紧密耦合。 评估人类评估是验证SRT方案的最终标准，因为我们将语义定义为语言发言者理解的意义。 确定SRT方案的理想程度是通过人类根据预先指定的要求对文本进行一些语义预测或注释，并将其与从SRT提取的信息进行比较。 另一种评估方法是基于任务的评估。NLP中的许多语义表示都是在考虑应用程序的情况下定义的，这使得这种评估方式显得更加自然。例如，AMR的一个主要动机是它对机器翻译的适用性，使MT成为AMR评估的一个自然（虽然迄今未开发）测试平台。 另一个例子是使用问答来评估基于知识查询中的语义解析。 评估语义方案的另一个常见标准是不变性（invariance），其中语义分析应该在释义或翻译对之间相似。 重要的是，这些评估标准也适用于自动引发表示而非手动定义的情况。 例如，通常通过基于任务的评估或者根据从它们计算的语义特征来评估向量空间表示，其有效性由人类注释者建立（例如，Agirre等人，2013,2014）。 最后，在通过手动注释（而不是通过自动化程序）引入语义方案的情况下，确定指南是否足够清晰以及类别是否定义明确的共同标准是通过为注释者指定一致来衡量注释者之间的一致性。 相同的文本和测量结果的相似性。 措施包括针对AMR的SMATCH测量（Cai和Knight，2013），以及适用于UCCA的DAG的PARSEVAL F-评分（Black等，1991）。 SRT方案在他们的注释者所需的背景和训练方面存在分歧。 一些方案需要广泛的训练（例如，AMR），而其他方案可以（至少部分地）通过众包（例如，UDS）来收集。 其他例子包括FrameNet，它需要专家注释者来创建新的事件，但是使用训练有素的内部注释器将现有的帧应用于文本; QASRL，远程使用非专家注释器; 和UCCA，它使用内部非专家，在初始培训期后证明对非专家注释器的专家没有优势。 GMB的另一种方法是使用在线协作，其中专家协作者参与手动纠正自动创建的表示。 他们进一步采用游戏化策略来收集注释的某些方面。 Universality. 语义分析（通过更多表面形式的分析）的巨大承诺之一是其跨语言潜力。 然而，尽管普遍性在语义学中的理论和应用重要性早已得到认可（Goddard，2011），但普遍语义学的本质仍然未知。 最近，诸如BabelNet（Ehrmann等人，2014），UBY（Gurevych等人，2012）和Open Multilingual Wordnet等项目构建了庞大的多语言语义网络，通过链接Wikipedia和WordNet等资源并使用现代NLP技术处理它们。 然而，此类项目目前侧重于词汇语义和百科全书信息，而不是文本语义。 诸如SRL方案和AMR之类的符号SRT方案也因其跨语言适用性而被研究（Pad’o和Lapata，2009; Sun等，2010; Xue等，2014），表明跨语言的部分可移植性。 已经为多种语言构建了PropBank和FrameNet的翻译版本（例如，Akbik等，2016; Hartmann和Gurevych，2013）。 然而，由于PropBank和FrameNet都是词汇化方案，并且由于词汇在跨语言中大相径庭，因此这些方案在跨语言移植时需要相当大的适应性（Kozhevnikov和Titov，2013）。 正在进行的研究解决了将VerbNet的非语言化角色概括为普遍适用的集合（例如，Schneider等，2015）。 很少有SRT方案将跨语言适用性作为其主要标准之一，例如UCCA和LinGO语法矩阵（Bender和Flickinger，2005），两者都借鉴了类型学理论。 在向量空间中嵌入单词和句子的向量空间模型也被用于引发共享的跨语言空间（Klementiev等，2012; Rajendran等，2015; Wu等，2016）。 然而，需要进一步评估以确定这些表示的可能含义的哪些方面可靠地反映。 结论NLP中的语义表示正在经历快速变化。 传统的语义工作要么使用侧重于特定语义现象的浅层方法，要么采用形式语义理论，这些理论通过语法 - 语义推理理论与句法方案相结合。 近年来，人们越来越关注一种独立于任何句法或分布标准定义语义结构的替代方法，这很大程度上归功于实现这种方法的语义树库的可用性。 语义方案在它们是否固定在文本的单词和短语(例如，所有类型的语义依赖关系和UCCA)中存在分歧(例如，AMR和基于逻辑的表示)。我们不认为这是一个主要的区别，因为大多数非雇定的表示(包括AMR)与句子中的单词保持着密切的亲和力，这可能是因为没有一个可行的词汇分解方案，而依赖结构可以转换成基于逻辑的表示(Reddy et al.， 2016)。在实践中，固定可以促进解析，而非锚定表示可以更灵活地使用不存在单词和语义组件的一一对应。 这篇论文总结了方案之间的主要区别因素是它们与句法方案之间的关系、它们的普遍性程度以及它们对注释者的专业知识的要求，这是解决注释瓶颈的一个重要因素。我们希望通过对语义表示的最先进的研究来促进研究者的讨论，使更多的研究人员接触到语义表示中最紧迫的问题。]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
  </entry>
</search>
