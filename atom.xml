<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>逍遥&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tangguangen.com/"/>
  <updated>2019-01-02T11:57:44.242Z</updated>
  <id>https://tangguangen.com/</id>
  
  <author>
    <name>逍遥</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>桂林、长沙元旦之旅</title>
    <link href="https://tangguangen.com/%E5%85%83%E6%97%A6%E6%B8%B8%E8%AE%B0/"/>
    <id>https://tangguangen.com/元旦游记/</id>
    <published>2019-01-02T10:47:16.000Z</published>
    <updated>2019-01-02T11:57:44.242Z</updated>
    
    <content type="html"><![CDATA[<p>趁着元旦假期去了桂林和长沙，感觉还是很开心的😄。</p><p>桂林山水甲天下，典型的<span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlOTYlODAlRTYlOTYlQUYlRTclODklQjk=" title="https://baike.baidu.com/item/%E5%96%80%E6%96%AF%E7%89%B9">喀斯特<i class="fa fa-external-link"></i></span>地形构成别具一格的桂林山水：“山青、水秀、洞奇、石美”。来到桂林，则必然要去游漓江，乘坐竹筏从杨堤码头出发，沿着漓江经过九马画山最后到兴坪，能够充分的欣赏别具一格的桂林山水。桂林景点非常多，在市区有象鼻山景区和两江四湖，象鼻山听说是桂林的代表，可惜没去成。还有龙脊梯田，芦笛岩等，桂林景点比较分散，交通也不是很发达，所以在桂林两天也就游了漓江、银子岩和两江四湖。如果喜欢桂林的山水，不怕累的话可以在桂林多玩几天，推荐在4、5月份或者10、11月份去。</p><p>长沙是一座非常美丽的城市，有美食美景，还有历史。到了长沙，则必然要去橘子洲头瞻仰青年毛泽东。那一天长沙刚好下大雪，雪中的橘子洲具有别样的景色。</p><a id="more"></a><p>长沙有美景，更有美食，最有名的当然是长沙臭豆腐。长沙有很多小吃街，我们逛得最多的是太平老街，整条街都是吃的。湘菜也是非常美味，我们吃了炊烟时代的小炒黄牛肉，非常赞，本来想去一盏灯吃炒鸭掌筋的，人太多了排不上队。</p><p>还去逛了湖南大学，岳麓书院因为雨雪天气闭园没去成，很可惜，还想去湖南广播电视台玩的，也没去成。长沙好玩的好吃的真是太多了，真的是值得去第二次的城市。</p><p>下面一些照片供大家欣赏</p><p>银子岩</p><p><img src="https://i.imgur.com/I3uPiU8.jpg" alt=""></p><p>20元人民币背景</p><p><img src="https://i.imgur.com/8t85pbm.jpg" alt=""></p><p>桂林双子塔</p><p><img src="https://i.imgur.com/NAHFsum.jpg" alt=""></p><p>青年毛浙东艺术雕塑</p><p><img src="https://i.imgur.com/W0WCLxH.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;趁着元旦假期去了桂林和长沙，感觉还是很开心的😄。&lt;/p&gt;
&lt;p&gt;桂林山水甲天下，典型的&lt;a href=&quot;https://baike.baidu.com/item/%E5%96%80%E6%96%AF%E7%89%B9&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;喀斯特&lt;/a&gt;地形构成别具一格的桂林山水：“山青、水秀、洞奇、石美”。来到桂林，则必然要去游漓江，乘坐竹筏从杨堤码头出发，沿着漓江经过九马画山最后到兴坪，能够充分的欣赏别具一格的桂林山水。桂林景点非常多，在市区有象鼻山景区和两江四湖，象鼻山听说是桂林的代表，可惜没去成。还有龙脊梯田，芦笛岩等，桂林景点比较分散，交通也不是很发达，所以在桂林两天也就游了漓江、银子岩和两江四湖。如果喜欢桂林的山水，不怕累的话可以在桂林多玩几天，推荐在4、5月份或者10、11月份去。&lt;/p&gt;
&lt;p&gt;长沙是一座非常美丽的城市，有美食美景，还有历史。到了长沙，则必然要去橘子洲头瞻仰青年毛泽东。那一天长沙刚好下大雪，雪中的橘子洲具有别样的景色。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://tangguangen.com/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>设计模式之工厂方法模式，附Java代码示例</title>
    <link href="https://tangguangen.com/factory-method-pattern/"/>
    <id>https://tangguangen.com/factory-method-pattern/</id>
    <published>2018-12-21T07:08:14.000Z</published>
    <updated>2018-12-21T08:05:37.442Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模式定义"><a href="#模式定义" class="headerlink" title="模式定义"></a>模式定义</h2><p><strong>工厂方法模式</strong>(Factory Method Pattern)又称为工厂模式，也叫<strong>虚拟构造器</strong>(<strong>Virtual Constructor</strong>)模式或者多态工厂(Polymorphic Factory)模式，它属于类创建型模式。在工厂方法模式中，工厂父类负责定义创建产品对象的公共接口，而工厂子类则负责生成具体的产品对象，这样做的目的是将产品类的实例化操作延迟到工厂子类中完成，即通过工厂子类来确定究竟应该实例化哪一个具体产品类。</p><p>概念有点抽象，给大家举个栗子：</p><blockquote><p>铁匠制造武器。精灵需要精灵的武器，兽人需要兽人的武器。根据不同的客户，召唤正确类型的铁匠。</p></blockquote><p>它提供了一种将实例化逻辑委托给子类的方法。</p><a id="more"></a><h2 id="模式结构"><a href="#模式结构" class="headerlink" title="模式结构"></a>模式结构</h2><p>工厂方法模式包含如下角色：</p><ul><li>Product：抽象产品</li><li>ConcreteProduct：具体产品</li><li>Factory：抽象工厂</li><li>ConcreteFactory：具体工厂</li></ul><p>在铁匠那个例子中，武器是抽象产品，精灵的武器和兽人的武器是具体产品；铁匠是抽象工厂，能够打造具体武器的铁匠是具体工厂。</p><h2 id="程序实例"><a href="#程序实例" class="headerlink" title="程序实例"></a>程序实例</h2><p>以铁匠为例。首先，我们有一个blacksmith接口和它的一些实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Blacksmith</span> </span>&#123;</span><br><span class="line">  <span class="function">Weapon <span class="title">manufactureWeapon</span><span class="params">(WeaponType weaponType)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ElfBlacksmith</span> <span class="keyword">implements</span> <span class="title">Blacksmith</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Weapon <span class="title">manufactureWeapon</span><span class="params">(WeaponType weaponType)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ElfWeapon(weaponType);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrcBlacksmith</span> <span class="keyword">implements</span> <span class="title">Blacksmith</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Weapon <span class="title">manufactureWeapon</span><span class="params">(WeaponType weaponType)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> OrcWeapon(weaponType);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在，随着客户的到来，正确类型的铁匠被召集起来，要求制造武器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Blacksmith blacksmith = <span class="keyword">new</span> ElfBlacksmith();</span><br><span class="line">blacksmith.manufactureWeapon(WeaponType.SPEAR);</span><br><span class="line">blacksmith.manufactureWeapon(WeaponType.AXE);</span><br><span class="line"><span class="comment">// Elvish weapons are created</span></span><br></pre></td></tr></table></figure><h2 id="适用环境"><a href="#适用环境" class="headerlink" title="适用环境"></a>适用环境</h2><p>适合工厂方法模式的情形：</p><ul><li>一个类不知道它所需要的对象的类：在工厂方法模式中，客户端不需要知道具体产品类的类名，只需要知道所对应的工厂即可，具体的产品对象由具体工厂类创建；客户端需要知道创建具体产品的工厂类。</li><li>一个类通过其子类来指定创建哪个对象：在工厂方法模式中，对于抽象工厂类只需要提供一个创建产品的接口，而由其子类来确定具体要创建的对象，利用面向对象的多态性和里氏代换原则，在程序运行时，子类对象将覆盖父类对象，从而使得系统更容易扩展。</li><li>将创建对象的任务委托给多个工厂子类中的某一个，客户端在使用时可以无须关心是哪一个工厂子类创建产品子类，需要时再动态指定，可将具体工厂类的类名存储在配置文件或数据库中。</li></ul><p><strong>参考：</strong></p><p><span class="exturl" data-url="aHR0cHM6Ly9kZXNpZ24tcGF0dGVybnMucmVhZHRoZWRvY3MuaW8vemhfQ04vbGF0ZXN0L2NyZWF0aW9uYWxfcGF0dGVybnMvZmFjdG9yeV9tZXRob2QuaHRtbA==" title="https://design-patterns.readthedocs.io/zh_CN/latest/creational_patterns/factory_method.html">图说设计模式<i class="fa fa-external-link"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9qYXZhLWRlc2lnbi1wYXR0ZXJucy5jb20vcGF0dGVybnMvZmFjdG9yeS1tZXRob2QvI2ludGVudA==" title="https://java-design-patterns.com/patterns/factory-method/#intent">https://java-design-patterns.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;模式定义&quot;&gt;&lt;a href=&quot;#模式定义&quot; class=&quot;headerlink&quot; title=&quot;模式定义&quot;&gt;&lt;/a&gt;模式定义&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;工厂方法模式&lt;/strong&gt;(Factory Method Pattern)又称为工厂模式，也叫&lt;strong&gt;虚拟构造器&lt;/strong&gt;(&lt;strong&gt;Virtual Constructor&lt;/strong&gt;)模式或者多态工厂(Polymorphic Factory)模式，它属于类创建型模式。在工厂方法模式中，工厂父类负责定义创建产品对象的公共接口，而工厂子类则负责生成具体的产品对象，这样做的目的是将产品类的实例化操作延迟到工厂子类中完成，即通过工厂子类来确定究竟应该实例化哪一个具体产品类。&lt;/p&gt;
&lt;p&gt;概念有点抽象，给大家举个栗子：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;铁匠制造武器。精灵需要精灵的武器，兽人需要兽人的武器。根据不同的客户，召唤正确类型的铁匠。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;它提供了一种将实例化逻辑委托给子类的方法。&lt;/p&gt;
    
    </summary>
    
      <category term="设计模式" scheme="https://tangguangen.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
    
      <category term="java" scheme="https://tangguangen.com/tags/java/"/>
    
      <category term="设计模式" scheme="https://tangguangen.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>看懂UML类图和时序图</title>
    <link href="https://tangguangen.com/design-patterns-uml/"/>
    <id>https://tangguangen.com/design-patterns-uml/</id>
    <published>2018-12-19T07:25:30.000Z</published>
    <updated>2018-12-21T08:10:49.036Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>参考：<span class="exturl" data-url="aHR0cHM6Ly9kZXNpZ24tcGF0dGVybnMucmVhZHRoZWRvY3MuaW8vemhfQ04vbGF0ZXN0L3JlYWRfdW1sLmh0bWw=" title="https://design-patterns.readthedocs.io/zh_CN/latest/read_uml.html">图说设计模式<i class="fa fa-external-link"></i></span></p></blockquote><h2 id="类之间的六种关系"><a href="#类之间的六种关系" class="headerlink" title="类之间的六种关系"></a>类之间的六种关系</h2><p>类的继承结构表现在UML中为：泛化(generalize)与实现(realize)。</p><h3 id="泛化关系-generalization"><a href="#泛化关系-generalization" class="headerlink" title="泛化关系(generalization)"></a>泛化关系(generalization)</h3><p>继承关系为 is-a的关系；两个对象之间如果可以用 is-a 来表示，就是继承关系：（..是..)</p><p>eg：自行车是车、猫是动物</p><p>泛化关系用一条<strong>带空心箭头的实线</strong>表示；如下图表示（A继承自B）；</p><p>注：最终代码中，<strong>泛化关系表现为继承非抽象类</strong>；</p><a id="more"></a><h3 id="实现关系-realize"><a href="#实现关系-realize" class="headerlink" title="实现关系(realize)"></a>实现关系(realize)</h3><p>实现关系用<strong>一条带空心箭头的虚线</strong>表示；</p><p>注：最终代码中，实现关系表现为继承抽象类；</p><h3 id="聚合关系-aggregation"><a href="#聚合关系-aggregation" class="headerlink" title="聚合关系(aggregation)"></a>聚合关系(aggregation)</h3><p>聚合关系用一条<strong>带空心菱形箭头的实线</strong>表示，如下图表示A聚合到B上，或者说B由A组成；</p><p>聚合关系用于表示实体对象之间的关系，<strong>表示整体由部分构成的语义</strong>；例如一个部门由多个员工组成；</p><p>与组合关系不同的是，整体和部分不是强依赖的，即使整体不存在了，部分仍然存在；例如， 部门撤销了，人员不会消失，他们依然存在；</p><h3 id="组合关系-composition"><a href="#组合关系-composition" class="headerlink" title="组合关系(composition)"></a>组合关系(composition)</h3><p>组合关系用<strong>一条带实心菱形箭头直线</strong>表示，如下图表示A组成B，或者B由A组成；</p><p>与聚合关系一样，组合关系同样表示整体由部分构成的语义；比如公司由多个部门组成；</p><p>但组合关系是一种强依赖的特殊聚合关系，如果整体不存在了，则部分也不存在了；例如， 公司不存在了，部门也将不存在了；</p><h3 id="关联关系-association"><a href="#关联关系-association" class="headerlink" title="关联关系(association)"></a>关联关系(association)</h3><p>关联关系是用<strong>一条直线</strong>表示的；它描述不同类的对象之间的结构关系；它是一种静态关系， 通常与运行状态无关，一般由常识等因素决定的；它一般用来定义对象之间静态的、天然的结构； 所以，关联关系是一种“强关联”的关系；</p><p>比如，乘车人和车票之间就是一种关联关系；学生和学校就是一种关联关系；</p><p>关联关系<strong>默认不强调方向</strong>，表示对象间相互知道；如果特别强调方向，如下图，表示A知道B，但 B不知道A；</p><p>注：在最终代码中，关联对象通常是以成员变量的形式实现的；</p><h3 id="依赖关系-dependency"><a href="#依赖关系-dependency" class="headerlink" title="依赖关系(dependency)"></a>依赖关系(dependency)</h3><p>依赖关系是用<strong>一套带箭头的虚线</strong>表示的；如下图表示A依赖于B；他描述一个对象在运行期间会用到另一个对象的关系；</p><p>与关联关系不同的是，它是一种临时性的关系，通常在运行期间产生，并且随着运行时的变化； 依赖关系也可能发生变化；</p><p>显然，依赖也有方向，双向依赖是一种非常糟糕的结构，我们总是应该保持单向依赖，杜绝双向依赖的产生；</p><p>注：在最终代码中，<strong>依赖关系体现为类构造方法及类方法的传入参数</strong>，箭头的指向为调用关系；依赖关系除了临时知道对方外，还是“使用”对方的方法和属性；</p><h2 id="时序图"><a href="#时序图" class="headerlink" title="时序图"></a>时序图</h2><p>时序图（Sequence Diagram）是显示对象之间交互的图，这些对象是按时间顺序排列的。时序图中显示的是参与交互的对象及其对象之间消息交互的顺序。</p><p>时序图包括的建模元素主要有：对象（Actor）、生命线（Lifeline）、控制焦点（Focus of control）、消息（Message）等等。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://design-patterns.readthedocs.io/zh_CN/latest/read_uml.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;图说设计模式&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;类之间的六种关系&quot;&gt;&lt;a href=&quot;#类之间的六种关系&quot; class=&quot;headerlink&quot; title=&quot;类之间的六种关系&quot;&gt;&lt;/a&gt;类之间的六种关系&lt;/h2&gt;&lt;p&gt;类的继承结构表现在UML中为：泛化(generalize)与实现(realize)。&lt;/p&gt;
&lt;h3 id=&quot;泛化关系-generalization&quot;&gt;&lt;a href=&quot;#泛化关系-generalization&quot; class=&quot;headerlink&quot; title=&quot;泛化关系(generalization)&quot;&gt;&lt;/a&gt;泛化关系(generalization)&lt;/h3&gt;&lt;p&gt;继承关系为 is-a的关系；两个对象之间如果可以用 is-a 来表示，就是继承关系：（..是..)&lt;/p&gt;
&lt;p&gt;eg：自行车是车、猫是动物&lt;/p&gt;
&lt;p&gt;泛化关系用一条&lt;strong&gt;带空心箭头的实线&lt;/strong&gt;表示；如下图表示（A继承自B）；&lt;/p&gt;
&lt;p&gt;注：最终代码中，&lt;strong&gt;泛化关系表现为继承非抽象类&lt;/strong&gt;；&lt;/p&gt;
    
    </summary>
    
      <category term="设计模式" scheme="https://tangguangen.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
    
      <category term="UML" scheme="https://tangguangen.com/tags/UML/"/>
    
  </entry>
  
  <entry>
    <title>如何学习之“组块化”总结</title>
    <link href="https://tangguangen.com/how-to-learn-chunkings-summary/"/>
    <id>https://tangguangen.com/how-to-learn-chunkings-summary/</id>
    <published>2018-12-14T11:00:01.000Z</published>
    <updated>2018-12-14T11:16:52.609Z</updated>
    
    <content type="html"><![CDATA[<p>从神经科学的角度说，<strong>组块</strong>是信息片段通过使用，经常也通过实际意义联系在一起。你可以把组块看成是闪烁的神经网络，将关键想法或动作紧密联系在一起。组块可以扩大和复杂化，同时也是你调动记忆内容的捷径，可以像缎带一样落入工作记忆的插槽中。</p><p>构建组块的最佳方式是 高度集中的注意力 对基本概念的理解 以及通过练习帮助你加深对模式和更大范围的情境的理解。<strong>回顾</strong> 脱离书本努力想起关键点 是促进组块化的最佳方式之一 。这似乎能帮助形成神经挂钩 帮助你更好地理解材料。另一种方式是 尝试在最初的学习场所外来回忆材料 这会使记忆更加深刻 容易调动 任何地方都可以 这对于考试非常有帮助。</p><a id="more"></a><p><strong>知识迁移</strong>是指 你在某个领域掌握的组块 可以帮助你学习另一个领域的组块 两个领域可能具有惊人的共通性。</p><p><strong>交替学习</strong> 选出的各种不同概念 方法与技术 在同一段时间全部加以练习 组块非常重要 但并不意味着能提高灵活性 而这一点对于真正精通所学内容十分重要 学习时对能力的错觉 也有研究 学会意识到你是否在欺骗自己 你是否真的在对材料进行学习 时不时自测 用一些小测验看看自己是否真正理解材料 还是在欺骗自己 你认为自己在学习 事实却不是这样 回忆 也是一种小测验的方式 要避免过度依赖拿荧光笔给要点做标记 这会让你以为自己记住了那些内容 实际却并未掌握 学习时不要害怕犯错 这能让你察觉到对自己能力的错觉 不要只练习简单的部分 这会给你一种自己已完全掌握材料的错觉 有意练习你认为困难的部分 这样可以更好地全面掌握材料 思维定势 是对事物已有的看法 或者说是你已经充分发展巩固了的神经模式 这会妨碍你产生更好的想法 或者让你不能灵活地接受那些 更好或更合适的新方法 幸运法则也很有帮助 幸运女神会眷顾努力之人 从微不足道的点开始学起 一个接一个 不断努力 你会得到满意的结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从神经科学的角度说，&lt;strong&gt;组块&lt;/strong&gt;是信息片段通过使用，经常也通过实际意义联系在一起。你可以把组块看成是闪烁的神经网络，将关键想法或动作紧密联系在一起。组块可以扩大和复杂化，同时也是你调动记忆内容的捷径，可以像缎带一样落入工作记忆的插槽中。&lt;/p&gt;
&lt;p&gt;构建组块的最佳方式是 高度集中的注意力 对基本概念的理解 以及通过练习帮助你加深对模式和更大范围的情境的理解。&lt;strong&gt;回顾&lt;/strong&gt; 脱离书本努力想起关键点 是促进组块化的最佳方式之一 。这似乎能帮助形成神经挂钩 帮助你更好地理解材料。另一种方式是 尝试在最初的学习场所外来回忆材料 这会使记忆更加深刻 容易调动 任何地方都可以 这对于考试非常有帮助。&lt;/p&gt;
    
    </summary>
    
      <category term="如何学习" scheme="https://tangguangen.com/categories/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>如何学习之回顾法</title>
    <link href="https://tangguangen.com/how-to-learn-recall/"/>
    <id>https://tangguangen.com/how-to-learn-recall/</id>
    <published>2018-12-13T10:59:16.000Z</published>
    <updated>2018-12-13T11:16:19.188Z</updated>
    
    <content type="html"><![CDATA[<p>最常见的学习方法之一 就是反复阅读， 不过心理学家Jeffrey Karpicke证明：这种方法的成效远不及另一种简单技巧，<strong>回顾——阅读材料后，移开视线，看看你能回忆起多少内容。</strong></p><p>在回顾知识时，我们并非机械地复述，而是在<strong>通过回顾</strong>这个过程<strong>加深理解</strong>，这有助于我们形成知识组块，就好像回忆过程帮助我们在神经上嵌入了“钩子” 以便我们串联起前后知识。</p><a id="more"></a><p>更让研究者们出乎意料的是，学生们单纯地阅读和回顾材料并不是最佳的学习方法。他们认为<strong>思维导图</strong>——即画出概念之间的联系才是最佳途径。然而根基还没打牢就开始空建框架联系，实属徒劳无功 。</p><p>比起被动重复阅读，<strong>回顾即在心里检索关键概念可以使你的学习更加专注高效</strong>。只有隔上一定时间后再重读才会有效果，因为重读就更像是间隔重复练习。</p><p>回顾是一种有效工具，不过这里有另一个小贴士：在常<strong>规学习场所以外回顾材料</strong>会帮助你加深对材料的理解。你可能没有意识到这一点，但是当你学习新事物的时候，你通常会把最开始接触材料的地方当作潜意识中的提示，但一到考试就乱了阵脚。因为考试与学习场所通常不同，通过在不同物理环境下回顾和思考学习资料，你会脱离对给定场所的依赖，这会帮助你避免由于考试与学习场所的不同而产生的问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最常见的学习方法之一 就是反复阅读， 不过心理学家Jeffrey Karpicke证明：这种方法的成效远不及另一种简单技巧，&lt;strong&gt;回顾——阅读材料后，移开视线，看看你能回忆起多少内容。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在回顾知识时，我们并非机械地复述，而是在&lt;strong&gt;通过回顾&lt;/strong&gt;这个过程&lt;strong&gt;加深理解&lt;/strong&gt;，这有助于我们形成知识组块，就好像回忆过程帮助我们在神经上嵌入了“钩子” 以便我们串联起前后知识。&lt;/p&gt;
    
    </summary>
    
      <category term="如何学习" scheme="https://tangguangen.com/categories/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>学术英语之12种动词时态</title>
    <link href="https://tangguangen.com/academic-english-verb-tenses/"/>
    <id>https://tangguangen.com/academic-english-verb-tenses/</id>
    <published>2018-12-11T04:22:42.000Z</published>
    <updated>2018-12-11T12:45:18.231Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Simple-Tenses"><a href="#Simple-Tenses" class="headerlink" title="Simple Tenses"></a>Simple Tenses</h2><p>首先是一般时态，一般现在时，一般过去时，一般将来时</p><ul><li>Simple Present Tense<ul><li>Jerry play<strong>s</strong> tennis everyday.</li></ul></li><li>Simple Past Tense<ul><li>Jerry played tennis yesterday.</li></ul></li><li>Simple Future Tense<ul><li>Jerry will play tennis next weekend.</li></ul></li></ul><p><strong>一般现在时</strong>用来表示通常会发生或不会发生的事情，它表示一种习惯，因此“Jerry每天都打网球”表示他经常做的事情。动词plays,你会发现末尾有个s对于单数名词来讲，在<strong>一般现在时态里我们必须在动词末尾加s</strong>，如果是复数名词，你在动词末尾不能再加任何字母。</p><p><strong>一般过去时</strong>表示我们想说明某事发生在过去并且在过去某个特定时间已经做完，Jerry昨天打了网球，当他做某事的时候是个特定时间，为了表示动作发生在一般过去时，对于常规动词来讲，只要末尾加-ed。比较难得部分是不常规动词比如sit或eat你必须要去背所有这些不常规动词的特殊的过去时态。</p><p>要表示<strong>一般将来时</strong>，我们在<strong>动词原形前加上单词will</strong>所以，will play用来表示将来时态。</p><a id="more"></a><h2 id="Progressive-Tenses"><a href="#Progressive-Tenses" class="headerlink" title="Progressive Tenses"></a>Progressive Tenses</h2><p>进行时</p><ul><li>Present Progressive<ul><li>Jennifer is walking to class.</li></ul></li><li>Past Progressive<ul><li>Jennifer was walking to class.</li></ul></li><li>Future Progressive<ul><li>Jennifer will be walking to class.</li></ul></li></ul><p><strong>现在进行时</strong>表示正在发生的事情。为了表示现在进行时，我们两个动词部分我们要有be动词和动词ing形式，如果主语是单数，你就用单数be动词is如果主语是复数比如说Jennifer and Mary 那你就用be动词are, 加上动词ing。</p><p><strong>过去进行时</strong>我们要表达的是一个动作在过去某段时间持续进行。同样的，我们要有动词的两个部分，be动词和-ing 但是在过去进行时里，你要用动词的过去形式，was 和 were ，was表示单数，were表示复数，然后加上动词ing将来进行时表示某件事情在未来的某段时间里会持续发生。</p><p>要表示<strong>将来进行时</strong>，我们要用到will加be,再加上动词ing形式will后面是自动的要跟动词原形的，所以我们不需要is和are在will后你跟be动词原形再加动词ing形式。</p><h2 id="Perfect-Tenses"><a href="#Perfect-Tenses" class="headerlink" title="Perfect Tenses"></a>Perfect Tenses</h2><p>完成时态</p><ul><li>Present Perfect<ul><li>Steve has eaten sushi before.</li></ul></li><li>Past Perfect<ul><li>Alan had not studied before he took the test.</li></ul></li><li>Future Perfect<ul><li>By next weekend, I will have seen the new movies six times.</li></ul></li></ul><p><strong>现在完成时</strong>用来表示某事发生在现在之前的某个不确定时间里，或者某事在过去经常发生大部分时间，但是同样的是个不确定的时间。现在完成时的重点是，表示<strong>事情发生在过去即现在之前</strong>。因此他和当前有联系，但是事情来自过去在第一个例子里，Steve has eaten sushi before 他什么时候吃了寿司？之前的某个时间，但我们不知道具体什么时候。</p><p>要表示现在完成时，我们需要两个部分的动词我们需要现在时态的动词have如果是单数的话我们用has，然后我们要一个过去分词eat的过去分词是eaten</p><p><strong>过去完成时</strong>有点难，过去完成时与现在没有任何关系，这就是它和现在完成时的区别。现在完成时“现在”很重要，过去完成时“现在”是不重要的。</p><p>当我们使用过去完成时的时候，我们是在讲过去的两个动作我们要表达的是<strong>一个动作发生在另一个动作之前</strong>都是过去的动作。因此，在这个例子里，Alan had not studied before he took the test 是哪两个动作？学习和考试所以我们要表达的是，在过去他考试之前没有学习，这是一个否定句，过去完成时由have的过去时态也就是had，以及study的过去分词，也就是studied，构成。在这里动词只是一般过去形式，一个动作发生在另一个动作之前就是过去完成时了</p><p><strong>将来完成时</strong>涉及到将来的将来的两个动作，同样的，“现在”在将来完成时里并不重要。</p><p>在这个例子里，I will have seen the new movie six times by nest weekend 有两个时间节点，看电影和下个周末。所以在下个周末之前，我已经看了这部电影。表达一个事情将发生在另一个之前，我是用将来完成时，will have seen我用will 加上动词have 的原型，因为它在will之后 以及see的过去分词，也就是seen将来完成时告诉我这个动作将会发生在这个时间之前。</p><h2 id="Perfect-Progressive-Tenses"><a href="#Perfect-Progressive-Tenses" class="headerlink" title="Perfect Progressive Tenses"></a>Perfect Progressive Tenses</h2><ul><li>Present Perfect Progressive<ul><li>She has been waiting for a long time.</li></ul></li><li>Past Perfect Progressive<ul><li>He had been sleeping for ten hours.</li></ul></li><li>Future Perfect Progressive<ul><li>We will have been studing for a month.</li></ul></li></ul><p><strong>现在完成进行时</strong>有三个动词部分,你要有have动词，have或has。因为这表示完成，就在这个时候已经完成。你要有be动词的过去分词been，因为这也是完成时一部分，你还需要在be后面加-ing动词，这表示动作持续进行。</p><p>一般现在完成时表示某事情开始于之前并且持续进行，而且可能持续更长时间。一般现在完成时和现在完成时的差别不是很大，重点在于当时动作在继续进行。</p><p><strong>过去完成进行</strong>时表示某事发生在过去，并且可能继续发生在另一件事之前。He had been sleeping for ten hours. 那表示他一直在进行这个行为，持续了10个小时，重点在于他做这件事情的时间长度。过去完成进行时的构成是，have的过去式，因此我们又要用had了然后be动词的过去分词been, 最后一个-ing动词</p><p><strong>将来完成进行时</strong>强调一个发生在未来的持续动作，这个动作还会保持继续。We will have been studying for a month 和将来完成时有相同的意思，we will have studied for a month但是因为它既是完成时又是进行时我们强调的是在某个时间段里动作的持续进行</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Simple-Tenses&quot;&gt;&lt;a href=&quot;#Simple-Tenses&quot; class=&quot;headerlink&quot; title=&quot;Simple Tenses&quot;&gt;&lt;/a&gt;Simple Tenses&lt;/h2&gt;&lt;p&gt;首先是一般时态，一般现在时，一般过去时，一般将来时&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple Present Tense&lt;ul&gt;
&lt;li&gt;Jerry play&lt;strong&gt;s&lt;/strong&gt; tennis everyday.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simple Past Tense&lt;ul&gt;
&lt;li&gt;Jerry played tennis yesterday.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simple Future Tense&lt;ul&gt;
&lt;li&gt;Jerry will play tennis next weekend.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;一般现在时&lt;/strong&gt;用来表示通常会发生或不会发生的事情，它表示一种习惯，因此“Jerry每天都打网球”表示他经常做的事情。动词plays,你会发现末尾有个s对于单数名词来讲，在&lt;strong&gt;一般现在时态里我们必须在动词末尾加s&lt;/strong&gt;，如果是复数名词，你在动词末尾不能再加任何字母。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一般过去时&lt;/strong&gt;表示我们想说明某事发生在过去并且在过去某个特定时间已经做完，Jerry昨天打了网球，当他做某事的时候是个特定时间，为了表示动作发生在一般过去时，对于常规动词来讲，只要末尾加-ed。比较难得部分是不常规动词比如sit或eat你必须要去背所有这些不常规动词的特殊的过去时态。&lt;/p&gt;
&lt;p&gt;要表示&lt;strong&gt;一般将来时&lt;/strong&gt;，我们在&lt;strong&gt;动词原形前加上单词will&lt;/strong&gt;所以，will play用来表示将来时态。&lt;/p&gt;
    
    </summary>
    
      <category term="Academic English" scheme="https://tangguangen.com/categories/Academic-English/"/>
    
    
      <category term="English" scheme="https://tangguangen.com/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>二叉树遍历</title>
    <link href="https://tangguangen.com/binary-tree-traversal/"/>
    <id>https://tangguangen.com/binary-tree-traversal/</id>
    <published>2018-12-10T09:29:58.000Z</published>
    <updated>2018-12-10T09:57:00.834Z</updated>
    
    <content type="html"><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vc29uZ3dlbmppZS9wLzg5NTU4NTYuaHRtbA==" title="https://www.cnblogs.com/songwenjie/p/8955856.html">https://www.cnblogs.com/songwenjie/p/8955856.html<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vc29uZ3dlbmppZS9wLzg5NTU4NTYuaHRtbA==&quot; title=&quot;https://www.cnblogs.com/songw
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>杭州的雪</title>
    <link href="https://tangguangen.com/xihulake-snow/"/>
    <id>https://tangguangen.com/xihulake-snow/</id>
    <published>2018-12-10T01:24:18.000Z</published>
    <updated>2018-12-10T01:55:53.753Z</updated>
    
    <content type="html"><![CDATA[<p>今年杭州早早就下起了大雪，来杭州两年，有幸遇见两场大雪。</p><p>大雪恰逢周末，于是一大早就去西湖赏雪，看断桥残雪。</p><p>断桥上还是熟悉的场景﻿﻿</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-25f0cc7bf4dd9f93.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50" alt=""></p><a id="more"></a><p>西湖的雪景还是非常美的</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-6c085dd9a45d73fe.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50" alt=""></p><p>大爱西湖</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-c927e84bbb0e7bfe.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50" alt=""></p><p>雪后的码头</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-34ef88a406b0a2d0.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50" alt=""></p><p>登上了宝石山，山顶的景色很特别</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-6d515353b298295e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50" alt=""></p><p>﻿山顶看白堤</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-d29c55cb8c381cb7.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50" alt=""></p><p>﻿哈哈哈，我堆的</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-14b9adddf4cbfdde.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50" alt=""></p><p>﻿﻿山顶雪景很美，人也非常多</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-3048c84cf6a135cc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50" alt=""></p><p>冬菊</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-748d3e9f9330bc56.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今年杭州早早就下起了大雪，来杭州两年，有幸遇见两场大雪。&lt;/p&gt;
&lt;p&gt;大雪恰逢周末，于是一大早就去西湖赏雪，看断桥残雪。&lt;/p&gt;
&lt;p&gt;断桥上还是熟悉的场景﻿﻿&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/15167924-25f0cc7bf4dd9f93.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://tangguangen.com/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="杭州" scheme="https://tangguangen.com/tags/%E6%9D%AD%E5%B7%9E/"/>
    
      <category term="断桥残雪" scheme="https://tangguangen.com/tags/%E6%96%AD%E6%A1%A5%E6%AE%8B%E9%9B%AA/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 226. 翻转二叉树</title>
    <link href="https://tangguangen.com/leetcode-226-invert-binary-tree/"/>
    <id>https://tangguangen.com/leetcode-226-invert-binary-tree/</id>
    <published>2018-12-07T04:42:39.000Z</published>
    <updated>2018-12-07T05:09:28.618Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>翻转一棵二叉树。</p><p><strong>示例：</strong></p><p>输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;      4</span><br><span class="line">&gt;    /   \</span><br><span class="line">&gt;   2     7</span><br><span class="line">&gt;  / \   / \</span><br><span class="line">&gt; 1   3 6   9</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;      4</span><br><span class="line">&gt;    /   \</span><br><span class="line">&gt;   7     2</span><br><span class="line">&gt;  / \   / \</span><br><span class="line">&gt; 9   6 3   1</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>备注:</strong><br>这个问题是受到 <span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9teGNs" title="https://twitter.com/mxcl">Max Howell <i class="fa fa-external-link"></i></span>的 <span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9teGNsL3N0YXR1cy82MDg2ODIwMTYyMDUzNDQ3Njg=" title="https://twitter.com/mxcl/status/608682016205344768">原问题<i class="fa fa-external-link"></i></span> 启发的 ：</p><blockquote><p>谷歌：我们90％的工程师使用您编写的软件(Homebrew)，但是您却无法在面试时在白板上写出翻转二叉树这道题，这太糟糕了。</p></blockquote></blockquote><a id="more"></a><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>递归</p><h2 id="AC代码（Java）"><a href="#AC代码（Java）" class="headerlink" title="AC代码（Java）"></a>AC代码（Java）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * public class TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode left;</span></span><br><span class="line"><span class="comment"> *     TreeNode right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) &#123; val = x; &#125;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> TreeNode <span class="title">invertTree</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (root == <span class="keyword">null</span>)</span><br><span class="line"><span class="keyword">return</span> root;</span><br><span class="line">TreeNode tmp = root.left;</span><br><span class="line">root.left = invertTree(root.right);</span><br><span class="line">root.right = invertTree(tmp);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;p&gt;翻转一棵二叉树。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;      4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;    /   \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;   2     7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;  / \   / \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 1   3 6   9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;输出：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;      4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;    /   \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;   7     2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;  / \   / \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 9   6 3   1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;备注:&lt;/strong&gt;&lt;br&gt;这个问题是受到 &lt;a href=&quot;https://twitter.com/mxcl&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Max Howell &lt;/a&gt;的 &lt;a href=&quot;https://twitter.com/mxcl/status/608682016205344768&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;原问题&lt;/a&gt; 启发的 ：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;谷歌：我们90％的工程师使用您编写的软件(Homebrew)，但是您却无法在面试时在白板上写出翻转二叉树这道题，这太糟糕了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
      <category term="java" scheme="https://tangguangen.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 219. 存在重复元素 II</title>
    <link href="https://tangguangen.com/leetcode-219-contain-duplicate-two/"/>
    <id>https://tangguangen.com/leetcode-219-contain-duplicate-two/</id>
    <published>2018-12-06T13:19:18.000Z</published>
    <updated>2018-12-07T05:13:17.767Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个整数数组和一个整数 <em>k</em>，判断数组中是否存在两个不同的索引 <em>i</em> 和 <em>j</em>，使得 <strong>nums [i] = nums [j]</strong>，并且 <em>i</em> 和 <em>j</em> 的差的绝对值最大为 <em>k</em>。</p><p><strong>示例 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: nums = [1,2,3,1], k = 3</span><br><span class="line">&gt; 输出: true</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>示例 2:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: nums = [1,0,1,1], k = 1</span><br><span class="line">&gt; 输出: true</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>示例 3:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: nums = [1,2,3,1,2,3], k = 2</span><br><span class="line">&gt; 输出: false</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><a id="more"></a><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>想将数组进行排序，排序后的数组容易检查是否存在相同元素，若存在相同元素在进一步判断索引绝对值是否符合要求</p><h2 id="AC代码（Java）"><a href="#AC代码（Java）" class="headerlink" title="AC代码（Java）"></a>AC代码（Java）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">containsNearbyDuplicate</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span>[] temps = Arrays.copyOf(nums, nums.length);</span><br><span class="line">Arrays.sort(temps);</span><br><span class="line"><span class="keyword">int</span> temp = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">boolean</span> flag = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; temps.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line"><span class="keyword">if</span> (temps[i] == temps[i + <span class="number">1</span>]) &#123;</span><br><span class="line">temp = temps[i];</span><br><span class="line">flag = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (flag)</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nums.length - <span class="number">1</span>; j++) &#123;</span><br><span class="line"><span class="keyword">if</span> (nums[j] == temp) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">1</span>; index &lt;= k; index++) &#123;</span><br><span class="line"><span class="keyword">if</span> (j + index &lt; nums.length &amp;&amp; nums[j + index] == temp)</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;p&gt;给定一个整数数组和一个整数 &lt;em&gt;k&lt;/em&gt;，判断数组中是否存在两个不同的索引 &lt;em&gt;i&lt;/em&gt; 和 &lt;em&gt;j&lt;/em&gt;，使得 &lt;strong&gt;nums [i] = nums [j]&lt;/strong&gt;，并且 &lt;em&gt;i&lt;/em&gt; 和 &lt;em&gt;j&lt;/em&gt; 的差的绝对值最大为 &lt;em&gt;k&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例 1:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输入: nums = [1,2,3,1], k = 3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输出: true&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;示例 2:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输入: nums = [1,0,1,1], k = 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输出: true&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;示例 3:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输入: nums = [1,2,3,1,2,3], k = 2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输出: false&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
      <category term="java" scheme="https://tangguangen.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>深入理解Hadoop之HDFS架构</title>
    <link href="https://tangguangen.com/hdfs-architecture/"/>
    <id>https://tangguangen.com/hdfs-architecture/</id>
    <published>2018-12-04T13:43:48.000Z</published>
    <updated>2018-12-05T03:22:37.880Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop分布式文件系统（HDFS）是一种分布式文件系统。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的差异是值得我们注意的：</p><ol><li>HDFS具有<strong>高度容错</strong>能力，旨在部署在<strong>低成本</strong>硬件上。(高容错)</li><li>HDFS提供对数据的<strong>高吞吐量</strong>访问，适用于具有<strong>海量数据集</strong>的应用程序。（高吞吐量）</li><li>HDFS放宽了一些POSIX要求，以实现对文件系统数据的<strong>流式访问</strong>。（流式访问）</li></ol><p>HDFS最初是作为Apache Nutch网络搜索引擎项目的基础设施而构建的。HDFS是Apache Hadoop Core项目的一部分。项目URL是<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnLw==" title="http://hadoop.apache.org/">http://hadoop.apache.org/<i class="fa fa-external-link"></i></span></p><a id="more"></a><h2 id="目标和假设"><a href="#目标和假设" class="headerlink" title="目标和假设"></a>目标和假设</h2><p><strong>硬件故障检测</strong>：<strong>硬件故障是常态而非例外。</strong>Hadoop通常部署在低成本的硬件上，并且通常包含成百上千的服务器，每个服务器都存储文件系统数据的一部分。由于存在大量的组件，并且每个组件都具有不可忽略（non-trivial ）的故障概率，这意味着HDFS的某些组件始终都不起作用。因此，<strong>故障检测</strong>并<strong>快速恢复</strong>是HDFS的<strong>核心</strong>架构目标。</p><p><strong>流式访问</strong>：HDFS更适合<strong>批处理</strong>而不是交互式使用，更加注重数据访问的<strong>高吞吐量</strong>而不是数据访问的低延迟。在HDFS上运行的应用程序需要对其数据集进行<strong>流式访问</strong>。</p><p><strong>海量数据集</strong>：运行在HDFS上的应用程序具有大型数据集，HDFS中的一个典型文件的大小是g到tb，因此，HDFS被调优为支持大文件。它应该提供高聚合数据带宽，并可扩展到单个集群中的数百个节点。它应该在一个实例中支持数千万个文件。</p><p><strong>一致性模型</strong>：HDFS应用程序需要一个一次写入多次读取的文件访问模型。文件一旦创建、写入和关闭，除了追加和截断操作外，无需要更改。支持将内容追加到文件末尾，但无法在任意点更新。该假设简化了数据一致性问题并实现了高吞吐量数据访问。MapReduce应用程序或Web爬虫应用程序完全适合此模型。</p><p><strong>移动计算比移动数据便宜</strong>：应用程序请求的计算如果在其操作的数据附近执行，效率会高得多。当数据集的大小很大时尤其如此。这可以最大限度地减少网络拥塞并提高系统的整体吞吐量。<strong>因此更好的做法是将计算迁移到更靠近数据所在的位置</strong>，而不是将数据移动到运行应用程序的位置。HDFS为应用程序提供了一些接口，使它们自己更接近数据所在的位置。</p><p><strong>跨平台和可移植</strong>：Hadoop使用Java语言开发，使得Hadoop具有良好的跨平台性。</p><h2 id="NameNode和DataNodes"><a href="#NameNode和DataNodes" class="headerlink" title="NameNode和DataNodes"></a>NameNode和DataNodes</h2><p>HDFS具有<strong>主/从</strong>（ <strong>master/slave</strong>）架构。HDFS集群由一个<strong>NameNode</strong>和许多<strong>DataNode</strong>组成，NameNode是一个主服务器（master），管理文件系统名称空间并管理客端对数据的访问（<strong>NameNode在Hadoop集群中充当<u>管家</u>的角色</strong>）。此外集群中每个节点通常是一个DataNode，DataNode管理它们的节点上存储的数据。</p><p>HDFS公开文件系统名称空间，并允许用户数据存储在文件中。在内部，文件被分成一个或多个块（block），这些块存储在DataNode中。NameNode执行文件系统名称空间的相关操作，如打开、关闭和重命名文件和目录。它还确定了块到DataNode的映射（块存储到哪个DataNode中）。数据节点负责服务来自文件系统客户端的读写请求。数据节点还根据NameNode的指令执行块创建、删除和复制。</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-f4c1ebcbc6191438.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="HDFS架构"></p><p>集群中单一NameNode的结构大大简化了系统的架构。NameNode是所有HDFS元数据的仲裁者和管理者，这样，用户数据永远不会流过NameNode。</p><h2 id="文件系统名称空间-namespace"><a href="#文件系统名称空间-namespace" class="headerlink" title="文件系统名称空间(namespace)"></a>文件系统名称空间(namespace)</h2><p>HDFS支持传统的<strong>层次型</strong>文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名称空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。</p><p>NameNode负责维护文件系统的名称空间，任何对文件系统名称空间或属性的修改都将被NameNode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由NameNode保存的。</p><p>如果想深入了解HDFS文件系统名称空间可以查看这篇博文：<span class="exturl" data-url="aHR0cDovL2xlb3RzZTkwLmNvbS8yMDE1LzEwLzAxL0hERlMlRTYlOTYlODclRTQlQkIlQjYlRTclQjMlQkIlRTclQkIlOUYlRTUlOTElQkQlRTUlOTAlOEQlRTclQTklQkElRTklOTclQjQv" title="http://leotse90.com/2015/10/01/HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/">http://leotse90.com/…<i class="fa fa-external-link"></i></span></p><h2 id="数据复制"><a href="#数据复制" class="headerlink" title="数据复制"></a>数据复制</h2><p>HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。</p><p><strong>NameNode</strong>全权管理数据块的复制，它周期性地从集群中的每个<strong>DataNode</strong>接收<strong>心跳</strong>信号(Heartbeat )和<strong>块状态报告</strong>(Blockreport)。</p><ul><li>接收到心跳信号意味着该DataNode节点工作正常。</li><li>块状态报告包含了一个该Datanode上所有数据块的列表。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/15167924-50c8113a159e8a74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="数据块的复制"></p><h3 id="副本存放-最最开始的一步"><a href="#副本存放-最最开始的一步" class="headerlink" title="副本存放: 最最开始的一步"></a>副本存放: 最最开始的一步</h3><p>副本的存放是HDFS<strong>可靠性</strong>和<strong>性能</strong>的关键。<strong>优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。</strong>这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为<strong>机架感知</strong>(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。目前实现的副本存放策略只是在这个方向上的第一步。实现这个策略的短期目标是验证它在生产环境下的有效性，观察它的行为，为实现更先进的策略打下测试和研究的基础。</p><p>大型HDFS实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通讯需要经过交换机。在大多数情况下，<strong>同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。</strong></p><p>通过一个<span class="exturl" data-url="aHR0cHM6Ly9oYWRvb3AuYXBhY2hlLm9yZy9kb2NzL3IxLjAuNC9jbi9jbHVzdGVyX3NldHVwLmh0bWwjSGFkb29wJUU3JTlBJTg0JUU2JTlDJUJBJUU2JTlFJUI2JUU2JTg0JTlGJUU3JTlGJUE1" title="https://hadoop.apache.org/docs/r1.0.4/cn/cluster_setup.html#Hadoop%E7%9A%84%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5">机架感知<i class="fa fa-external-link"></i></span>的过程，NameNode可以确定每个DataNode所属的机架id。一个简单但没有优化的策略就是将副本存放在不同的机架上。这样可以有效防止当整个机架失效时数据的丢失，并且允许读数据的时候充分利用多个机架的带宽。这种策略设置可以将副本均匀分布在集群中，有利于当组件失效情况下的负载均衡。但是，因为这种策略的一个写操作需要传输数据块到多个机架，这增加了写的代价。</p><p>在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。这种策略减少了机架间的数据传输，这就提高了写操作的效率。机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。于此同时，因为数据块只放在两个（不是三个）不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。在这种策略下，副本并不是均匀分布在不同的机架上。三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能。</p><h3 id="副本选择"><a href="#副本选择" class="headerlink" title="副本选择"></a>副本选择</h3><p>为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。(<strong>就近原则</strong>)</p><h3 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h3><p>NameNode启动后会进入一个称为安全模式的特殊状态。处于安全模式的NameNode是不会进行数据块的复制的。NameNode从所有的 DataNode接收心跳信号和块状态报告。块状态报告包括了某个DataNode所有的数据块列表。每个数据块都有一个指定的最小副本数。当NameNode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是<strong>副本安全</strong>(safely replicated)的；在一定百分比（这个参数可配置）的数据块被NameNode检测确认是安全之后（加上一个额外的30秒等待时间），NameNode将退出安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他DataNode上。</p><h2 id="文件系统元数据的持久化"><a href="#文件系统元数据的持久化" class="headerlink" title="文件系统元数据的持久化"></a>文件系统元数据的持久化</h2><p>NameNode上保存着HDFS的DataNode空间。对于任何对文件系统元数据产生修改的操作，NameNode都会使用一种称为EditLog的事务日志记录下来。例如，在HDFS中创建一个文件，NameNode就会在Editlog中插入一条记录来表示；同样地，修改文件的副本系数也将往Editlog插入一条记录。NameNode在本地操作系统的文件系统中存储这个Editlog。整个文件系统的DataNode空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为FsImage的文件中，这个文件也是放在NameNode所在的本地文件系统上。</p><p>NameNode在内存中保存着整个文件系统的DataNode空间和文件数据块映射(Blockmap)的映像。这个关键的元数据结构设计得很紧凑，因而一个有4G内存的NameNode足够支撑大量的文件和目录。当NameNode启动时，它从硬盘中读取Editlog和FsImage，将所有Editlog中的事务作用在内存中的FsImage上，并将这个新版本的FsImage从内存中保存到本地磁盘上，然后删除旧的Editlog，因为这个旧的Editlog的事务都已经作用在FsImage上了。这个过程称为一个检查点(checkpoint)。在当前实现中，检查点只发生在NameNode启动时，在不久的将来将实现支持周期性的检查点。</p><p>Datanode将HDFS数据以文件的形式存储在本地的文件系统中，它并不知道有关HDFS文件的信息。它把每个HDFS数据块存储在本地文件系统的一个单独的文件中。Datanode并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目录中支持大量的文件。当一个Datanode启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有HDFS数据块的列表，然后作为报告发送到NameNode，这个报告就是块状态报告。</p><h2 id="通讯协议"><a href="#通讯协议" class="headerlink" title="通讯协议"></a>通讯协议</h2><p>所有的HDFS通讯协议都是建立在TCP/IP协议之上。客户端通过一个可配置的TCP端口连接到NameNode，通过ClientProtocol协议与NameNode交互。而Datanode使用DatanodeProtocol协议与NameNode交互。一个远程过程调用(RPC)模型被抽象出来封装ClientProtocol和Datanodeprotocol协议。在设计上，NameNode不会主动发起RPC，而是响应来自客户端或 Datanode 的RPC请求。</p><h2 id="健壮性"><a href="#健壮性" class="headerlink" title="健壮性"></a>健壮性</h2><p>HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：NameNode出错, Datanode出错和网络割裂(network partitions)。</p><h3 id="磁盘数据错误，心跳检测和重新复制"><a href="#磁盘数据错误，心跳检测和重新复制" class="headerlink" title="磁盘数据错误，心跳检测和重新复制"></a>磁盘数据错误，心跳检测和重新复制</h3><p>每个Datanode节点周期性地向NameNode发送心跳信号。网络割裂可能导致一部分Datanode跟NameNode失去联系。NameNode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号Datanode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机Datanode上的数据将不再有效。Datanode的宕机可能会引起一些数据块的副本系数低于指定值，NameNode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。在下列情况下，可能需要重新复制：某个Datanode节点失效，某个副本遭到损坏，Datanode上的硬盘错误，或者文件的副本系数增大。</p><h3 id="集群均衡"><a href="#集群均衡" class="headerlink" title="集群均衡"></a>集群均衡</h3><p>HDFS的架构支持数据均衡策略。如果某个Datanode节点上的空闲空间低于特定的临界点，按照均衡策略系统就会自动地将数据从这个Datanode移动到其他空闲的Datanode。当对某个文件的请求突然增加，那么也可能启动一个计划创建该文件新的副本，并且同时重新平衡集群中的其他数据。这些均衡策略目前还没有实现。</p><h3 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h3><p>从某个Datanode获取的数据块有可能是损坏的，损坏可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFSDataNode空间下。当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode获取该数据块的副本。</p><h3 id="元数据磁盘错误"><a href="#元数据磁盘错误" class="headerlink" title="元数据磁盘错误"></a>元数据磁盘错误</h3><p>FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS实例都将失效。因而，NameNode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低NameNode每秒处理的DataNode空间事务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当NameNode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。</p><p>增加故障恢复能力的另一个选择是使用多个NameNode <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSERGU0hpZ2hBdmFpbGFiaWxpdHlXaXRoTkZTLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">在NFS上<i class="fa fa-external-link"></i></span>使用<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSERGU0hpZ2hBdmFpbGFiaWxpdHlXaXRoTkZTLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">共享存储<i class="fa fa-external-link"></i></span>或使用<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSERGU0hpZ2hBdmFpbGFiaWxpdHlXaXRoUUpNLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">分布式编辑日志<i class="fa fa-external-link"></i></span>（称为Journal）来启用高可用性。后者是推荐的方法。</p><h3 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h3><p><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1NuYXBzaG90cy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">快照<i class="fa fa-external-link"></i></span>支持在特定时刻存储数据副本。快照功能的一种用途可以是将损坏的HDFS实例回滚到先前已知的良好时间点。</p><h2 id="数据组织"><a href="#数据组织" class="headerlink" title="数据组织"></a>数据组织</h2><h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h3><p>HDFS被设计成支持大文件，适用HDFS的是那些需要处理大规模的数据集的应用。这些应用都是只写入数据一次，但却读取一次或多次，并且读取速度应能满足流式读取的需要。HDFS支持文件的“一次写入多次读取”语义。一个典型的数据块大小是128MB。因而，HDFS中的文件总是按照128M被切分成不同的块，每个块尽可能地存储于不同的Datanode中。</p><h3 id="流水线复制"><a href="#流水线复制" class="headerlink" title="流水线复制"></a>流水线复制</h3><p>当客户端向HDFS文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副本系数设置为3，当本地临时文件累积到一个数据块的大小时，客户端会从NameNode获取一个Datanode列表用于存放副本。然后客户端开始向第一个Datanode传输数据，第一个Datanode一小部分一小部分(4 KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个Datanode节点。第二个Datanode也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个Datanode。最后，第三个Datanode接收数据并存储在本地。因此，Datanode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个Datanode复制到下一个。</p><h2 id="可访问性"><a href="#可访问性" class="headerlink" title="可访问性"></a>可访问性</h2><p>可以通过多种不同方式从应用程序访问HDFS。本地，HDFS 为应用程序提供了<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3MvY3VycmVudC9hcGkv" title="http://hadoop.apache.org/docs/current/api/">FileSystem Java API<i class="fa fa-external-link"></i></span>。一<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvTGliSGRmcy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/LibHdfs.html">本Java API的C语言包装<i class="fa fa-external-link"></i></span>和<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvV2ViSERGUy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html">REST API<i class="fa fa-external-link"></i></span>也是可用的。此外，还有HTTP浏览器，也可用于浏览HDFS实例的文件。通过使用<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc05mc0dhdGV3YXkuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS网关<i class="fa fa-external-link"></i></span>，HDFS可以作为客户端本地文件系统的一部分进行安装。</p><h3 id="FS-Shell"><a href="#FS-Shell" class="headerlink" title="FS Shell"></a>FS Shell</h3><p>HDFS以文件和目录的形式组织用户数据。它提供了一个命令行的接口(FS Shell)让用户与HDFS中的数据进行交互。命令的语法和用户熟悉的其他shell(例如 bash, csh)工具类似。下面是一些动作/命令的示例：</p><table><thead><tr><th>创建一个名为<code>/foodir</code>的目录</th><th><code>bin /hadoop dfs -mkdir /foodir</code></th></tr></thead><tbody><tr><td>删除名为<code>/foodir</code>的目录</td><td><code>bin /hadoop fs -rm -R /foodir</code></td></tr><tr><td>查看名为<code>/foodir/myfile.txt</code>的文件的内容</td><td><code>bin /hadoop dfs -cat /foodir/myfile.txt</code></td></tr></tbody></table><p>FS shell适用于需要脚本语言与存储数据交互的应用程序。</p><h3 id="DFSAdmin"><a href="#DFSAdmin" class="headerlink" title="DFSAdmin"></a>DFSAdmin</h3><p>典型的HDFS安装配置Web服务器以通过可配置的TCP端口公开HDFS命名空间。这允许用户使用Web浏览器导航HDFS命名空间并查看其文件的内容。</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-3842129a5a9d163b.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Web界面"></p><h2 id="存储空间回收"><a href="#存储空间回收" class="headerlink" title="存储空间回收"></a>存储空间回收</h2><h3 id="文件的删除和恢复"><a href="#文件的删除和恢复" class="headerlink" title="文件的删除和恢复"></a>文件的删除和恢复</h3><p>如果启用了回收站配置，当用户或应用程序删除某个文件时，这个文件并没有立刻从HDFS中删除。实际上，HDFS会将这个文件重命名转移到/trash目录(/user/<username>/.Trash)。只要文件还在/trash目录中，该文件就可以被迅速地恢复。文件在/trash中保存的时间是可配置的，当超过这个时间时，NameNode就会将该文件从DataNode空间中删除。删除文件会使得该文件相关的数据块被释放。注意，从用户删除文件到HDFS空闲空间的增加之间会有一定时间的延迟。</username></p><p>以下是一个示例，它将显示FS Shell如何从HDFS中删除文件。我们在目录delete下创建了2个文件（test1和test2）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -mkdir -p delete/test1</span><br><span class="line">$ hadoop fs -mkdir -p delete/test2</span><br><span class="line">$ hadoop fs -ls delete/</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2015-05-08 12:39 delete/test1</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2015-05-08 12:40 delete/test2</span><br></pre></td></tr></table></figure><p>我们将删除文件test1。下面的注释显示该文件已移至/trash目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -rm -r delete/test1</span><br><span class="line">Moved: hdfs://localhost:8020/user/hadoop/delete/test1 to trash at: hdfs://localhost:8020/user/hadoop/.Trash/Current</span><br></pre></td></tr></table></figure><p>现在我们将使用skipTrash选项删除该文件，该选项不会将文件发送到Trash。它将从HDFS中完全删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -rm -r -skipTrash delete/test2</span><br><span class="line">Deleted delete/test2</span><br></pre></td></tr></table></figure><p>我们现在可以看到Trash目录只包含文件test1。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls .Trash/Current/user/hadoop/delete/</span><br><span class="line">Found 1 items\</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2015-05-08 12:39 .Trash/Current/user/hadoop/delete/test1</span><br></pre></td></tr></table></figure><p>因此文件test1进入垃圾箱并永久删除文件test2。</p><h3 id="减少副本系数"><a href="#减少副本系数" class="headerlink" title="减少副本系数"></a>减少副本系数</h3><p>当一个文件的副本系数被减小后，NameNode会选择过剩的副本删除。下次心跳检测时会将该信息传递给Datanode。Datanode遂即移除相应的数据块，集群中的空闲空间加大。同样，在调用setReplication API结束和集群中空闲空间增加间会有一定的延迟。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>Hadoop <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3MvY3VycmVudC9hcGkv" title="http://hadoop.apache.org/docs/current/api/">JavaDoc API<i class="fa fa-external-link"></i></span></p><p>HDFS源代码：<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL3ZlcnNpb25fY29udHJvbC5odG1s" title="http://hadoop.apache.org/version_control.html">http://hadoop.apache.org/…<i class="fa fa-external-link"></i></span></p><p>Hadoop Doc: <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2luZGV4Lmh0bWw=" title="http://hadoop.apache.org/docs/stable/index.html">http://hadoop.apache.org/docs/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop分布式文件系统（HDFS）是一种分布式文件系统。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的差异是值得我们注意的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HDFS具有&lt;strong&gt;高度容错&lt;/strong&gt;能力，旨在部署在&lt;strong&gt;低成本&lt;/strong&gt;硬件上。(高容错)&lt;/li&gt;
&lt;li&gt;HDFS提供对数据的&lt;strong&gt;高吞吐量&lt;/strong&gt;访问，适用于具有&lt;strong&gt;海量数据集&lt;/strong&gt;的应用程序。（高吞吐量）&lt;/li&gt;
&lt;li&gt;HDFS放宽了一些POSIX要求，以实现对文件系统数据的&lt;strong&gt;流式访问&lt;/strong&gt;。（流式访问）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HDFS最初是作为Apache Nutch网络搜索引擎项目的基础设施而构建的。HDFS是Apache Hadoop Core项目的一部分。项目URL是&lt;a href=&quot;http://hadoop.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://hadoop.apache.org/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/HDFS/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="https://tangguangen.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 190.颠倒二进制位</title>
    <link href="https://tangguangen.com/leetcode-190-revers-bits/"/>
    <id>https://tangguangen.com/leetcode-190-revers-bits/</id>
    <published>2018-12-04T12:57:31.000Z</published>
    <updated>2018-12-07T05:12:31.522Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>颠倒给定的 32 位无符号整数的二进制位。</p><p><strong>示例:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: 43261596</span><br><span class="line">&gt; 输出: 964176192</span><br><span class="line">&gt; 解释: 43261596 的二进制表示形式为 00000010100101000001111010011100 ，</span><br><span class="line">&gt;      返回 964176192，其二进制表示形式为 00111001011110000010100101000000 。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>进阶</strong>:<br>如果多次调用这个函数，你将如何优化你的算法？</p></blockquote><a id="more"></a><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>通过位运算<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU0JUJEJThEJUU2JTkzJThEJUU0JUJEJTlD" title="https://zh.wikipedia.org/wiki/%E4%BD%8D%E6%93%8D%E4%BD%9C">wiki<i class="fa fa-external-link"></i></span>，依次截取n的二进制位的低位，放入result的高位。这一题是简单题，需掌握位运算</p><h2 id="AC代码（Java）"><a href="#AC代码（Java）" class="headerlink" title="AC代码（Java）"></a>AC代码（Java）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="comment">// you need treat n as an unsigned value</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> ans=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">32</span>;i++)</span><br><span class="line">            ans|=((n&gt;&gt;&gt;i)&amp;<span class="number">1</span>)&lt;&lt;(<span class="number">31</span>-i);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;p&gt;颠倒给定的 32 位无符号整数的二进制位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输入: 43261596&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输出: 964176192&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 解释: 43261596 的二进制表示形式为 00000010100101000001111010011100 ，&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;      返回 964176192，其二进制表示形式为 00111001011110000010100101000000 。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;进阶&lt;/strong&gt;:&lt;br&gt;如果多次调用这个函数，你将如何优化你的算法？&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
      <category term="java" scheme="https://tangguangen.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 557. 反转字符串中的单词 III</title>
    <link href="https://tangguangen.com/leetcode-557-reverse-words/"/>
    <id>https://tangguangen.com/leetcode-557-reverse-words/</id>
    <published>2018-12-04T05:51:34.000Z</published>
    <updated>2018-12-07T05:12:45.062Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个字符串，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。</p><p><strong>示例 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: &quot;Let&apos;s take LeetCode contest&quot;</span><br><span class="line">&gt; 输出: &quot;s&apos;teL ekat edoCteeL tsetnoc&quot; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>注意：</strong>在字符串中，每个单词由单个空格分隔，并且字符串中不会有任何额外的空格。</p></blockquote><a id="more"></a><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>由于字符串中不会有任何额外的空格，根据空格来找单词，将每个单词依次进行字符串的反转即可，字符串的反转可以看我这篇博客：<a href="https://tangguangen.com/leetcode-344-reverse-string/">LeetCode反转字符串</a></p><h2 id="AC代码（Java）"><a href="#AC代码（Java）" class="headerlink" title="AC代码（Java）"></a>AC代码（Java）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">reverseWords</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span>[] cl = s.toCharArray();</span><br><span class="line">        <span class="keyword">int</span> start = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> nextSpace = s.indexOf(<span class="string">' '</span>,start);  <span class="comment">// 返回字符串s从索引start开始的第一个空格的索引，若没有，则返回-1</span></span><br><span class="line">        <span class="keyword">while</span>(nextSpace != -<span class="number">1</span>) &#123;</span><br><span class="line">            reverse(cl,start,nextSpace - <span class="number">1</span>);</span><br><span class="line">            start = nextSpace + <span class="number">1</span>;</span><br><span class="line">            nextSpace = s.indexOf(<span class="string">' '</span>,start);</span><br><span class="line">        &#125;</span><br><span class="line">        reverse(cl,start,cl.length - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String(cl);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reverse</span><span class="params">(<span class="keyword">char</span>[] cl,<span class="keyword">int</span> start,<span class="keyword">int</span> end)</span></span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(start &lt; end)&#123;</span><br><span class="line">            <span class="keyword">char</span> temp = cl[start];</span><br><span class="line">            cl[start] = cl[end];</span><br><span class="line">            cl[end] = temp;</span><br><span class="line">            start ++;</span><br><span class="line">            end --;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;p&gt;给定一个字符串，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例 1:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输入: &amp;quot;Let&amp;apos;s take LeetCode contest&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; 输出: &amp;quot;s&amp;apos;teL ekat edoCteeL tsetnoc&amp;quot; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;在字符串中，每个单词由单个空格分隔，并且字符串中不会有任何额外的空格。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle机器学习入门教程一</title>
    <link href="https://tangguangen.com/kaggle-learn-machine-learning-introduction/"/>
    <id>https://tangguangen.com/kaggle-learn-machine-learning-introduction/</id>
    <published>2018-12-03T10:50:30.000Z</published>
    <updated>2018-12-07T05:22:35.617Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型是如何工作的"><a href="#模型是如何工作的" class="headerlink" title="模型是如何工作的"></a>模型是如何工作的</h2><p>原文链接：<span class="exturl" data-url="aHR0cHM6Ly93d3cua2FnZ2xlLmNvbS9kYW5zYmVja2VyL2hvdy1tb2RlbHMtd29yaw==" title="https://www.kaggle.com/dansbecker/how-models-work">https://www.kaggle.com/…<i class="fa fa-external-link"></i></span></p><p>这门课程将从机器学习模型<strong>如何工作</strong>以及<strong>如何使用</strong>它们开始，如果您以前做过统计建模或机器学习，这可能会觉得很基础。别担心，我们很快就会建立强大的模型。</p><p>本课程将让您为以下场景构建模型:</p><p>（<strong>房价预测</strong>）你表弟在房地产投机上赚了数百万美元。由于你对数据科学的兴趣，他愿意和你成为商业伙伴。他提供钱，你提供模型来预测不同房子的价值。</p><p>你问你表弟他过去是如何预测房地产价值的。他说这只是直觉。但更多的问题表明，他已经从过去看到的房子中识别出了价格模式，并利用这些模式对他正在考虑的新房子做出了预测。</p><p>机器学习也是如此。我们将从一个叫做<strong>决策树</strong>的模型开始。用更漂亮的模型可以给出更准确的预测。但是决策树很容易理解，它们是数据科学中一些最佳模型的基本构建块。</p><p><strong>为了简单起见，我们将从最简单的决策树开始。</strong></p><a id="more"></a><p><img src="https://upload-images.jianshu.io/upload_images/15167924-f3ca1ac3b54ef275.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="简单决策树"></p><p>它<strong>只把房子分为两类</strong>。任何被考虑的房屋的预测价格是同一类别房屋的历史平均价格。</p><p>我们用数据来决定如何把房子分成两组，然后再确定每组的预测价格。从数据中获取模式的这一步称为模型<strong>拟合</strong>或<strong>训练</strong>。用于<strong>拟合模型</strong>的数据称为<strong>训练数据</strong>。</p><p>模型如何拟合(例如如何分割数据)的细节非常复杂，我们将在以后进行详细讲解。模型拟合后，您可以将其应用于新的数据，以<strong>预测</strong>其他房屋的价格。</p><h3 id="改进决策树"><a href="#改进决策树" class="headerlink" title="改进决策树"></a>改进决策树</h3><p>以下两种决策树中，哪一种更有可能通过拟合房地产训练数据而得到结果?</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-500e8335b0a2ba16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="决策树对比"></p><p>左边的决策树(Decision Tree 1)可能更有意义，因为它捕捉到了这样一个事实: 卧室较多的房子往往比卧室较少的房子售价更高。这种模式最大的缺点是它没有捕捉到影响房价的其他的大部分因素，如浴室数量、面积、位置等。</p><p>您可以使用具有更多“分支”的树捕获更多的因子。这些树被称为“深”树。一个决策树，也考虑每个房子的面积大小，可能是这样的:</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-1be930db63267c0c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="改进的决策树"></p><p>可以通过追踪决策树来预测房子的价格，每次决策都选择与房子特征相对应的路径。这所房子的预测价格在树的底部。在底部的点叫做<strong>叶子节点</strong>。</p><p>叶节点的分割和值将由数据决定，所以现在是您检查将要处理的数据的时候了。</p><h2 id="探索数据"><a href="#探索数据" class="headerlink" title="探索数据"></a>探索数据</h2><h3 id="使用pandas探索数据"><a href="#使用pandas探索数据" class="headerlink" title="使用pandas探索数据"></a>使用pandas探索数据</h3><p>任何机器学习项目的第一步都是熟悉数据。pandas库是科学家用来探索和操做数据的主要工具。大多数人在他们的代码中将pandas缩写为pd。我们用下面的命令导入pandas库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><p>pandas库最重要的部分是DataFrame。DataFrame保存您可能认为是表的数据类型。这类似于Excel中的工作表，或SQL数据库中的表。</p><p>pandas提供了强大的方法，可以处理您想要处理的大多数此类数据。</p><p>举个例子，我们来看看澳大利亚墨尔本的房价数据。在实践练习中，您将把相同的过程应用到一个新的数据集，该数据集包含衣阿华州的房价。</p><p>示例(墨尔本)数据位于文件路径<strong>../input/melbourne-housing-snapshot/melb_data.csv</strong></p><p>我们使用以下命令加载和浏览数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save filepath to variable for easier access</span></span><br><span class="line">melbourne_file_path = <span class="string">'../input/melbourne-housing-snapshot/melb_data.csv'</span></span><br><span class="line"><span class="comment"># read the data and store data in DataFrame titled melbourne_data</span></span><br><span class="line">melbourne_data = pd.read_csv(melbourne_file_path) </span><br><span class="line"><span class="comment"># print a summary of the data in Melbourne data</span></span><br><span class="line">melbourne_data.describe()</span><br></pre></td></tr></table></figure><table><thead><tr><th>Rooms</th><th>Price</th><th>Distance</th><th>Postcode</th><th>Bedroom2</th><th>Bathroom</th><th>Car</th><th>Landsize</th><th>BuildingArea</th><th>YearBuilt</th><th>Lattitude</th><th>Longtitude</th><th>Propertycount</th><th></th></tr></thead><tbody><tr><td>count</td><td>13580.000000</td><td>1.358000e+04</td><td>13580.000000</td><td>13580.000000</td><td>13580.000000</td><td>13580.000000</td><td>13518.000000</td><td>13580.000000</td><td>7130.000000</td><td>8205.000000</td><td>13580.000000</td><td>13580.000000</td><td>13580.000000</td></tr><tr><td>mean</td><td>2.937997</td><td>1.075684e+06</td><td>10.137776</td><td>3105.301915</td><td>2.914728</td><td>1.534242</td><td>1.610075</td><td>558.416127</td><td>151.967650</td><td>1964.684217</td><td>-37.809203</td><td>144.995216</td><td>7454.417378</td></tr><tr><td>std</td><td>0.955748</td><td>6.393107e+05</td><td>5.868725</td><td>90.676964</td><td>0.965921</td><td>0.691712</td><td>0.962634</td><td>3990.669241</td><td>541.014538</td><td>37.273762</td><td>0.079260</td><td>0.103916</td><td>4378.581772</td></tr><tr><td>min</td><td>1.000000</td><td>8.500000e+04</td><td>0.000000</td><td>3000.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>1196.000000</td><td>-38.182550</td><td>144.431810</td><td>249.000000</td></tr><tr><td>25%</td><td>2.000000</td><td>6.500000e+05</td><td>6.100000</td><td>3044.000000</td><td>2.000000</td><td>1.000000</td><td>1.000000</td><td>177.000000</td><td>93.000000</td><td>1940.000000</td><td>-37.856822</td><td>144.929600</td><td>4380.000000</td></tr><tr><td>50%</td><td>3.000000</td><td>9.030000e+05</td><td>9.200000</td><td>3084.000000</td><td>3.000000</td><td>1.000000</td><td>2.000000</td><td>440.000000</td><td>126.000000</td><td>1970.000000</td><td>-37.802355</td><td>145.000100</td><td>6555.000000</td></tr><tr><td>75%</td><td>3.000000</td><td>1.330000e+06</td><td>13.000000</td><td>3148.000000</td><td>3.000000</td><td>2.000000</td><td>2.000000</td><td>651.000000</td><td>174.000000</td><td>1999.000000</td><td>-37.756400</td><td>145.058305</td><td>10331.000000</td></tr><tr><td>max</td><td>10.000000</td><td>9.000000e+06</td><td>48.100000</td><td>3977.000000</td><td>20.000000</td><td>8.000000</td><td>10.000000</td><td>433014.000000</td><td>44515.000000</td><td>2018.000000</td><td>-37.408530</td><td>145.526350</td><td>21650.000000</td></tr></tbody></table><h3 id="解释Data-Description"><a href="#解释Data-Description" class="headerlink" title="解释Data Description"></a>解释Data Description</h3><p>结果为原始数据集中的每一列显示8个数字。第一个数字是count，它显示有多少行没有缺失值。</p><p>缺失值的原因有很多。例如，在测量1居室的房子时，不会收集第2居室的大小。</p><p>第二个值是均值<strong>mean</strong>，也就是平均值。在这种情况下，<strong>std</strong>是标准偏差，它度量数值的分布情况。</p><p>要解释最小值<strong>min</strong>、<strong>25%</strong>、<strong>50%</strong>、<strong>75%</strong>和最大值<strong>max</strong> ，请想像对每个列从最低值到最高值进行排序。第一个(最小的)值是最小值。如果您遍历列表的四分之一，您会发现一个值大于25%，小于75%。这就是25%的值。第50百分位和第75百分位的定义类似，最大值是最大的数字。</p><h2 id="练习：探索数据"><a href="#练习：探索数据" class="headerlink" title="练习：探索数据"></a>练习：探索数据</h2><p>kaggle链接：<span class="exturl" data-url="aHR0cHM6Ly93d3cua2FnZ2xlLmNvbS9rZXJuZWxzL2ZvcmsvMTI1ODk1NA==" title="https://www.kaggle.com/kernels/fork/1258954">https://www.kaggle.com/…<i class="fa fa-external-link"></i></span></p><p>这个练习将测试您读取数据文件和理解有关数据的统计信息的能力。</p><p>你的数据中最新的房子并不新鲜。对此有几个可能的解释:</p><ol><li>他们还没有在收集这些数据的地方建造新房子。</li><li>这些数据是很久以前收集的。数据发布后建造的房屋不会出现。</li></ol><p>如果原因是上面的解释#1，那么这会影响您对使用这些数据构建的模型的信任吗?如果这是第二个原因呢?</p><p>你如何深入研究这些数据，看看哪种解释更合理?</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;模型是如何工作的&quot;&gt;&lt;a href=&quot;#模型是如何工作的&quot; class=&quot;headerlink&quot; title=&quot;模型是如何工作的&quot;&gt;&lt;/a&gt;模型是如何工作的&lt;/h2&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://www.kaggle.com/dansbecker/how-models-work&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.kaggle.com/…&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这门课程将从机器学习模型&lt;strong&gt;如何工作&lt;/strong&gt;以及&lt;strong&gt;如何使用&lt;/strong&gt;它们开始，如果您以前做过统计建模或机器学习，这可能会觉得很基础。别担心，我们很快就会建立强大的模型。&lt;/p&gt;
&lt;p&gt;本课程将让您为以下场景构建模型:&lt;/p&gt;
&lt;p&gt;（&lt;strong&gt;房价预测&lt;/strong&gt;）你表弟在房地产投机上赚了数百万美元。由于你对数据科学的兴趣，他愿意和你成为商业伙伴。他提供钱，你提供模型来预测不同房子的价值。&lt;/p&gt;
&lt;p&gt;你问你表弟他过去是如何预测房地产价值的。他说这只是直觉。但更多的问题表明，他已经从过去看到的房子中识别出了价格模式，并利用这些模式对他正在考虑的新房子做出了预测。&lt;/p&gt;
&lt;p&gt;机器学习也是如此。我们将从一个叫做&lt;strong&gt;决策树&lt;/strong&gt;的模型开始。用更漂亮的模型可以给出更准确的预测。但是决策树很容易理解，它们是数据科学中一些最佳模型的基本构建块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为了简单起见，我们将从最简单的决策树开始。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://tangguangen.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="kaggle" scheme="https://tangguangen.com/tags/kaggle/"/>
    
      <category term="机器学习" scheme="https://tangguangen.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：多节点集群</title>
    <link href="https://tangguangen.com/hadoop-multi-node-cluster/"/>
    <id>https://tangguangen.com/hadoop-multi-node-cluster/</id>
    <published>2018-12-01T12:45:48.000Z</published>
    <updated>2018-12-07T05:21:23.266Z</updated>
    
    <content type="html"><![CDATA[<p>本章介绍了Hadoop多节点集群在分布式环境中的设置。</p><p>由于无法演示整个集群，我们使用三个系统(一个主系统和两个从系统)解释Hadoop集群环境;以下是他们的IP地址。</p><ul><li>Hadoop Master: 192.168.1.15 (hadoop-master)</li><li>Hadoop Slave: 192.168.1.16 (hadoop-slave-1)</li><li>Hadoop Slave: 192.168.1.17 (hadoop-slave-2)</li></ul><p>按照下面给出的步骤设置Hadoop多节点集群。</p><a id="more"></a><h2 id="安装Java"><a href="#安装Java" class="headerlink" title="安装Java"></a>安装Java</h2><p>参考：<a href="https://tangguangen.com/hadoop-enviornment-setup">Hadoop安装与环境设置</a></p><h2 id="映射节点"><a href="#映射节点" class="headerlink" title="映射节点"></a>映射节点</h2><p>您必须在<strong>/etc/</strong>文件夹中编辑所有节点上的<strong>hosts</strong> 文件，指定每个系统的IP地址及其主机名。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vi /etc/hosts</span></span><br><span class="line">enter the following lines in the /etc/hosts file.</span><br><span class="line">192.168.1.109 hadoop-master </span><br><span class="line">192.168.1.145 hadoop-slave-1 </span><br><span class="line">192.168.56.1 hadoop-slave-2</span><br></pre></td></tr></table></figure><h2 id="配置基于密钥的登录"><a href="#配置基于密钥的登录" class="headerlink" title="配置基于密钥的登录"></a>配置基于密钥的登录</h2><p>在每个节点上设置ssh，这样它们就可以彼此通信，而无需提示输入密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># su hadoop </span><br><span class="line">$ ssh-keygen -t rsa </span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub tutorialspoint@hadoop-master </span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop_tp1@hadoop-slave-1 </span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop_tp2@hadoop-slave-2 </span><br><span class="line">$ chmod 0600 ~/.ssh/authorized_keys </span><br><span class="line">$ exit</span><br></pre></td></tr></table></figure><h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h2><p>在主服务器中，使用以下命令下载和安装Hadoop。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir /opt/hadoop </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> /opt/hadoop/ </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> wget http://apache.mesi.com.ar/hadoop/common/hadoop-1.2.1/hadoop-1.2.0.tar.gz </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tar -xzf hadoop-1.2.0.tar.gz </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mv hadoop-1.2.0 hadoop</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> chown -R hadoop /opt/hadoop </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> /opt/hadoop/hadoop/</span></span><br></pre></td></tr></table></figure><h2 id="配置Hadoop"><a href="#配置Hadoop" class="headerlink" title="配置Hadoop"></a>配置Hadoop</h2><p>您必须配置Hadoop服务器，方法如下所示。</p><h3 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h3><p>打开<strong>core-site.xml</strong>文件并编辑它，如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;fs.default.name&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;hdfs://hadoop-master:9000/&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;dfs.permissions&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;false&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h3><p>打开<strong>hdfs-site.xml</strong> 文件并编辑它，如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;dfs.data.dir&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;/opt/hadoop/hadoop/dfs/name/data&lt;/value&gt; </span><br><span class="line">      &lt;final&gt;true&lt;/final&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line"></span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;dfs.name.dir&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;/opt/hadoop/hadoop/dfs/name&lt;/value&gt; </span><br><span class="line">      &lt;final&gt;true&lt;/final&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line"></span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;dfs.replication&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;1&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h3><p>打开<strong>mapred-site.xml</strong>文件并编辑它，如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;mapred.job.tracker&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;hadoop-master:9001&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h3><p>打开<strong>hadoop-env.sh</strong>文件并编辑JAVA_HOME、hadoop op_conf_dir和hadoop op_opts，如下所示。</p><p>注意:按照系统配置设置JAVA_HOME。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/jdk1.7.0_17 export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true export HADOOP_CONF_DIR=/opt/hadoop/hadoop/conf</span><br></pre></td></tr></table></figure><h2 id="在从服务器上安装Hadoop"><a href="#在从服务器上安装Hadoop" class="headerlink" title="在从服务器上安装Hadoop"></a>在从服务器上安装Hadoop</h2><p>按照给定的命令在所有从服务器上安装Hadoop。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># su hadoop </span></span><br><span class="line">$ <span class="built_in">cd</span> /opt/hadoop </span><br><span class="line">$ scp -r hadoop hadoop-slave-1:/opt/hadoop </span><br><span class="line">$ scp -r hadoop hadoop-slave-2:/opt/hadoop</span><br></pre></td></tr></table></figure><h2 id="在主服务器上配置Hadoop"><a href="#在主服务器上配置Hadoop" class="headerlink" title="在主服务器上配置Hadoop"></a>在主服务器上配置Hadoop</h2><p>打开主服务器并按照给定的命令配置它。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># su hadoop </span></span><br><span class="line">$ <span class="built_in">cd</span> /opt/hadoop/hadoop</span><br></pre></td></tr></table></figure><h3 id="配置主节点"><a href="#配置主节点" class="headerlink" title="配置主节点"></a>配置主节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vi etc/hadoop/masters</span><br><span class="line">hadoop-master</span><br></pre></td></tr></table></figure><h3 id="配置从节点"><a href="#配置从节点" class="headerlink" title="配置从节点"></a>配置从节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vi etc/hadoop/slaves</span><br><span class="line">hadoop-slave-1 </span><br><span class="line">hadoop-slave-2</span><br></pre></td></tr></table></figure><h3 id="在Hadoop-Master上格式化NameNode"><a href="#在Hadoop-Master上格式化NameNode" class="headerlink" title="在Hadoop Master上格式化NameNode"></a>在Hadoop Master上格式化NameNode</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># su hadoop </span></span><br><span class="line">$ <span class="built_in">cd</span> /opt/hadoop/hadoop </span><br><span class="line">$ bin/hadoop namenode –format</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">11/10/14 10:58:07 INFO namenode.NameNode: STARTUP_MSG: /************************************************************ </span><br><span class="line">STARTUP_MSG: Starting NameNode </span><br><span class="line">STARTUP_MSG: host = hadoop-master/192.168.1.109 </span><br><span class="line">STARTUP_MSG: args = [-format] </span><br><span class="line">STARTUP_MSG: version = 1.2.0 </span><br><span class="line">STARTUP_MSG: build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2 -r 1479473; compiled by <span class="string">'hortonfo'</span> on Mon May 6 06:59:37 UTC 2013 </span><br><span class="line">STARTUP_MSG: java = 1.7.0_71 ************************************************************/ 11/10/14 10:58:08 INFO util.GSet: Computing capacity <span class="keyword">for</span> map BlocksMap editlog=/opt/hadoop/hadoop/dfs/name/current/edits</span><br><span class="line">………………………………………………….</span><br><span class="line">………………………………………………….</span><br><span class="line">…………………………………………………. 11/10/14 10:58:08 INFO common.Storage: Storage directory /opt/hadoop/hadoop/dfs/name has been successfully formatted. 11/10/14 10:58:08 INFO namenode.NameNode: </span><br><span class="line">SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at hadoop-master/192.168.1.15 ************************************************************/</span><br></pre></td></tr></table></figure><h2 id="启动Hadoop服务"><a href="#启动Hadoop服务" class="headerlink" title="启动Hadoop服务"></a>启动Hadoop服务</h2><p>下面的命令是在Hadoop- master上启动所有Hadoop服务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line">$ start-all.sh</span><br></pre></td></tr></table></figure><h2 id="在Hadoop集群中添加一个新的DataNode"><a href="#在Hadoop集群中添加一个新的DataNode" class="headerlink" title="在Hadoop集群中添加一个新的DataNode"></a>在Hadoop集群中添加一个新的DataNode</h2><p>下面给出了向Hadoop集群添加新节点的步骤。</p><h3 id="配置网络"><a href="#配置网络" class="headerlink" title="配置网络"></a>配置网络</h3><p>使用适当的网络配置向现有Hadoop集群添加新节点。假设以下网络配置。</p><p>新节点配置:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IP address : 192.168.1.103 </span><br><span class="line">netmask : 255.255.255.0</span><br><span class="line">hostname : slave3.in</span><br></pre></td></tr></table></figure><h2 id="添加用户和SSH访问"><a href="#添加用户和SSH访问" class="headerlink" title="添加用户和SSH访问"></a>添加用户和SSH访问</h2><h3 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h3><p>在新节点上，使用以下命令添加“hadoop”用户，并将hadoop用户的密码设置为“hadoop op123”或您想要的任何值。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">useradd hadoop</span><br><span class="line">passwd hadoop</span><br></pre></td></tr></table></figure><p>设置从主服务器到新从服务器的连接。</p><h3 id="在主服务器上执行以下操作"><a href="#在主服务器上执行以下操作" class="headerlink" title="在主服务器上执行以下操作"></a>在主服务器上执行以下操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/.ssh </span><br><span class="line">chmod 700 <span class="variable">$HOME</span>/.ssh </span><br><span class="line">ssh-keygen -t rsa -P <span class="string">''</span> -f <span class="variable">$HOME</span>/.ssh/id_rsa </span><br><span class="line">cat <span class="variable">$HOME</span>/.ssh/id_rsa.pub &gt;&gt; <span class="variable">$HOME</span>/.ssh/authorized_keys </span><br><span class="line">chmod 644 <span class="variable">$HOME</span>/.ssh/authorized_keys</span><br><span class="line">Copy the public key to new slave node <span class="keyword">in</span> hadoop user <span class="variable">$HOME</span> directory</span><br><span class="line">scp <span class="variable">$HOME</span>/.ssh/id_rsa.pub hadoop@192.168.1.103:/home/hadoop/</span><br></pre></td></tr></table></figure><h3 id="对从服务器执行以下操作"><a href="#对从服务器执行以下操作" class="headerlink" title="对从服务器执行以下操作"></a>对从服务器执行以下操作</h3><p>登录到hadoop。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su hadoop ssh -X hadoop@192.168.1.103</span><br></pre></td></tr></table></figure><p>将公钥内容复制到文件<strong>“$HOME/.ssh/authorized_keys”</strong>中。然后通过执行以下命令更改相同的权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.ssh </span><br><span class="line">chmod 700 <span class="variable">$HOME</span>/.ssh</span><br><span class="line">cat id_rsa.pub &gt;&gt;<span class="variable">$HOME</span>/.ssh/authorized_keys </span><br><span class="line">chmod 644 <span class="variable">$HOME</span>/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p>检查主机上的ssh登录。现在检查是否可以在不需要主节点密码的情况下ssh到新节点。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop@192.168.1.103 or hadoop@slave3</span><br></pre></td></tr></table></figure><h2 id="设置新节点的主机名"><a href="#设置新节点的主机名" class="headerlink" title="设置新节点的主机名"></a>设置新节点的主机名</h2><p>您可以在文件<strong>/etc/sysconfig/network</strong>中设置主机名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">On new slave3 machine</span><br><span class="line">NETWORKING=yes </span><br><span class="line">HOSTNAME=slave3.in</span><br></pre></td></tr></table></figure><p>要使更改生效，可以重新启动机器，也可以使用相应的主机名将主机名命令运行到新机器上(重新启动是一个不错的选择)。</p><p>slave3节点机器上:</p><p>主机名slave3.in</p><p>使用以下行更新集群所有机器上的/etc/hosts:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.1.102 slave3.in slave3</span><br></pre></td></tr></table></figure><p>现在尝试用主机名ping计算机，检查它是否解析到IP。</p><p>在新节点机器上:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping master.in</span><br></pre></td></tr></table></figure><h2 id="在新节点上启动DataNode"><a href="#在新节点上启动DataNode" class="headerlink" title="在新节点上启动DataNode"></a>在新节点上启动DataNode</h2><p>使用<strong>$HADOOP_HOME/bin/hadoop-daemon.sh</strong>手动启动datanode守护进程。它将自动联系主节点(NameNode)并加入集群。我们还应该将新节点添加到主服务器中的conf/slave文件中。基于脚本的命令将识别新节点。</p><p><strong>登录到新节点</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su hadoop or ssh -X hadoop@192.168.1.103</span><br></pre></td></tr></table></figure><p><strong>使用以下命令在新添加的从节点上启动HDFS</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure><p><strong>使用jps命令检查新节点</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">7141 DataNode</span><br><span class="line">10312 Jps</span><br></pre></td></tr></table></figure><h2 id="从Hadoop集群中删除DataNode"><a href="#从Hadoop集群中删除DataNode" class="headerlink" title="从Hadoop集群中删除DataNode"></a>从Hadoop集群中删除DataNode</h2><p>我们可以在节点运行时动态地从集群中删除节点，而不会丢失任何数据。HDFS提供了一个退役特性，可以确保安全地删除节点。使用方法如下:</p><h3 id="Step-1-登录到主服务器"><a href="#Step-1-登录到主服务器" class="headerlink" title="Step 1: 登录到主服务器"></a>Step 1: 登录到主服务器</h3><p>登录到安装Hadoop的主机用户。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ su hadoop</span><br></pre></td></tr></table></figure><h3 id="Step-2-改变集群配置"><a href="#Step-2-改变集群配置" class="headerlink" title="Step 2: 改变集群配置"></a>Step 2: 改变集群配置</h3><p>在启动集群之前，必须配置一个排除文件。添加一个名为dfs.hosts的键。排除到我们的<strong>$HADOOP_HOME/etc/hadoop/hdfs-site.xml</strong>文件。与此键关联的值提供了NameNode本地文件系统上文件的完整路径，该文件系统包含不允许连接到HDFS的机器列表。</p><p>例如，将这些行添加到<strong>etc/hadoop/hdfs-site.xml</strong>文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt; </span><br><span class="line">   &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; </span><br><span class="line">   &lt;value&gt;/home/hadoop/hadoop-1.2.1/hdfs_exclude.txt&lt;/value&gt; </span><br><span class="line">   &lt;description&gt;DFS exclude&lt;/description&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h3 id="Step-3-确定要删除的主机"><a href="#Step-3-确定要删除的主机" class="headerlink" title="Step 3: 确定要删除的主机"></a>Step 3: 确定要删除的主机</h3><p>要删除的每台机器都应该添加到<strong>hdfs_exclude.txt</strong>文件中，每行一个域名。这将阻止它们连接到NameNode。如果您想删除DataNode2，则<strong>“/home/hadoop/hadoop-1.2.1/hdfs_exclude.txt”</strong>的内容文件如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slave2.in</span><br></pre></td></tr></table></figure><h3 id="Step-4-强制重加载配置"><a href="#Step-4-强制重加载配置" class="headerlink" title="Step 4: 强制重加载配置"></a>Step 4: 强制重加载配置</h3><p>运行<strong>“$HADOOP_HOME/bin/hadoop dfsadmin -refreshNodes”</strong>命令，不带引号。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop dfsadmin -refreshNodes</span><br></pre></td></tr></table></figure><p>这将强制NameNode重新读取其配置，包括最新更新的“exclude”文件。它将在一段时间内对节点进行退役，允许将每个节点的块复制到计划保持活动的机器上。</p><p>在<strong>slave2.in</strong>，检查jps命令输出。一段时间后，您将看到DataNode进程自动关闭。</p><h3 id="Step-5-关闭节点"><a href="#Step-5-关闭节点" class="headerlink" title="Step 5: 关闭节点"></a>Step 5: 关闭节点</h3><p>删除过程完成后，删除的硬件可以安全停机进行维护。向dfsadmin运行report命令检查状态。下面的命令将描述删除节点和连接到集群的节点的状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop dfsadmin -report</span><br></pre></td></tr></table></figure><h3 id="Step-6-再次排除文件"><a href="#Step-6-再次排除文件" class="headerlink" title="Step 6: 再次排除文件"></a>Step 6: 再次排除文件</h3><p>机器一旦退役，就可以从“exclude”文件中删除它们。再次运行<strong>“$HADOOP_HOME/bin/hadoop dfsadmin -refreshNodes”</strong>会将排斥文件读入NameNode;允许数据节点在维护完成后重新加入集群，或者在集群中再次需要额外的容量，等等。</p><p><strong>特别提示：</strong>如果遵循上述流程，且tasktracker流程仍在节点上运行，则需要关闭该流程。一种方法是断开机器，就像我们在上述步骤中所做的那样。主进程将自动识别进程，并将声明为已死。删除任务跟踪器不需要遵循相同的过程，因为与DataNode相比，任务跟踪器并不重要。DataNode包含您希望安全地删除的数据，而不会丢失任何数据。</p><p>任务跟踪器可以在任何时间点通过以下命令动态运行/关闭。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop-daemon.sh stop tasktracker <span class="variable">$HADOOP_HOME</span>/bin/hadoop-daemon.sh start tasktracker</span><br></pre></td></tr></table></figure><p><strong>原文链接：</strong> <span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfbXVsdGlfbm9kZV9jbHVzdGVyLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_multi_node_cluster.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本章介绍了Hadoop多节点集群在分布式环境中的设置。&lt;/p&gt;
&lt;p&gt;由于无法演示整个集群，我们使用三个系统(一个主系统和两个从系统)解释Hadoop集群环境;以下是他们的IP地址。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hadoop Master: 192.168.1.15 (hadoop-master)&lt;/li&gt;
&lt;li&gt;Hadoop Slave: 192.168.1.16 (hadoop-slave-1)&lt;/li&gt;
&lt;li&gt;Hadoop Slave: 192.168.1.17 (hadoop-slave-2)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;按照下面给出的步骤设置Hadoop多节点集群。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：流</title>
    <link href="https://tangguangen.com/hadoop-streaming/"/>
    <id>https://tangguangen.com/hadoop-streaming/</id>
    <published>2018-12-01T12:08:25.000Z</published>
    <updated>2018-12-07T05:21:31.241Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop流是Hadoop发行版附带的实用程序。这个实用程序允许您使用任何可执行文件或脚本作为mapper 和/或reducer创建和运行Map/Reduce作业。</p><h2 id="Python例子"><a href="#Python例子" class="headerlink" title="Python例子"></a>Python例子</h2><p>对于Hadoop流，我们正在考虑word-count 问题。Hadoop中的任何工作都必须有两个阶段:mapper和reducer。我们已经在python脚本中为mapper和reducer编写了在Hadoop下运行它的代码。也可以用Perl和Ruby编写相同的代码。</p><a id="more"></a><h3 id="Mapper-Phase-Code"><a href="#Mapper-Phase-Code" class="headerlink" title="Mapper Phase Code"></a>Mapper Phase Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">!/usr/bin/python</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="comment"># Input takes from standard input for myline in sys.stdin: </span></span><br><span class="line"><span class="comment"># Remove whitespace either side myline = myline.strip() </span></span><br><span class="line"><span class="comment"># Break the line into words words = myline.split() </span></span><br><span class="line"><span class="comment"># Iterate the words list for myword in words: </span></span><br><span class="line"><span class="comment"># Write the results to standard output print '%s\t%s' % (myword, 1)</span></span><br></pre></td></tr></table></figure><p>确保该文件具有执行权限(chmod +x /home/ expert/hadoop-1.2.1/mapper.py)。</p><h3 id="Reducer-Phase-Code"><a href="#Reducer-Phase-Code" class="headerlink" title="Reducer Phase Code"></a>Reducer Phase Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter </span><br><span class="line"><span class="keyword">import</span> sys </span><br><span class="line">current_word = <span class="string">""</span></span><br><span class="line">current_count = <span class="number">0</span> </span><br><span class="line">word = <span class="string">""</span> </span><br><span class="line"><span class="comment"># Input takes from standard input for myline in sys.stdin: </span></span><br><span class="line"><span class="comment"># Remove whitespace either side myline = myline.strip() </span></span><br><span class="line"><span class="comment"># Split the input we got from mapper.py word, count = myline.split('\t', 1) </span></span><br><span class="line"><span class="comment"># Convert count variable to integer </span></span><br><span class="line">   <span class="keyword">try</span>: </span><br><span class="line">      count = int(count) </span><br><span class="line"><span class="keyword">except</span> ValueError: </span><br><span class="line">   <span class="comment"># Count was not a number, so silently ignore this line continue</span></span><br><span class="line"><span class="keyword">if</span> current_word == word: </span><br><span class="line">   current_count += count </span><br><span class="line"><span class="keyword">else</span>: </span><br><span class="line">   <span class="keyword">if</span> current_word: </span><br><span class="line">      <span class="comment"># Write result to standard output print '%s\t%s' % (current_word, current_count) </span></span><br><span class="line">   current_count = count</span><br><span class="line">   current_word = word</span><br><span class="line"><span class="comment"># Do not forget to output the last word if needed! </span></span><br><span class="line"><span class="keyword">if</span> current_word == word: </span><br><span class="line">   <span class="keyword">print</span> <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br></pre></td></tr></table></figure><p>将mapper和reducer代码分别保存在Hadoop home目录下的mapper.py and reducer.py 文件。确保这些文件具有执行权限(chmod +x mapper.py)。和chmod +x reducer.py)。由于python对缩进敏感，所以可以从下面的链接下载相同的代码。</p><h2 id="WordCount程序的执行"><a href="#WordCount程序的执行" class="headerlink" title="WordCount程序的执行"></a>WordCount程序的执行</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop jar contrib/streaming/hadoop-streaming-1.</span><br><span class="line">2.1.jar \</span><br><span class="line">   -input input_dirs \ </span><br><span class="line">   -output output_dir \ </span><br><span class="line">   -mapper &lt;path/mapper.py \ </span><br><span class="line">   -reducer &lt;path/reducer.py</span><br></pre></td></tr></table></figure><p>其中“\”用于行延续，以确保清晰的可读性。</p><p>例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop jar contrib/streaming/hadoop-streaming-1.2.1.jar -input myinput -output myoutput -mapper /home/expert/hadoop-1.2.1/mapper.py -reducer /home/expert/hadoop-1.2.1/reducer.py</span><br></pre></td></tr></table></figure><h2 id="流是如何工作的"><a href="#流是如何工作的" class="headerlink" title="流是如何工作的"></a>流是如何工作的</h2><p>在上面的例子中，mapper和reducer都是python脚本，它们从标准输入读取输入并将输出输出到标准输出。该实用程序将创建Map/Reduce作业，将作业提交到适当的集群，并监视作业的进度，直到作业完成。</p><p>当为mappers指定脚本时，每个mapper任务将在初始化mapper时作为单独的进程启动脚本。当mapper任务运行时，它将其输入转换为行，并将这些行提供给流程的标准输入(STDIN)。同时，mapper从流程的标准输出(STDOUT)中收集面向行的输出，并将每一行转换为键/值对，作为mapper的输出进行收集。默认情况下，直到第一个制表符的行前缀是键，行其余部分(不包括制表符)是值。如果行中没有制表符，则将整行视为键，值为null。但是，这可以根据需要定制。</p><p>当为reducers指定脚本时，每个reducer任务将作为单独的进程启动脚本，然后初始化reducer。当reducer任务运行时，它将输入键/值对转换为行，并将这些行提供给流程的标准输入(STDIN)。同时，reducer从流程的标准输出(STDOUT)中收集面向行的输出，将每一行转换为键/值对，作为reducer的输出进行收集。默认情况下，直到第一个制表符的行前缀是键，行其余部分(不包括制表符)是值。但是，这可以根据特定的需求进行定制。</p><h2 id="重要的命令"><a href="#重要的命令" class="headerlink" title="重要的命令"></a>重要的命令</h2><table><thead><tr><th>Parameters</th><th>Description</th></tr></thead><tbody><tr><td>-input directory/file-name</td><td>Input location for mapper. (Required)</td></tr><tr><td>-output directory-name</td><td>Output location for reducer. (Required)</td></tr><tr><td>-mapper executable or script or JavaClassName</td><td>Mapper executable. (Required)</td></tr><tr><td>-reducer executable or script or JavaClassName</td><td>Reducer executable. (Required)</td></tr><tr><td>-file file-name</td><td>Makes the mapper, reducer, or combiner executable available locally on the compute nodes.</td></tr><tr><td>-inputformat JavaClassName</td><td>Class you supply should return key/value pairs of Text class. If not specified, TextInputFormat is used as the default.</td></tr><tr><td>-outputformat JavaClassName</td><td>Class you supply should take key/value pairs of Text class. If not specified, TextOutputformat is used as the default.</td></tr><tr><td>-partitioner JavaClassName</td><td>Class that determines which reduce a key is sent to.</td></tr><tr><td>-combiner streamingCommand or JavaClassName</td><td>Combiner executable for map output.</td></tr><tr><td>-cmdenv name=value</td><td>Passes the environment variable to streaming commands.</td></tr><tr><td>-inputreader</td><td>For backwards-compatibility: specifies a record reader class (instead of an input format class).</td></tr><tr><td>-verbose</td><td>Verbose output.</td></tr><tr><td>-lazyOutput</td><td>Creates output lazily. For example, if the output format is based on FileOutputFormat, the output file is created only on the first call to output.collect (or Context.write).</td></tr><tr><td>-numReduceTasks</td><td>Specifies the number of reducers.</td></tr><tr><td>-mapdebug</td><td>Script to call when map task fails.</td></tr><tr><td>-reducedebug</td><td>Script to call when reduce task fails.</td></tr></tbody></table><p><strong>原文链接：</strong> <span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3Bfc3RyZWFtaW5nLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_streaming.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop流是Hadoop发行版附带的实用程序。这个实用程序允许您使用任何可执行文件或脚本作为mapper 和/或reducer创建和运行Map/Reduce作业。&lt;/p&gt;
&lt;h2 id=&quot;Python例子&quot;&gt;&lt;a href=&quot;#Python例子&quot; class=&quot;headerlink&quot; title=&quot;Python例子&quot;&gt;&lt;/a&gt;Python例子&lt;/h2&gt;&lt;p&gt;对于Hadoop流，我们正在考虑word-count 问题。Hadoop中的任何工作都必须有两个阶段:mapper和reducer。我们已经在python脚本中为mapper和reducer编写了在Hadoop下运行它的代码。也可以用Perl和Ruby编写相同的代码。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：MapReduce</title>
    <link href="https://tangguangen.com/hadoop-mapreduce/"/>
    <id>https://tangguangen.com/hadoop-mapreduce/</id>
    <published>2018-12-01T10:25:44.000Z</published>
    <updated>2018-12-07T05:21:14.139Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce是一个框架，我们可以使用它编写应用程序，以一种可靠的方式，并行地在大型商品硬件集群上处理大量数据。</p><h2 id="MapReduce是什么"><a href="#MapReduce是什么" class="headerlink" title="MapReduce是什么"></a>MapReduce是什么</h2><p>MapReduce是一种基于java的分布式计算处理技术和程序模型。MapReduce算法包含两个重要的任务，即Map和Reduce。Map接受一组数据并将其转换为另一组数据，其中单个元素被分解为元组(键/值对)。其次是reduce task，它将来自映射的输出作为输入，并将这些数据元组组合成较小的元组集合。顾名思义，reduce任务总是在映射作业之后执行。</p><p>MapReduce的主要优点是，它很容易在多个计算节点上扩展数据处理。在MapReduce模型下，数据处理原语称为映射器和约简器。将数据处理应用程序分解为映射器和还原器有时是很重要的。但是，一旦我们在MapReduce表单中编写了一个应用程序，将应用程序扩展到集群中的成百上千甚至上万台机器上，这仅仅是一个配置更改。正是这种简单的可伸缩性吸引了许多程序员使用MapReduce模型。</p><a id="more"></a><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul><li>通常MapReduce范例是基于将计算机发送到数据所在的位置!</li><li>MapReduce程序分三个阶段执行，即map阶段、shuffle阶段和reduce阶段。<ul><li><strong>Map stage</strong> : 映射或映射程序的工作是处理输入数据。通常输入数据以文件或目录的形式存储在Hadoop文件系统(HDFS)中。输入文件逐行传递给mapper函数。映射器处理数据并创建几个小数据块。</li><li><strong>Reduce stage</strong> : 这一阶段是<strong>Shuffle</strong> 阶段和<strong>Reduce</strong> 阶段的结合。<strong>Reduce</strong> 的工作是处理来自mapper的数据。处理之后，它会生成一组新的输出，这些输出将存储在HDFS中。</li></ul></li><li>在MapReduce作业期间，Hadoop将映射和Reduce任务发送到集群中的适当服务器。</li><li>该框架管理数据传递的所有细节，例如发出任务、验证任务完成以及在节点之间围绕集群复制数据。</li><li>大多数计算发生在节点上，节点上的数据位于本地磁盘上，从而减少了网络流量。</li><li>在完成给定的任务后，集群收集并减少数据，形成适当的结果，并将其发送回Hadoop服务器。</li></ul><img title="mapreduce_algorithm" alt="mapreduce_algorithm" src="http://cdn.tangguangen.com/images/mapreduce_algorithm.jpg"><h2 id="输入和输出-Java方面"><a href="#输入和输出-Java方面" class="headerlink" title="输入和输出(Java方面)"></a>输入和输出(Java方面)</h2><p>MapReduce框架对对进行操作，即框架将作业的输入视为一组对，并生成一组对作为作业的输出，可以想象为不同类型。</p><p>键和值类应该由框架序列化，因此需要实现可写接口。此外，关键类必须实现可写可比较的接口，以便按框架进行排序。MapReduce作业的输入输出类型:(Input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt;-&gt; reduce -&gt; &lt;k3, v3&gt;(Output)</p><table><thead><tr><th></th><th>Input</th><th>Output</th></tr></thead><tbody><tr><td>Map</td><td>&lt;k1, v1&gt;</td><td>list (&lt;k2, v2&gt;)</td></tr><tr><td>Reduce</td><td>&lt;k2, list(v2)&gt;</td><td>list (&lt;k3, v3&gt;)</td></tr></tbody></table><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><ul><li><strong>PayLoad</strong> - 应用程序实现了映射和Reduce函数，构成了作业的核心。</li><li><strong>Mapper</strong> - <strong>Mapper</strong>将输入键/值对映射到一组中间键/值对。</li><li><strong>NamedNode</strong> - 管理Hadoop分布式文件系统(HDFS)的节点。</li><li><strong>DataNode</strong> - 存放数据的节点。</li><li><strong>MasterNode</strong> - 作业跟踪程序运行的节点，它接受来自客户端的作业请求。</li><li><strong>SlaveNode</strong> - 节点，Map和Reduce程序在此运行。</li><li><strong>JobTracker</strong> - 计划作业并跟踪分配给Task tracker的作业。</li><li><strong>Task Tracker</strong> - 跟踪任务并向JobTracker报告状态。</li><li><strong>Job</strong> - 程序是Mapper 和Reducer 在数据集上的执行。</li><li><strong>Task</strong> - 在数据片上执行Mapper 或Reducer 。</li><li><strong>Task Attempt</strong> - 试图在SlaveNode上执行任务的特定实例。</li></ul><h2 id="示例场景"><a href="#示例场景" class="headerlink" title="示例场景"></a>示例场景</h2><p>下面是关于一个组织的电力消耗的数据。它包含了每个月的用电量和不同年份的年平均用电量。</p><table><thead><tr><th></th><th>Jan</th><th>Feb</th><th>Mar</th><th>Apr</th><th>May</th><th>Jun</th><th>Jul</th><th>Aug</th><th>Sep</th><th>Oct</th><th>Nov</th><th>Dec</th><th>Avg</th></tr></thead><tbody><tr><td>1979</td><td>23</td><td>23</td><td>2</td><td>43</td><td>24</td><td>25</td><td>26</td><td>26</td><td>26</td><td>26</td><td>25</td><td>26</td><td>25</td></tr><tr><td>1980</td><td>26</td><td>27</td><td>28</td><td>28</td><td>28</td><td>30</td><td>31</td><td>31</td><td>31</td><td>30</td><td>30</td><td>30</td><td>29</td></tr><tr><td>1981</td><td>31</td><td>32</td><td>32</td><td>32</td><td>33</td><td>34</td><td>35</td><td>36</td><td>36</td><td>34</td><td>34</td><td>34</td><td>34</td></tr><tr><td>1984</td><td>39</td><td>38</td><td>39</td><td>39</td><td>39</td><td>41</td><td>42</td><td>43</td><td>40</td><td>39</td><td>38</td><td>38</td><td>40</td></tr><tr><td>1985</td><td>38</td><td>39</td><td>39</td><td>39</td><td>39</td><td>41</td><td>41</td><td>41</td><td>00</td><td>40</td><td>39</td><td>39</td><td>45</td></tr></tbody></table><p>如果以上述数据作为输入，我们必须编写应用程序来处理它，并产生诸如查找最大使用年、最小使用年等结果。这是一个针对记录数量有限的程序员的演练。它们将简单地编写逻辑来生成所需的输出，并将数据传递给所写的应用程序。</p><p>但是，想想一个州自形成以来所有大型工业的电力消耗数据。</p><p>当我们编写应用程序来处理这种大容量数据时，</p><ul><li>它们将花费大量的时间来执行。</li><li>当我们将数据从源移动到网络服务器等时，将会有大量的网络流量。</li></ul><p>为了解决这些问题，我们有<strong>MapReduce</strong>框架。</p><h3 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h3><p>以上数据保存为<strong>sample.txt</strong>作为输入。输入文件如下所示</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1979   23   23   2   43   24   25   26   26   26   26   25   26  25 </span><br><span class="line">1980   26   27   28  28   28   30   31   31   31   30   30   30  29 </span><br><span class="line">1981   31   32   32  32   33   34   35   36   36   34   34   34  34 </span><br><span class="line">1984   39   38   39  39   39   41   42   43   40   39   38   38  40 </span><br><span class="line">1985   38   39   39  39   39   41   41   41   00   40   39   39  45</span><br></pre></td></tr></table></figure><h3 id="程序实例"><a href="#程序实例" class="headerlink" title="程序实例"></a>程序实例</h3><p>下面是使用MapReduce框架对示例数据的程序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> hadoop; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException; </span><br><span class="line"><span class="keyword">import</span> java.io.IOException; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.*; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.*; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.*; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.*; </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessUnits</span> </span></span><br><span class="line"><span class="class"></span>&#123; </span><br><span class="line">   <span class="comment">//Mapper class </span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">E_EMapper</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> </span></span><br><span class="line"><span class="class">   <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span> ,/*<span class="title">Input</span> <span class="title">key</span> <span class="title">Type</span> */ </span></span><br><span class="line"><span class="class">   <span class="title">Text</span>,                /*<span class="title">Input</span> <span class="title">value</span> <span class="title">Type</span>*/ </span></span><br><span class="line"><span class="class">   <span class="title">Text</span>,                /*<span class="title">Output</span> <span class="title">key</span> <span class="title">Type</span>*/ </span></span><br><span class="line"><span class="class">   <span class="title">IntWritable</span>&gt;        /*<span class="title">Output</span> <span class="title">value</span> <span class="title">Type</span>*/ </span></span><br><span class="line"><span class="class">   </span>&#123; </span><br><span class="line">      </span><br><span class="line">      <span class="comment">//Map function </span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, </span></span></span><br><span class="line"><span class="function"><span class="params">      OutputCollector&lt;Text, IntWritable&gt; output,   </span></span></span><br><span class="line"><span class="function"><span class="params">      Reporter reporter)</span> <span class="keyword">throws</span> IOException </span></span><br><span class="line"><span class="function">      </span>&#123; </span><br><span class="line">         String line = value.toString(); </span><br><span class="line">         String lasttoken = <span class="keyword">null</span>; </span><br><span class="line">         StringTokenizer s = <span class="keyword">new</span> StringTokenizer(line,<span class="string">"\t"</span>); </span><br><span class="line">         String year = s.nextToken(); </span><br><span class="line">         </span><br><span class="line">         <span class="keyword">while</span>(s.hasMoreTokens())</span><br><span class="line">            &#123;</span><br><span class="line">               lasttoken=s.nextToken();</span><br><span class="line">            &#125; </span><br><span class="line">            </span><br><span class="line">         <span class="keyword">int</span> avgprice = Integer.parseInt(lasttoken); </span><br><span class="line">         output.collect(<span class="keyword">new</span> Text(year), <span class="keyword">new</span> IntWritable(avgprice)); </span><br><span class="line">      &#125; </span><br><span class="line">   &#125; </span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   <span class="comment">//Reducer class </span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">E_EReduce</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> </span></span><br><span class="line"><span class="class">   <span class="title">Reducer</span>&lt; <span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span> &gt; </span></span><br><span class="line"><span class="class">   </span>&#123;  </span><br><span class="line">   </span><br><span class="line">      <span class="comment">//Reduce function </span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">( Text key, Iterator &lt;IntWritable&gt; values, </span></span></span><br><span class="line"><span class="function"><span class="params">         OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span></span><br><span class="line"><span class="function">         </span>&#123; </span><br><span class="line">            <span class="keyword">int</span> maxavg=<span class="number">30</span>; </span><br><span class="line">            <span class="keyword">int</span> val=Integer.MIN_VALUE; </span><br><span class="line">            </span><br><span class="line">            <span class="keyword">while</span> (values.hasNext()) </span><br><span class="line">            &#123; </span><br><span class="line">               <span class="keyword">if</span>((val=values.next().get())&gt;maxavg) </span><br><span class="line">               &#123; </span><br><span class="line">                  output.collect(key, <span class="keyword">new</span> IntWritable(val)); </span><br><span class="line">               &#125; </span><br><span class="line">            &#125; </span><br><span class="line"> </span><br><span class="line">         &#125; </span><br><span class="line">   &#125;  </span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   <span class="comment">//Main function </span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span><span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">   </span>&#123; </span><br><span class="line">      JobConf conf = <span class="keyword">new</span> JobConf(ProcessUnits.class); </span><br><span class="line">      </span><br><span class="line">      conf.setJobName(<span class="string">"max_eletricityunits"</span>); </span><br><span class="line">      conf.setOutputKeyClass(Text.class);</span><br><span class="line">      conf.setOutputValueClass(IntWritable.class); </span><br><span class="line">      conf.setMapperClass(E_EMapper.class); </span><br><span class="line">      conf.setCombinerClass(E_EReduce.class); </span><br><span class="line">      conf.setReducerClass(E_EReduce.class); </span><br><span class="line">      conf.setInputFormat(TextInputFormat.class); </span><br><span class="line">      conf.setOutputFormat(TextOutputFormat.class); </span><br><span class="line">      </span><br><span class="line">      FileInputFormat.setInputPaths(conf, <span class="keyword">new</span> Path(args[<span class="number">0</span>])); </span><br><span class="line">      FileOutputFormat.setOutputPath(conf, <span class="keyword">new</span> Path(args[<span class="number">1</span>])); </span><br><span class="line">      </span><br><span class="line">      JobClient.runJob(conf); </span><br><span class="line">   &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将上述程序保存为<strong>Process .java</strong>。下面将解释程序的编译和执行。</p><h2 id="编译和执行"><a href="#编译和执行" class="headerlink" title="编译和执行"></a>编译和执行</h2><p>让我们假设我们在Hadoop用户的主目录中(例如/home/hadoop)。</p><p>按照下面给出的步骤编译和执行上述程序</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>下面的命令是创建一个目录来存储编译后的java类。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir units</span><br></pre></td></tr></table></figure><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p>下载<strong>Hadoop-core-1.2.1.jar</strong>，它用于编译和执行MapReduce程序。请访问以下链接<span class="exturl" data-url="aHR0cDovL212bnJlcG9zaXRvcnkuY29tL2FydGlmYWN0L29yZy5hcGFjaGUuaGFkb29wL2hhZG9vcC1jb3JlLzEuMi4x" title="http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core/1.2.1">http://mvnrepository.com/…<i class="fa fa-external-link"></i></span>下载jar。让我们假设下载的文件夹是<strong>/home/hadoop/</strong>。</p><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>以下命令用于编译<strong>ProcessUnits.java</strong>程序，并为该程序创建一个jar。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ javac -classpath hadoop-core-1.2.1.jar -d units ProcessUnits.java </span><br><span class="line">$ jar -cvf units.jar -C units/ .</span><br></pre></td></tr></table></figure><h3 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h3><p>下面的命令用于在HDFS中创建一个输入目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir input_dir</span><br></pre></td></tr></table></figure><h3 id="Step-7"><a href="#Step-7" class="headerlink" title="Step 7"></a>Step 7</h3><p>下面的命令用于通过从输入目录中获取输入文件来运行Eleunit_max应用程序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop jar units.jar hadoop.ProcessUnits input_dir output_dir</span><br></pre></td></tr></table></figure><p>稍等片刻，直到执行该文件。执行后，如下图所示，输出将包含输入分割数、映射任务数、reducer任务数等。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">INFO mapreduce.Job: Job job_1414748220717_0002 </span><br><span class="line">completed successfully </span><br><span class="line"><span class="number">14</span>/<span class="number">10</span>/<span class="number">31</span> <span class="number">06</span>:<span class="number">02</span>:<span class="number">52</span> </span><br><span class="line">INFO mapreduce.Job: Counters: <span class="number">49</span> </span><br><span class="line">File System Counters </span><br><span class="line"> </span><br><span class="line">FILE: Number of bytes read=<span class="number">61</span> </span><br><span class="line">FILE: Number of bytes written=<span class="number">279400</span> </span><br><span class="line">FILE: Number of read operations=<span class="number">0</span> </span><br><span class="line">FILE: Number of large read operations=<span class="number">0</span>   </span><br><span class="line">FILE: Number of write operations=<span class="number">0</span> </span><br><span class="line">HDFS: Number of bytes read=<span class="number">546</span> </span><br><span class="line">HDFS: Number of bytes written=<span class="number">40</span> </span><br><span class="line">HDFS: Number of read operations=<span class="number">9</span> </span><br><span class="line">HDFS: Number of large read operations=<span class="number">0</span> </span><br><span class="line">HDFS: Number of write operations=<span class="number">2</span> Job Counters </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   Launched map tasks=<span class="number">2</span>  </span><br><span class="line">   Launched reduce tasks=<span class="number">1</span> </span><br><span class="line">   Data-local map tasks=<span class="number">2</span>  </span><br><span class="line">   <span class="function">Total time spent by all maps in occupied <span class="title">slots</span> <span class="params">(ms)</span></span>=<span class="number">146137</span> </span><br><span class="line">   <span class="function">Total time spent by all reduces in occupied <span class="title">slots</span> <span class="params">(ms)</span></span>=<span class="number">441</span>   </span><br><span class="line">   <span class="function">Total time spent by all map <span class="title">tasks</span> <span class="params">(ms)</span></span>=<span class="number">14613</span> </span><br><span class="line">   <span class="function">Total time spent by all reduce <span class="title">tasks</span> <span class="params">(ms)</span></span>=<span class="number">44120</span> </span><br><span class="line">   Total vcore-seconds taken by all map tasks=<span class="number">146137</span> </span><br><span class="line">   </span><br><span class="line">   Total vcore-seconds taken by all reduce tasks=<span class="number">44120</span> </span><br><span class="line">   Total megabyte-seconds taken by all map tasks=<span class="number">149644288</span> </span><br><span class="line">   Total megabyte-seconds taken by all reduce tasks=<span class="number">45178880</span> </span><br><span class="line">   </span><br><span class="line">Map-Reduce Framework </span><br><span class="line"> </span><br><span class="line">Map input records=<span class="number">5</span>  </span><br><span class="line">   Map output records=<span class="number">5</span>   </span><br><span class="line">   Map output bytes=<span class="number">45</span>  </span><br><span class="line">   Map output materialized bytes=<span class="number">67</span>  </span><br><span class="line">   Input split bytes=<span class="number">208</span> </span><br><span class="line">   Combine input records=<span class="number">5</span>  </span><br><span class="line">   Combine output records=<span class="number">5</span> </span><br><span class="line">   Reduce input groups=<span class="number">5</span>  </span><br><span class="line">   Reduce shuffle bytes=<span class="number">6</span>  </span><br><span class="line">   Reduce input records=<span class="number">5</span>  </span><br><span class="line">   Reduce output records=<span class="number">5</span>  </span><br><span class="line">   Spilled Records=<span class="number">10</span>  </span><br><span class="line">   Shuffled Maps =<span class="number">2</span>  </span><br><span class="line">   Failed Shuffles=<span class="number">0</span>  </span><br><span class="line">   Merged Map outputs=<span class="number">2</span>  </span><br><span class="line">   <span class="function">GC time <span class="title">elapsed</span> <span class="params">(ms)</span></span>=<span class="number">948</span>  </span><br><span class="line">   <span class="function">CPU time <span class="title">spent</span> <span class="params">(ms)</span></span>=<span class="number">5160</span>  </span><br><span class="line">   <span class="function">Physical <span class="title">memory</span> <span class="params">(bytes)</span> snapshot</span>=<span class="number">47749120</span>  </span><br><span class="line">   <span class="function">Virtual <span class="title">memory</span> <span class="params">(bytes)</span> snapshot</span>=<span class="number">2899349504</span>  </span><br><span class="line">   <span class="function">Total committed heap <span class="title">usage</span> <span class="params">(bytes)</span></span>=<span class="number">277684224</span></span><br><span class="line">     </span><br><span class="line">File Output Format Counters </span><br><span class="line"> </span><br><span class="line">   Bytes Written=<span class="number">40</span></span><br></pre></td></tr></table></figure><h3 id="Step-8"><a href="#Step-8" class="headerlink" title="Step 8"></a>Step 8</h3><p>下面的命令用于验证输出文件夹中生成的文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -ls output_dir/</span><br></pre></td></tr></table></figure><h3 id="Step-9"><a href="#Step-9" class="headerlink" title="Step 9"></a>Step 9</h3><p>下面的命令用于查看<strong>Part-00000</strong>文件中的输出。这个文件是由HDFS生成的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -cat output_dir/part-00000</span><br></pre></td></tr></table></figure><p>下面是MapReduce程序生成的输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1981    34 </span><br><span class="line">1984    40 </span><br><span class="line">1985    45</span><br></pre></td></tr></table></figure><h3 id="Step-10"><a href="#Step-10" class="headerlink" title="Step 10"></a>Step 10</h3><p>下面的命令用于将输出文件夹从HDFS复制到本地文件系统进行分析。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -cat output_dir/part-00000/bin/hadoop dfs get output_dir /home/hadoop</span><br></pre></td></tr></table></figure><h2 id="重要的命令"><a href="#重要的命令" class="headerlink" title="重要的命令"></a>重要的命令</h2><p>所有Hadoop命令都由<strong>$HADOOP_HOME/bin/hadoop</strong>命令调用。在没有任何参数的情况下运行Hadoop脚本将打印所有命令的描述。</p><p><strong>Usage</strong> : hadoop [–config confdir] COMMAND</p><p>下表列出了可用的选项及其描述。</p><table><thead><tr><th>Options</th><th>Description</th></tr></thead><tbody><tr><td>namenode -format</td><td>Formats the DFS filesystem.</td></tr><tr><td>secondarynamenode</td><td>Runs the DFS secondary namenode.</td></tr><tr><td>namenode</td><td>Runs the DFS namenode.</td></tr><tr><td>datanode</td><td>Runs a DFS datanode.</td></tr><tr><td>dfsadmin</td><td>Runs a DFS admin client.</td></tr><tr><td>mradmin</td><td>Runs a Map-Reduce admin client.</td></tr><tr><td>fsck</td><td>Runs a DFS filesystem checking utility.</td></tr><tr><td>fs</td><td>Runs a generic filesystem user client.</td></tr><tr><td>balancer</td><td>Runs a cluster balancing utility.</td></tr><tr><td>oiv</td><td>Applies the offline fsimage viewer to an fsimage.</td></tr><tr><td>fetchdt</td><td>Fetches a delegation token from the NameNode.</td></tr><tr><td>jobtracker</td><td>Runs the MapReduce job Tracker node.</td></tr><tr><td>pipes</td><td>Runs a Pipes job.</td></tr><tr><td>tasktracker</td><td>Runs a MapReduce task Tracker node.</td></tr><tr><td>historyserver</td><td>Runs job history servers as a standalone daemon.</td></tr><tr><td>job</td><td>Manipulates the MapReduce jobs.</td></tr><tr><td>queue</td><td>Gets information regarding JobQueues.</td></tr><tr><td>version</td><td>Prints the version.</td></tr><tr><td>jar <jar></jar></td><td>Runs a jar file.</td></tr><tr><td>distcp <srcurl> <desturl></desturl></srcurl></td><td>Copies file or directories recursively.</td></tr><tr><td>distcp2 <srcurl> <desturl></desturl></srcurl></td><td>DistCp version 2.</td></tr><tr><td>archive -archiveName NAME -p</td><td>Creates a hadoop archive.</td></tr><tr><td><parent path=""> <src>* <dest></dest></src></parent></td><td></td></tr><tr><td>classpath</td><td>Prints the class path needed to get the Hadoop jar and the required libraries.</td></tr><tr><td>daemonlog</td><td>Get/Set the log level for each daemon</td></tr></tbody></table><h2 id="如何与MapReduce作业交互"><a href="#如何与MapReduce作业交互" class="headerlink" title="如何与MapReduce作业交互"></a>如何与MapReduce作业交互</h2><p>Usage: hadoop job [GENERIC_OPTIONS]</p><p>以下是Hadoop作业中可用的通用选项。</p><table><thead><tr><th>GENERIC_OPTIONS</th><th>Description</th></tr></thead><tbody><tr><td>-submit <job-file></job-file></td><td>Submits the job.</td></tr><tr><td>-status <job-id></job-id></td><td>Prints the map and reduce completion percentage and all job counters.</td></tr><tr><td>-counter <job-id> <group-name> <countername></countername></group-name></job-id></td><td>Prints the counter value.</td></tr><tr><td>-kill <job-id></job-id></td><td>Kills the job.</td></tr><tr><td>-events <job-id> &lt;fromevent-#&gt; &lt;#-of-events&gt;</job-id></td><td>Prints the events’ details received by jobtracker for the given range.</td></tr><tr><td>-history [all] <joboutputdir> - history &lt; jobOutputDir&gt;</joboutputdir></td><td>Prints job details, failed and killed tip details. More details about the job such as successful tasks and task attempts made for each task can be viewed by specifying the [all] option.</td></tr><tr><td>-list[all]</td><td>Displays all jobs. -list displays only jobs which are yet to complete.</td></tr><tr><td>-kill-task <task-id></task-id></td><td>Kills the task. Killed tasks are NOT counted against failed attempts.</td></tr><tr><td>-fail-task <task-id></task-id></td><td>Fails the task. Failed tasks are counted against failed attempts.</td></tr><tr><td>-set-priority <job-id> <priority></priority></job-id></td><td>Changes the priority of the job. Allowed priority values are VERY_HIGH, HIGH, NORMAL, LOW, VERY_LOW</td></tr></tbody></table><h3 id="查看工作状态"><a href="#查看工作状态" class="headerlink" title="查看工作状态"></a>查看工作状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -status &lt;JOB-ID&gt; </span><br><span class="line">e.g. </span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -status job_201310191043_0004</span><br></pre></td></tr></table></figure><h3 id="查看作业输出目录的历史"><a href="#查看作业输出目录的历史" class="headerlink" title="查看作业输出目录的历史"></a>查看作业输出目录的历史</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -<span class="built_in">history</span> &lt;DIR-NAME&gt; </span><br><span class="line">e.g. </span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -<span class="built_in">history</span> /user/expert/output</span><br></pre></td></tr></table></figure><h3 id="kill作业"><a href="#kill作业" class="headerlink" title="kill作业"></a>kill作业</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -<span class="built_in">kill</span> &lt;JOB-ID&gt; </span><br><span class="line">e.g. </span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -<span class="built_in">kill</span> job_201310191043_0004</span><br></pre></td></tr></table></figure><p><strong>原文链接：</strong> <span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfbWFwcmVkdWNlLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MapReduce是一个框架，我们可以使用它编写应用程序，以一种可靠的方式，并行地在大型商品硬件集群上处理大量数据。&lt;/p&gt;
&lt;h2 id=&quot;MapReduce是什么&quot;&gt;&lt;a href=&quot;#MapReduce是什么&quot; class=&quot;headerlink&quot; title=&quot;MapReduce是什么&quot;&gt;&lt;/a&gt;MapReduce是什么&lt;/h2&gt;&lt;p&gt;MapReduce是一种基于java的分布式计算处理技术和程序模型。MapReduce算法包含两个重要的任务，即Map和Reduce。Map接受一组数据并将其转换为另一组数据，其中单个元素被分解为元组(键/值对)。其次是reduce task，它将来自映射的输出作为输入，并将这些数据元组组合成较小的元组集合。顾名思义，reduce任务总是在映射作业之后执行。&lt;/p&gt;
&lt;p&gt;MapReduce的主要优点是，它很容易在多个计算节点上扩展数据处理。在MapReduce模型下，数据处理原语称为映射器和约简器。将数据处理应用程序分解为映射器和还原器有时是很重要的。但是，一旦我们在MapReduce表单中编写了一个应用程序，将应用程序扩展到集群中的成百上千甚至上万台机器上，这仅仅是一个配置更改。正是这种简单的可伸缩性吸引了许多程序员使用MapReduce模型。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：命令手册</title>
    <link href="https://tangguangen.com/hadoop-command-reference/"/>
    <id>https://tangguangen.com/hadoop-command-reference/</id>
    <published>2018-12-01T10:11:02.000Z</published>
    <updated>2018-12-07T05:24:01.612Z</updated>
    
    <content type="html"><![CDATA[<p>与这里演示的相比，<strong>“$HADOOP_HOME/bin/hadoop fs”</strong> 中有更多的命令，尽管这些基本操作可以帮助您入门。不带附加参数运行 ./bin/hadoop dfs 将列出所有可以与 FsShell 系统一起运行的命令。此外，如果遇到问题，<strong>$HADOOP_HOME/bin/hadoop fs -help</strong>命令名将显示有关操作的简短使用摘要。</p><p>所有操作的表如下所示。参数使用以下约定:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"&lt;path&gt;"</span> means any file or directory name. </span><br><span class="line"><span class="string">"&lt;path&gt;..."</span> means one or more file or directory names. </span><br><span class="line"><span class="string">"&lt;file&gt;"</span> means any filename. </span><br><span class="line"><span class="string">"&lt;src&gt;"</span> and <span class="string">"&lt;dest&gt;"</span> are path names <span class="keyword">in</span> a directed operation. </span><br><span class="line"><span class="string">"&lt;localSrc&gt;"</span> and <span class="string">"&lt;localDest&gt;"</span> are paths as above, but on the <span class="built_in">local</span> file system.</span><br></pre></td></tr></table></figure><p>所有其他文件和路径名都引用HDFS中的对象。</p><a id="more"></a><table><thead><tr><th>序号</th><th>命令</th></tr></thead><tbody><tr><td>1.</td><td><strong>ls <path></path></strong> <br>列出路径指定的目录的内容，显示每个条目的名称、权限、所有者、大小和修改日期。</td></tr><tr><td>2.</td><td><strong>lsr <path></path></strong><br>行为类似于-ls，但是递归地显示path的所有子目录中的条目。</td></tr><tr><td>3.</td><td><strong>du <path></path></strong><br>显示与路径匹配的所有文件的磁盘使用情况(以字节为单位);使用完整的HDFS协议前缀报告文件名。</td></tr><tr><td>4.</td><td><strong>dus <path></path></strong><br>与-du类似，但打印路径中所有文件/目录的磁盘使用情况摘要。</td></tr><tr><td>5.</td><td><strong>mv <src><dest></dest></src></strong><br>将src指示的文件或目录移动到HDFS内的dest。</td></tr><tr><td>6.</td><td><strong>cp <src> <dest></dest></src></strong><br>在HDFS中将src标识的文件或目录复制到dest。</td></tr><tr><td>7.</td><td><strong>rm <path></path></strong><br>删除路径标识的文件或空目录。</td></tr><tr><td>8.</td><td><strong>rmr <path></path></strong><br>删除路径标识的文件或目录。递归地删除任何子条目 (i.e., files or subdirectories of path).</td></tr><tr><td>9.</td><td><strong>put <localsrc> <dest></dest></localsrc></strong><br>将由localSrc标识的本地文件系统中的文件或目录复制到DFS中的dest。</td></tr><tr><td>10.</td><td><strong>copyFromLocal <localsrc> <dest><br></dest></localsrc></strong>-put相同</td></tr><tr><td>11.</td><td><strong>moveFromLocal <localsrc> <dest></dest></localsrc></strong><br>将由localSrc标识的本地文件系统中的文件或目录复制到HDFS中的dest，然后成功删除本地副本。</td></tr><tr><td>12.</td><td><strong>get [-crc] <src> <localdest></localdest></src></strong><br>将src标识的HDFS中的文件或目录复制到localDest标识的本地文件系统路径。</td></tr><tr><td>13.</td><td><strong>getmerge <src> <localdest></localdest></src></strong><br>检索与HDFS中的路径src匹配的所有文件，并将它们复制到localDest标识的本地文件系统中合并的单个文件。</td></tr><tr><td>14.</td><td><strong>cat <filen-ame></filen-ame></strong><br>在标准输出上显示文件名的内容。</td></tr><tr><td>15.</td><td><strong>copyToLocal <src> <localdest></localdest></src></strong><br>与 -get相同</td></tr><tr><td>16.</td><td><strong>moveToLocal <src> <localdest></localdest></src></strong><br>类似于-get，但成功时删除HDFS副本。</td></tr><tr><td>17.</td><td><strong>mkdir <path></path></strong><br>在HDFS中创建一个名为path的目录。<br>在路径中创建缺少的任何父目录(e.g., mkdir -p in Linux).</td></tr><tr><td>18.</td><td><strong>setrep [-R][-w] rep <path></path></strong><br>为通过路径到rep标识的文件设置目标复制因子(随着时间的推移，实际复制因子将向目标移动)</td></tr><tr><td>19.</td><td><strong>touchz <path></path></strong><br>在包含当前时间作为时间戳的路径上创建一个文件。如果文件在路径上已经存在，则失败，除非文件的大小已经为0。</td></tr><tr><td>20.</td><td><strong>test -[ezd] <path></path></strong><br>如果路径存在，返回1;长度为零;或者是目录，或者是0。</td></tr><tr><td>21.</td><td><strong>stat [format] <path></path></strong><br>打印关于路径的信息。Format是一个字符串，它接受块大小(%b)、文件名(%n)、块大小(%o)、复制(%r)和修改日期(%y， %y)。</td></tr><tr><td>22.</td><td><strong>tail [-f] <file2name></file2name></strong><br>显示stdout上文件的最后1KB。</td></tr><tr><td>23.</td><td><strong>chmod [-R] mode,mode,… <path></path>…</strong><br>与一个或多个相关联的文件权限对象的更改了路径….使用r模式递归执行更改是3位八进制模式，或{augo}+/-{rwxX}。假设没有指定范围且不应用umask。</td></tr><tr><td>24.</td><td><strong>chown [-R][owner][:[group]] <path></path>…</strong><br>集拥有用户和/或组的文件或目录被路径….如果指定-R，则递归设置所有者。</td></tr><tr><td>25.</td><td>chgrp [-R] group <path></path>…<br>设置拥有小组确认的文件或目录路径….如果指定-R，则递归地设置组。</td></tr><tr><td>26.</td><td><strong>help <cmd-name></cmd-name></strong><br>返回上述命令之一的使用信息。你必须省略cmd中的“-”字符。</td></tr></tbody></table><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfY29tbWFuZF9yZWZlcmVuY2UuaHRt" title="https://www.tutorialspoint.com/hadoop/hadoop_command_reference.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;与这里演示的相比，&lt;strong&gt;“$HADOOP_HOME/bin/hadoop fs”&lt;/strong&gt; 中有更多的命令，尽管这些基本操作可以帮助您入门。不带附加参数运行 ./bin/hadoop dfs 将列出所有可以与 FsShell 系统一起运行的命令。此外，如果遇到问题，&lt;strong&gt;$HADOOP_HOME/bin/hadoop fs -help&lt;/strong&gt;命令名将显示有关操作的简短使用摘要。&lt;/p&gt;
&lt;p&gt;所有操作的表如下所示。参数使用以下约定:&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;&amp;lt;path&amp;gt;&quot;&lt;/span&gt; means any file or directory name. &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;&amp;lt;path&amp;gt;...&quot;&lt;/span&gt; means one or more file or directory names. &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;&amp;lt;file&amp;gt;&quot;&lt;/span&gt; means any filename. &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;&amp;lt;src&amp;gt;&quot;&lt;/span&gt; and &lt;span class=&quot;string&quot;&gt;&quot;&amp;lt;dest&amp;gt;&quot;&lt;/span&gt; are path names &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; a directed operation. &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;&amp;lt;localSrc&amp;gt;&quot;&lt;/span&gt; and &lt;span class=&quot;string&quot;&gt;&quot;&amp;lt;localDest&amp;gt;&quot;&lt;/span&gt; are paths as above, but on the &lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt; file system.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;所有其他文件和路径名都引用HDFS中的对象。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：HDFS操作</title>
    <link href="https://tangguangen.com/hadoop-hdfs-operations/"/>
    <id>https://tangguangen.com/hadoop-hdfs-operations/</id>
    <published>2018-12-01T09:58:19.000Z</published>
    <updated>2018-12-07T05:20:47.876Z</updated>
    
    <content type="html"><![CDATA[<h2 id="启动HDFS"><a href="#启动HDFS" class="headerlink" title="启动HDFS"></a>启动HDFS</h2><p>首先，您必须格式化配置的HDFS文件系统，打开namenode (HDFS服务器)，并执行以下命令。</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop namenode -format</span><br></pre></td></tr></table></figure><p>格式化HDFS之后，启动分布式文件系统。下面的命令将启动namenode以及数据节点作为集群。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-dfs.sh</span><br></pre></td></tr></table></figure><h2 id="列出HDFS中的文件"><a href="#列出HDFS中的文件" class="headerlink" title="列出HDFS中的文件"></a>列出HDFS中的文件</h2><p>在服务器中加载信息后，我们可以使用“ls”查找目录中的文件列表、文件状态。下面给出了可以作为参数传递到目录或文件名的ls语法。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -ls &lt;args&gt;</span><br></pre></td></tr></table></figure><h2 id="将数据插入HDFS"><a href="#将数据插入HDFS" class="headerlink" title="将数据插入HDFS"></a>将数据插入HDFS</h2><p>假设我们在本地系统中一个名为file.txt的文件，应该保存在hdfs文件系统中。按照下面给出的步骤在Hadoop文件系统中插入所需的文件。</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>您必须创建一个输入目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir /user/input</span><br></pre></td></tr></table></figure><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p>使用put命令将数据文件从本地系统传输和存储到Hadoop文件系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -put /home/file.txt /user/input</span><br></pre></td></tr></table></figure><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>您可以使用ls命令验证该文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -ls /user/input</span><br></pre></td></tr></table></figure><h2 id="从HDFS检索数据"><a href="#从HDFS检索数据" class="headerlink" title="从HDFS检索数据"></a>从HDFS检索数据</h2><p>假设HDFS中有一个名为outfile的文件。下面是一个从Hadoop文件系统检索所需文件的简单演示。</p><h3 id="Step-1-1"><a href="#Step-1-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>首先，使用cat命令查看来自HDFS的数据。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -cat /user/output/outfile</span><br></pre></td></tr></table></figure><h3 id="Step-2-1"><a href="#Step-2-1" class="headerlink" title="Step 2"></a>Step 2</h3><p>使用get命令将文件从HDFS获取到本地文件系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -get /user/output/ /home/hadoop_tp/</span><br></pre></td></tr></table></figure><h2 id="关闭HDFS"><a href="#关闭HDFS" class="headerlink" title="关闭HDFS"></a>关闭HDFS</h2><p>可以使用以下命令关闭HDFS</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stop-dfs.sh</span><br></pre></td></tr></table></figure><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfaGRmc19vcGVyYXRpb25zLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_hdfs_operations.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;启动HDFS&quot;&gt;&lt;a href=&quot;#启动HDFS&quot; class=&quot;headerlink&quot; title=&quot;启动HDFS&quot;&gt;&lt;/a&gt;启动HDFS&lt;/h2&gt;&lt;p&gt;首先，您必须格式化配置的HDFS文件系统，打开namenode (HDFS服务器)，并执行以下命令。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：HDFS概述</title>
    <link href="https://tangguangen.com/hadoop-hdfs-overview/"/>
    <id>https://tangguangen.com/hadoop-hdfs-overview/</id>
    <published>2018-12-01T09:35:37.000Z</published>
    <updated>2018-12-07T05:20:56.303Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop文件系统采用分布式文件系统设计开发。它在普通硬件上运行。与其他分布式系统不同，HDFS具有很高的容错性，并且使用低成本的硬件进行设计。</p><p>HDFS存储大量数据并提供更容易的访问。为了存储如此巨大的数据，文件被存储在多台机器上。这些文件以冗余的方式存储，以便在发生故障时将系统从可能的数据损失中拯救出来。HDFS还使应用程序可用于并行处理。</p><h2 id="HDFS的特点"><a href="#HDFS的特点" class="headerlink" title="HDFS的特点"></a>HDFS的特点</h2><ul><li>适用于分布式存储和处理。</li><li>Hadoop提供了一个与HDFS交互的命令接口。</li><li>namenode和datanode的内置服务器可以方便地检查集群的状态。</li><li>对文件系统数据的流访问。</li><li>HDFS提供文件权限和身份验证。</li></ul><a id="more"></a><h2 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h2><p>下面给出Hadoop文件系统的架构。</p><img title="hdfs_architecture" alt="hdfs_architecture" src="http://cdn.tangguangen.com/images/hdfs_architecture.jpg"><p>HDFS遵循主从体系结构，它具有以下元素。</p><h3 id="Namenode"><a href="#Namenode" class="headerlink" title="Namenode"></a>Namenode</h3><p>namenode是包含GNU/Linux操作系统和namenode软件的商品硬件。它是一种可以在普通硬件上运行的软件。具有namenode的系统充当主服务器，它执行以下任务:</p><ul><li>管理文件系统名称空间。</li><li>管理客户对文件的访问。</li><li>它还执行文件系统操作，如重命名、关闭和打开文件和目录。</li></ul><h3 id="Datanode"><a href="#Datanode" class="headerlink" title="Datanode"></a>Datanode</h3><p>datanode是一种具有GNU/Linux操作系统和datanode软件的普通硬件。对于集群中的每个节点(商品硬件/系统)，都将有一个datanode。这些节点管理其系统的数据存储。</p><ul><li>数据节点根据客户端请求在文件系统上执行读写操作。</li><li>它们还根据namenode的指令执行块创建、删除和复制等操作。</li></ul><h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><p>用户数据一般存储在HDFS文件中。文件系统中的文件将被分成一个或多个段和/或存储在单个数据节点中。这些文件段称为块。换句话说，HDFS可以读写的最小数据量称为块。默认块大小为64MB，但是可以根据需要在HDFS配置中进行更改而增加。</p><h2 id="HDFS的目标"><a href="#HDFS的目标" class="headerlink" title="HDFS的目标"></a>HDFS的目标</h2><ul><li><strong>故障检测与恢复:</strong> 由于HDFS包含大量的商用硬件，部件故障频繁。因此，HDFS应该具有快速、自动的故障检测和恢复机制。</li><li><strong>海量的数据集:</strong> HDFS每个集群应该有数百个节点，以管理拥有庞大数据集的应用程序。</li><li><strong>硬件在数据上</strong> 当计算发生在数据附近时，可以有效地完成请求的任务。特别是在涉及到大量数据集的情况下，它会减少网络流量并增加吞吐量。</li></ul><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfaGRmc19vdmVydmlldy5odG0=" title="https://www.tutorialspoint.com/hadoop/hadoop_hdfs_overview.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop文件系统采用分布式文件系统设计开发。它在普通硬件上运行。与其他分布式系统不同，HDFS具有很高的容错性，并且使用低成本的硬件进行设计。&lt;/p&gt;
&lt;p&gt;HDFS存储大量数据并提供更容易的访问。为了存储如此巨大的数据，文件被存储在多台机器上。这些文件以冗余的方式存储，以便在发生故障时将系统从可能的数据损失中拯救出来。HDFS还使应用程序可用于并行处理。&lt;/p&gt;
&lt;h2 id=&quot;HDFS的特点&quot;&gt;&lt;a href=&quot;#HDFS的特点&quot; class=&quot;headerlink&quot; title=&quot;HDFS的特点&quot;&gt;&lt;/a&gt;HDFS的特点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;适用于分布式存储和处理。&lt;/li&gt;
&lt;li&gt;Hadoop提供了一个与HDFS交互的命令接口。&lt;/li&gt;
&lt;li&gt;namenode和datanode的内置服务器可以方便地检查集群的状态。&lt;/li&gt;
&lt;li&gt;对文件系统数据的流访问。&lt;/li&gt;
&lt;li&gt;HDFS提供文件权限和身份验证。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
</feed>
