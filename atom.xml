<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>逍遥&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tangguangen.com/"/>
  <updated>2018-11-28T07:32:18.954Z</updated>
  <id>https://tangguangen.com/</id>
  
  <author>
    <name>逍遥</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LeetCode 292. Nim游戏</title>
    <link href="https://tangguangen.com/2018/11/28/LeetCode-292-Nim%E6%B8%B8%E6%88%8F/"/>
    <id>https://tangguangen.com/2018/11/28/LeetCode-292-Nim游戏/</id>
    <published>2018-11-28T07:19:19.000Z</published>
    <updated>2018-11-28T07:32:18.954Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h2><blockquote><p>你和你的朋友，两个人一起玩 <span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS9OaW0lRTYlQjglQjglRTYlODglOEYvNjczNzEwNQ==" title="https://baike.baidu.com/item/Nim%E6%B8%B8%E6%88%8F/6737105">Nim游戏<i class="fa fa-external-link"></i></span>：桌子上有一堆石头，每次你们轮流拿掉 1 - 3 块石头。 拿掉最后一块石头的人就是获胜者。你作为先手。</p><p>你们是聪明人，每一步都是最优解。 编写一个函数，来判断你是否可以在给定石头数量的情况下赢得游戏。</p><p><strong>示例:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: 4</span><br><span class="line">&gt; 输出: false </span><br><span class="line">&gt; 解释: 如果堆中有 4 块石头，那么你永远不会赢得比赛；</span><br><span class="line">&gt;      因为无论你拿走 1 块、2 块 还是 3 块石头，最后一块石头总是会被你的朋友拿走。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>简单题，<span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlQjclQjQlRTQlQkIlODAlRTUlOEQlOUElRTUlQTUlOTUvNzEzOTc4Mg==" title="https://baike.baidu.com/item/%E5%B7%B4%E4%BB%80%E5%8D%9A%E5%A5%95/7139782">巴什博弈<i class="fa fa-external-link"></i></span>。</p><p>显然，如果n=m+1，那么由于一次最多只能取m个，所以，无论先取者拿走多少个，后取者都能够一次拿走剩余的物品，后者取胜。因此我们发现了如何取胜的法则：如果n=（m+1）r+s，（r为任意自然数，s≤m),那么先取者要拿走s个物品，如果后取者拿走k（≤m)个，那么先取者再拿走m+1-k个，结果剩下（m+1）（r-1）个，以后保持这样的取法，那么先取者肯定获胜。总之，要保持给对手留下（m+1）的倍数，就能最后获胜。</p><p>对于巴什博弈，那么我们规定，如果最后取光者输，那么又会如何呢？</p><p>（n-1）%（m+1）==0则后手胜利</p><p>先手会重新决定策略，所以不是简单的相反行的</p><h2 id="JAVA-SOLUTION"><a href="#JAVA-SOLUTION" class="headerlink" title="JAVA SOLUTION"></a>JAVA SOLUTION</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canWinNim</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> n % <span class="number">4</span> == <span class="number">0</span> ? <span class="keyword">false</span> : <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><h3 id="威佐夫博奕"><a href="#威佐夫博奕" class="headerlink" title="威佐夫博奕"></a><span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlQTglODElRTQlQkQlOTAlRTUlQTQlQUIlRTUlOEQlOUElRTUlQkMlODgvMTk4NTgyNTY=" title="https://baike.baidu.com/item/%E5%A8%81%E4%BD%90%E5%A4%AB%E5%8D%9A%E5%BC%88/19858256">威佐夫博奕<i class="fa fa-external-link"></i></span></h3><p>威佐夫博弈（Wythoff’s game）：有两堆各若干个物品，两个人轮流从任一堆取至少一个或同时从两堆中取同样多的物品，规定每次至少取一个，多者不限，最后取光者得胜。</p><p><strong>两个人如果都采用正确操作，那么面对非奇异局势，先拿者必胜；反之，则后拿者取胜。</strong></p><p>那么任给一个局势， (a，b)，怎样判断它是不是奇异局势呢？我们有如下公式：<br>$$<br>ak =\lfloor \frac{k}{2}(1+\sqrt{5}) \rfloor ，<br>$$</p><p>$$<br>bk= ak + k \space \space\space\space（k=0，1，2，…n)<br>$$</p><h3 id="尼姆博奕"><a href="#尼姆博奕" class="headerlink" title="尼姆博奕"></a><span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlQjAlQkMlRTUlQTclODYlRTUlOEQlOUElRTUlQTUlOTU=" title="https://baike.baidu.com/item/%E5%B0%BC%E5%A7%86%E5%8D%9A%E5%A5%95">尼姆博奕<i class="fa fa-external-link"></i></span></h3><p>指的是这样的一个博弈游戏，目前有任意堆石子，每堆石子个数也是任意的，双方轮流从中取出石子，规则如下：<br>1)每一步应取走至少一枚石子；每一步只能从某一堆中取走部分或全部石子；<br>2)如果谁取到最后一枚石子就胜。</p><p>判断当前局势是否为必胜（必败）局势：<br>把所有堆的石子数目用二进制数表示出来，当<strong>全部这些数按位异或</strong>结果为0时当前局面为必败局面，否则为必胜局面；</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;  </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="keyword">int</span> temp[ <span class="number">20</span> ]; <span class="comment">//火柴的堆数  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> i, n, min;  </span><br><span class="line">    <span class="keyword">while</span>( <span class="built_in">cin</span> &gt;&gt; n )  </span><br><span class="line">    &#123;  </span><br><span class="line">        <span class="keyword">for</span>( i = <span class="number">0</span>; i &lt; n; i++ )  </span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; temp[ i ]; <span class="comment">//第i个火柴堆的数量  </span></span><br><span class="line">        min = temp[ <span class="number">0</span> ];  </span><br><span class="line">        <span class="keyword">for</span>( i = <span class="number">1</span>; i &lt; n ; i++ )  </span><br><span class="line">            min = min^temp[ i ]; <span class="comment">//按位异或  </span></span><br><span class="line">        <span class="keyword">if</span>( min == <span class="number">0</span> )  </span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"Lose"</span> &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//输  </span></span><br><span class="line">        <span class="keyword">else</span>  </span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"Win"</span> &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//赢  </span></span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="斐波那契博弈"><a href="#斐波那契博弈" class="headerlink" title="斐波那契博弈"></a><strong>斐波那契博弈</strong></h3><p>有一堆个数为n的石子，游戏双方轮流取石子，满足：<br>1)先手不能在第一次把所有的石子取完；<br>2)之后每次可以取的石子数介于1到对手刚取的石子数的2倍之间（包含1和对手刚取的石子数的2倍）。<br>约定取走最后一个石子的人为赢家，求必败态。</p><p>这个游戏叫做斐波那契博弈，肯定和<span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTYlOTYlOTAlRTYlQjMlQTIlRTklODIlQTMlRTUlQTUlOTElRTYlOTUlQjAlRTUlODglOTcvOTkxNDU=" title="https://baike.baidu.com/item/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/99145">斐波那契数列<i class="fa fa-external-link"></i></span>：$f[n]：1,2,3,5,8,13,21,34,55,89,… $有密切的关系。如果试验一番之后，可以猜测：<strong>先手胜当且仅当n不是斐波那契数。换句话说，必败态构成斐波那契数列。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目描述：&quot;&gt;&lt;a href=&quot;#题目描述：&quot; class=&quot;headerlink&quot; title=&quot;题目描述：&quot;&gt;&lt;/a&gt;题目描述：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;你和你的朋友，两个人一起玩 &lt;span class=&quot;exturl&quot; data-url=&quot;
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>[转载]原码, 反码, 补码 详解</title>
    <link href="https://tangguangen.com/2018/11/26/%E8%BD%AC%E8%BD%BD-%E5%8E%9F%E7%A0%81-%E5%8F%8D%E7%A0%81-%E8%A1%A5%E7%A0%81-%E8%AF%A6%E8%A7%A3/"/>
    <id>https://tangguangen.com/2018/11/26/转载-原码-反码-补码-详解/</id>
    <published>2018-11-26T13:42:58.000Z</published>
    <updated>2018-11-26T13:45:31.379Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作者：<span class="exturl" data-url="aHR0cDovL3d3dy5jbmJsb2dzLmNvbS96aGFuZ3ppcWl1Lw==" title="http://www.cnblogs.com/zhangziqiu/">张子秋<i class="fa fa-external-link"></i></span><br>出处：<span class="exturl" data-url="aHR0cDovL3d3dy5jbmJsb2dzLmNvbS96aGFuZ3ppcWl1Lw==" title="http://www.cnblogs.com/zhangziqiu/">http://www.cnblogs.com/zhangziqiu/<i class="fa fa-external-link"></i></span> </p></blockquote><p>本篇文章讲解了计算机的原码, 反码和补码. 并且进行了深入探求了为何要使用反码和补码, 以及更进一步的论证了为何可以用反码, 补码的加法计算原码的减法. 论证部分如有不对的地方请各位牛人帮忙指正! 希望本文对大家学习计算机基础有所帮助!</p><h2 id="一-机器数和真值"><a href="#一-机器数和真值" class="headerlink" title="一. 机器数和真值"></a>一. 机器数和真值</h2><p>在学习原码, 反码和补码之前, 需要先了解机器数和真值的概念.</p><h3 id="1、机器数"><a href="#1、机器数" class="headerlink" title="1、机器数"></a>1、机器数</h3><p>一个数在计算机中的二进制表示形式,  叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号, 正数为0, 负数为1.</p><p>比如，十进制中的数 +3 ，计算机字长为8位，转换成二进制就是00000011。如果是 -3 ，就是 10000011 。</p><p>那么，这里的 00000011 和 10000011 就是机器数。</p><h3 id="2、真值"><a href="#2、真值" class="headerlink" title="2、真值"></a>2、真值</h3><blockquote><p>因为第一位是符号位，所以机器数的形式值就不等于真正的数值。例如上面的有符号数 10000011，其最高位1代表负，其真正数值是 -3 而不是形式值131（10000011转换成十进制等于131）。所以，为区别起见，将带符号位的机器数对应的真正数值称为机器数的真值。</p></blockquote><p>例：0000 0001的真值 = +000 0001 = +1，1000 0001的真值 = –000 0001 = –1</p><h2 id="二-原码-反码-补码的基础概念和计算方法"><a href="#二-原码-反码-补码的基础概念和计算方法" class="headerlink" title="二. 原码, 反码, 补码的基础概念和计算方法."></a>二. 原码, 反码, 补码的基础概念和计算方法.</h2><p>在探求为何机器要使用补码之前, 让我们先了解原码, 反码和补码的概念.对于一个数, 计算机要使用一定的编码方式进行存储. 原码, 反码, 补码是机器存储一个具体数字的编码方式.</p><h3 id="1-原码"><a href="#1-原码" class="headerlink" title="1. 原码"></a>1. 原码</h3><p>原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是8位二进制:</p><blockquote><p>[+1]原 = 0000 0001</p><p>[-1]原 = 1000 0001</p></blockquote><p>第一位是符号位. 因为第一位是符号位, 所以8位二进制数的取值范围就是:</p><blockquote><p>[1111 1111 , 0111 1111]</p></blockquote><p>即</p><blockquote><p>[-127 , 127]</p></blockquote><p>原码是人脑最容易理解和计算的表示方式.</p><h3 id="2-反码"><a href="#2-反码" class="headerlink" title="2. 反码"></a>2. 反码</h3><p>反码的表示方法是:</p><p>正数的反码是其本身</p><p>负数的反码是在其原码的基础上, 符号位不变，其余各个位取反.</p><blockquote><p>[+1] = [00000001]原 = [00000001]反</p><p>[-1] = [10000001]原 = [11111110]反</p></blockquote><p>可见如果一个反码表示的是负数, 人脑无法直观的看出来它的数值. 通常要将其转换成原码再计算.</p><h3 id="3-补码"><a href="#3-补码" class="headerlink" title="3. 补码"></a>3. 补码</h3><p>补码的表示方法是:</p><p>正数的补码就是其本身</p><p>负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)</p><blockquote><p>[+1] = [00000001]原 = [00000001]反 = [00000001]补</p><p>[-1] = [10000001]原 = [11111110]反 = [11111111]补</p></blockquote><p>对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值.</p><h2 id="三-为何要使用原码-反码和补码"><a href="#三-为何要使用原码-反码和补码" class="headerlink" title="三. 为何要使用原码, 反码和补码"></a>三. 为何要使用原码, 反码和补码</h2><p>在开始深入学习前, 我的学习建议是先”死记硬背”上面的原码, 反码和补码的表示方式以及计算方法.</p><p>现在我们知道了计算机可以有三种编码方式表示一个数. 对于正数因为三种编码方式的结果都相同:</p><blockquote><p>[+1] = [00000001]原 = [00000001]反 = [00000001]补</p></blockquote><p>所以不需要过多解释. 但是对于负数:</p><blockquote><p>[-1] = [10000001]原 = [11111110]反 = [11111111]补</p></blockquote><p>可见原码, 反码和补码是完全不同的. 既然原码才是被人脑直接识别并用于计算表示方式, 为何还会有反码和补码呢?</p><p>首先, 因为人脑可以知道第一位是符号位, 在计算的时候我们会根据符号位, 选择对真值区域的加减. (真值的概念在本文最开头). 但是对于计算机, 加减乘数已经是最基础的运算, 要设计的尽量简单. 计算机辨别”符号位”显然会让计算机的基础电路设计变得十分复杂! 于是人们想出了将符号位也参与运算的方法. 我们知道, 根据运算法则减去一个正数等于加上一个负数, 即: 1-1 = 1 + (-1) = 0 , 所以机器可以只有加法而没有减法, 这样计算机运算的设计就更简单了.</p><p>于是人们开始探索 将符号位参与运算, 并且只保留加法的方法. 首先来看原码:</p><p>计算十进制的表达式: 1-1=0</p><blockquote><p>1 - 1 = 1 + (-1) = [00000001]原 + [10000001]原 = [10000010]原 = -2</p></blockquote><p>如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的.这也就是为何计算机内部不使用原码表示一个数.</p><p>为了解决原码做减法的问题, 出现了反码:</p><p>计算十进制的表达式: 1-1=0</p><blockquote><p>1 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原= [0000 0001]反 + [1111 1110]反 = [1111 1111]反 = [1000 0000]原 = -0</p></blockquote><p>发现用反码计算减法, 结果的真值部分是正确的. 而唯一的问题其实就出现在”0”这个特殊的数值上. 虽然人们理解上+0和-0是一样的, 但是0带符号是没有任何意义的. 而且会有[0000 0000]原和[1000 0000]原两个编码表示0.</p><p>于是补码的出现, 解决了0的符号以及两个编码的问题:</p><blockquote><p>1-1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]补 + [1111 1111]补 = [0000 0000]补=[0000 0000]原</p></blockquote><p>这样0用[0000 0000]表示, 而以前出现问题的-0则不存在了.而且可以用[1000 0000]表示-128:</p><blockquote><p>(-1) + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补</p></blockquote><p>-1-127的结果应该是-128, 在用补码运算的结果中, [1000 0000]补 就是-128. 但是注意因为实际上是使用以前的-0的补码来表示-128, 所以-128并没有原码和反码表示.(对-128的补码表示[1000 0000]补算出来的原码是[0000 0000]原, 这是不正确的)</p><p>使用补码, 不仅仅修复了0的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么8位二进制, 使用原码或反码表示的范围为[-127, +127], 而使用补码表示的范围为[-128, 127].</p><p>因为机器使用补码, 所以对于编程中常用到的32位int类型, 可以表示范围是: [-231, 231-1] 因为第一位表示的是符号位.而使用补码表示时又可以多保存一个最小值.</p><h2 id="四-原码-反码-补码-再深入"><a href="#四-原码-反码-补码-再深入" class="headerlink" title="四 原码, 反码, 补码 再深入"></a>四 原码, 反码, 补码 再深入</h2><p>计算机巧妙地把符号位参与运算, 并且将减法变成了加法, 背后蕴含了怎样的数学原理呢?</p><p>将钟表想象成是一个1位的12进制数. 如果当前时间是6点, 我希望将时间设置成4点, 需要怎么做呢?我们可以:</p><blockquote><p>\1. 往回拨2个小时: 6 - 2 = 4</p><p>\2. 往前拨10个小时: (6 + 10) mod 12 = 4</p><p>\3. 往前拨10+12=22个小时: (6+22) mod 12 =4</p></blockquote><p>2,3方法中的mod是指取模操作, 16 mod 12 =4 即用16除以12后的余数是4.</p><p>所以钟表往回拨(减法)的结果可以用往前拨(加法)替代!</p><p>现在的焦点就落在了如何用一个正数, 来替代一个负数. 上面的例子我们能感觉出来一些端倪, 发现一些规律. 但是数学是严谨的. 不能靠感觉.</p><p>首先介绍一个数学中相关的概念: 同余</p><h3 id="同余的概念"><a href="#同余的概念" class="headerlink" title="同余的概念"></a>同余的概念</h3><p>两个整数a，b，若它们除以整数m所得的余数相等，则称a，b对于模m同余</p><p>记作 a ≡ b (mod m)</p><p>读作 a 与 b 关于模 m 同余。</p><p>举例说明:</p><blockquote><p>4 mod 12 = 4</p><p>16 mod 12 = 4</p><p>28 mod 12 = 4</p></blockquote><p>所以4, 16, 28关于模 12 同余.</p><h3 id="负数取模"><a href="#负数取模" class="headerlink" title="负数取模"></a>负数取模</h3><p>正数进行mod运算是很简单的. 但是负数呢?</p><p>下面是关于mod运算的数学定义:</p><p><span class="exturl" data-url="aHR0cDovL2ltYWdlcy5jbmJsb2dzLmNvbS9jbmJsb2dzX2NvbS96aGFuZ3ppcWl1LzIwMTEwMy8yMDExMDMzMDIxNTU1MDc4OTQuanBn" title="http://images.cnblogs.com/cnblogs_com/zhangziqiu/201103/201103302155507894.jpg"><img src="https://images.cnblogs.com/cnblogs_com/zhangziqiu/201103/201103302155504514.jpg" alt="clip_image001"><i class="fa fa-external-link"></i></span></p><p>上面是截图, “取下界”符号找不到如何输入(word中粘贴过来后乱码). 下面是使用”L”和”J”替换上图的”取下界”符号:</p><blockquote><p>x mod y = x - y L x / y J</p></blockquote><p>上面公式的意思是:</p><p>x mod y等于 x 减去 y 乘上 x与y的商的下界.</p><p>以 -3 mod 2 举例:</p><blockquote><p>-3 mod 2</p><p>= -3 - 2xL -3/2 J</p><p>= -3 - 2xL-1.5J</p><p>= -3 - 2x(-2)</p><p>= -3 + 4 = 1</p></blockquote><p>所以:</p><blockquote><p>(-2) mod 12 = 12-2=10</p><p>(-4) mod 12 = 12-4 = 8</p><p>(-5) mod 12 = 12 - 5 = 7</p></blockquote><h3 id="开始证明"><a href="#开始证明" class="headerlink" title="开始证明"></a>开始证明</h3><p>再回到时钟的问题上:</p><blockquote><p>回拨2小时 = 前拨10小时</p><p>回拨4小时 = 前拨8小时</p><p>回拨5小时= 前拨7小时</p></blockquote><p>注意, 这里发现的规律!</p><p>结合上面学到的同余的概念.实际上:</p><blockquote><p>(-2) mod 12 = 10</p><p>10 mod 12 = 10</p></blockquote><p>-2与10是同余的.</p><blockquote><p>(-4) mod 12 = 8</p><p>8 mod 12 = 8</p></blockquote><p>-4与8是同余的.</p><p>距离成功越来越近了. 要实现用正数替代负数, 只需要运用同余数的两个定理:</p><p>反身性:</p><blockquote><p>a ≡ a (mod m)</p></blockquote><p>这个定理是很显而易见的.</p><p>线性运算定理:</p><blockquote><p>如果a ≡ b (mod m)，c ≡ d (mod m) 那么:</p><p>(1)a ± c ≡ b ± d (mod m)</p><p>(2)a <em> c ≡ b </em> d (mod m)</p></blockquote><p>如果想看这个定理的证明, 请看:<span class="exturl" data-url="aHR0cDovL2JhaWtlLmJhaWR1LmNvbS92aWV3Lzc5MjgyLmh0bQ==" title="http://baike.baidu.com/view/79282.htm">http://baike.baidu.com/view/79282.htm<i class="fa fa-external-link"></i></span></p><p>所以:</p><blockquote><p>7 ≡ 7 (mod 12)</p><p>(-2) ≡ 10 (mod 12)</p><p>7 -2 ≡ 7 + 10 (mod 12)</p></blockquote><p>现在我们为一个负数, 找到了它的正数同余数. 但是并不是7-2 = 7+10, 而是 7 -2 ≡ 7 + 10 (mod 12) , 即计算结果的余数相等.</p><p>接下来回到二进制的问题上, 看一下: 2-1=1的问题.</p><blockquote><p>2-1=2+(-1) = [0000 0010]原 + [1000 0001]原= [0000 0010]反 + [1111 1110]反</p></blockquote><p>先到这一步, -1的反码表示是1111 1110. 如果这里将[1111 1110]认为是原码, 则[1111 1110]原 = -126, 这里将符号位除去, 即认为是126.</p><p>发现有如下规律:</p><blockquote><p>(-1) mod 127 = 126</p><p>126 mod 127 = 126</p></blockquote><p>即:</p><blockquote><p>(-1) ≡ 126 (mod 127)</p><p>2-1 ≡ 2+126 (mod 127)</p></blockquote><p>2-1 与 2+126的余数结果是相同的! 而这个余数, 正式我们的期望的计算结果: 2-1=1</p><p>所以说一个数的反码, 实际上是这个数对于一个膜的同余数. 而这个膜并不是我们的二进制, 而是所能表示的最大值! 这就和钟表一样, 转了一圈后总能找到在可表示范围内的一个正确的数值!</p><p>而2+126很显然相当于钟表转过了一轮, 而因为符号位是参与计算的, 正好和溢出的最高位形成正确的运算结果.</p><p>既然反码可以将减法变成加法, 那么现在计算机使用的补码呢? 为什么在反码的基础上加1, 还能得到正确的结果?</p><blockquote><p>2-1=2+(-1) = [0000 0010]原 + [1000 0001]原 = [0000 0010]补 + [1111 1111]补</p></blockquote><p>如果把[1111 1111]当成原码, 去除符号位, 则:</p><blockquote><p>[0111 1111]原 = 127</p></blockquote><p>其实, 在反码的基础上+1, 只是相当于增加了膜的值:</p><blockquote><p>(-1) mod 128 = 127</p><p>127 mod 128 = 127</p><p>2-1 ≡ 2+127 (mod 128)</p></blockquote><p>此时, 表盘相当于每128个刻度转一轮. 所以用补码表示的运算结果最小值和最大值应该是[-128, 128].</p><p>但是由于0的特殊情况, 没有办法表示128, 所以补码的取值范围是[-128, 127]</p><p>本人一直不善于数学, 所以如果文中有不对的地方请大家多多包含, 多多指点!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;作者：&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cDovL3d3dy5jbmJsb2dzLmNvbS96aGFuZ3ppcWl1Lw==&quot; title=&quot;http://www.cnblogs.com/zhangziqiu
      
    
    </summary>
    
      <category term="计算机基础" scheme="https://tangguangen.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="计算机基础" scheme="https://tangguangen.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop兼容性</title>
    <link href="https://tangguangen.com/2018/11/26/Hadoop%E5%85%BC%E5%AE%B9%E6%80%A7/"/>
    <id>https://tangguangen.com/2018/11/26/Hadoop兼容性/</id>
    <published>2018-11-26T11:39:53.000Z</published>
    <updated>2018-11-26T12:08:52.367Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>本文档介绍了Apache Hadoop项目的兼容性目标。枚举了影响Hadoop开发人员，下游项目和最终用户的Hadoop版本之间的不同类型的兼容性。对于每种类型的兼容性，我们：</p><ul><li>描述对下游项目或最终用户的影响</li><li>在适用的情况下，当允许不兼容的更改时，请调用Hadoop开发人员采用的策略。</li></ul><h2 id="兼容性类型"><a href="#兼容性类型" class="headerlink" title="兼容性类型"></a>兼容性类型</h2><h3 id="Java-API"><a href="#Java-API" class="headerlink" title="Java API"></a>Java API</h3><p>Hadoop接口和类被注释为描述目标受众和稳定性，以保持与先前版本的兼容性。有关详细信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9JbnRlcmZhY2VDbGFzc2lmaWNhdGlvbi5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/InterfaceClassification.html">Hadoop接口分类<i class="fa fa-external-link"></i></span>。</p><ul><li>InterfaceAudience：捕获目标受众，可能的值是Public（对于最终用户和外部项目），LimitedPrivate（对于其他Hadoop组件，以及密切相关的项目，如YARN，MapReduce，HBase等）和Private（用于组件内部）。</li><li>InterfaceStability：描述允许哪些类型的接口更改。可能的值为Stable，Evolving，Unstable和Deprecated。</li></ul><h4 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h4><ul><li>需要公共稳定的API兼容性，以确保最终用户程序和下游项目继续工作而无需修改。</li><li>需要LimitedPrivate-Stable API兼容性，以允许跨次要版本升级单个组件。</li><li>滚动升级需要Private-Stable API兼容性。</li></ul><h4 id="政策"><a href="#政策" class="headerlink" title="政策"></a>政策</h4><ul><li>在主要版本中删除之前，必须至少弃用一个主要版本的Public-Stable API。</li><li>LimitedPrivate-Stable API可以在主要版本中更改，但不能在主要版本中更改。</li><li>Private-Stable API可以在主要版本中更改，但不能在主要版本中更改。</li><li>未注释的类是隐式“私有”。未注释的类成员继承封闭类的注释。</li><li>注意：从proto文件生成的API需要与滚动升级兼容。有关详细信息，请参阅有关电线兼容性的部分。API和有线通信的兼容性策略需要齐头并进，以解决这个问题。</li></ul><h3 id="语义兼容性"><a href="#语义兼容性" class="headerlink" title="语义兼容性"></a>语义兼容性</h3><p>Apache Hadoop努力确保API的行为在版本上保持一致，但正确性的更改可能会导致行为更改。测试和javadoc指定API的行为。社区正在更严格地指定一些API，并增强测试套件以验证是否符合规范，有效地为易于测试的行为子集创建了正式的规范。</p><h4 id="政策-1"><a href="#政策-1" class="headerlink" title="政策"></a>政策</h4><p>可以更改API的行为以修复不正确的行为，这种更改伴随着更新现有错误测试或在更改之前没有测试的情况下添加测试。</p><h3 id="电线兼容性"><a href="#电线兼容性" class="headerlink" title="电线兼容性"></a>电线兼容性</h3><p>线路兼容性涉及在Hadoop进程之间通过线路传输的数据。Hadoop使用Protocol Buffers进行大多数RPC通信。保持兼容性需要禁止如下所述的修改。还应考虑非RPC通信，例如使用HTTP传输HDFS映像作为快照或传输MapTask输出的一部分。潜在的沟通可以分类如下：</p><ul><li>客户端 - 服务器：Hadoop客户端和服务器之间的通信（例如，HDFS客户端到NameNode协议，或YARN客户端到ResourceManager协议）。</li><li>客户端 - 服务器（管理员）：值得区分仅由管理命令（例如，HAAdmin协议）使用的客户端 - 服务器协议的子集，因为这些协议仅影响能够容忍最终用户（使用通用客户端）的更改的管理员服务器协议）不能。</li><li>服务器 - 服务器：服务器之间的通信（例如，DataNode和NameNode之间的协议，或NodeManager和ResourceManager）</li></ul><h4 id="用例-1"><a href="#用例-1" class="headerlink" title="用例"></a>用例</h4><ul><li>即使在将服务器（群集）升级到更高版本（或反之亦然）之后，也需要客户端 - 服务器兼容性以允许用户继续使用旧客户端。例如，Hadoop 2.1.0客户端与Hadoop 2.3.0集群通信。</li><li>还需要客户端 - 服务器兼容性，以允许用户在升级服务器（群集）之前升级客户端。例如，Hadoop 2.4.0客户端与Hadoop 2.3.0集群通信。这允许在完全集群升级之前部署客户端错误修复。请注意，新客户端API或shell命令调用的新群集功能将无法使用。尝试使用尚未部署到群集的新API（包括数据结构中的新字段）的YARN应用程序可能会出现链接异常。</li><li>还需要客户端 - 服务器兼容性，以允许升级单个组件而不升级其他组件。例如，在不升级MapReduce的情况下将HDFS从版本2.1.0升级到2.2.0。</li><li>需要服务器 - 服务器兼容性以允许活动集群中的混合版本，以便可以在不停机的情况下以滚动方式升级集群。</li></ul><h4 id="政策-2"><a href="#政策-2" class="headerlink" title="政策"></a>政策</h4><ul><li>Client-Server和Server-Server兼容性都保留在主要版本中。（不同类别的不同政策尚待考虑。）</li><li>兼容性只能在主要版本中打破，但即使在主要版本中破坏兼容性也会产生严重后果，应在Hadoop社区中进行讨论。</li><li>Hadoop协议在.proto（ProtocolBuffers）文件中定义。客户端 - 服务器协议和服务器 - 服务器协议.proto文件标记为稳定。当.proto文件标记为稳定时，意味着应以兼容的方式进行更改，如下所述：<ul><li>以下更改是兼容的，并且随时允许：<ul><li>添加一个可选字段，期望代码处理由于与旧版本代码的通信而丢失的字段。</li><li>向服务添加新的rpc /方法</li><li>向Message添加新的可选请求</li><li>重命名字段</li><li>重命名.proto文件</li><li>更改影响代码生成的.proto注释（例如java包的名称）</li></ul></li><li>以下更改不兼容，但只能在主要版本中考虑<ul><li>更改rpc /方法名称</li><li>更改rpc / method参数类型或返回类型</li><li>删除rpc /方法</li><li>更改服务名称</li><li>更改消息的名称</li><li>以不兼容的方式修改字段类型（以递归方式定义）</li><li>将可选字段更改为必需</li><li>添加或删除必填字段</li><li>只要可选字段具有允许删除的合理默认值，就删除可选字段</li></ul></li><li>以下更改不兼容，因此从不允许<ul><li>更改字段ID</li><li>重用以前删除的旧字段。</li><li>字段数字很便宜，改变和重用并不是一个好主意。</li></ul></li></ul></li></ul><h3 id="最终用户应用程序的Java二进制兼容性，即Apache-Hadoop-ABI"><a href="#最终用户应用程序的Java二进制兼容性，即Apache-Hadoop-ABI" class="headerlink" title="最终用户应用程序的Java二进制兼容性，即Apache Hadoop ABI"></a>最终用户应用程序的Java二进制兼容性，即Apache Hadoop ABI</h3><p>随着Apache Hadoop修订版的升级，最终用户合理地期望他们的应用程序在没有任何修改的情况下继续工作。这是通过支持API兼容性，语义兼容性和线路兼容性来实现的。</p><p>但是，Apache Hadoop是一个非常复杂的分布式系统，可以为各种各样的用例提供服务。特别是，Apache Hadoop MapReduce是一个非常非常广泛的API; 从某种意义上说，最终用户可以做出广泛的假设，例如当他们的map / reduce任务执行时本地磁盘的布局，他们的任务的环境变量等。在这种情况下，很难完全指定和支持，绝对兼容性。</p><h4 id="用例-2"><a href="#用例-2" class="headerlink" title="用例"></a>用例</h4><ul><li>现有的MapReduce应用程序，包括现有打包的最终用户应用程序罐和Apache Pig，Apache Hive，Cascading等项目，在指向主要版本中升级的Apache Hadoop集群时，应该不加修改。</li><li>现有的YARN应用程序，包括现有打包的最终用户应用程序罐和Apache Tez等项目，在指向主要版本中升级的Apache Hadoop集群时，应该不加修改。</li><li>将数据传入/传出HDFS的现有应用程序（包括现有打包的最终用户应用程序罐和Apache Flume等框架）在指向主要版本中的升级后的Apache Hadoop集群时应该不加修改。</li></ul><h4 id="政策-3"><a href="#政策-3" class="headerlink" title="政策"></a>政策</h4><ul><li>现有的MapReduce，YARN和HDFS应用程序和框架应该在主要版本中不加修改，即支持Apache Hadoop ABI。</li><li>很少一部分应用程序可能会受到磁盘布局等变化的影响，开发人员社区将尽力减少这些变化，并且不会将它们放在次要版本中。在更加严重的情况下，我们将考虑强烈恢复这些重大变更，并在必要时使违规版本无效。</li><li>特别是对于MapReduce应用程序，开发人员社区将尽力支持跨主要版本提供二进制兼容性，例如使用org.apache.hadoop.mapred的应用程序。</li><li>在hadoop-1.x和hadoop-2.x之间兼容支持API。有关详细信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50L2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50LWNvcmUvTWFwUmVkdWNlX0NvbXBhdGliaWxpdHlfSGFkb29wMV9IYWRvb3AyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">hadoop-1.x和hadoop-2.x之间的MapReduce应用程序的兼容性<i class="fa fa-external-link"></i></span>。</li></ul><h3 id="REST-API"><a href="#REST-API" class="headerlink" title="REST API"></a>REST API</h3><p>REST API兼容性对应于请求（URL）和对每个请求的响应（内容，可能包含其他URL）。Hadoop REST API专门用于发布版本（甚至是主要版本）的客户端的稳定使用。以下是公开的REST API：</p><ul><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvV2ViSERGUy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS<i class="fa fa-external-link"></i></span> - 稳定</li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvUmVzb3VyY2VNYW5hZ2VyUmVzdC5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">的ResourceManager<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvTm9kZU1hbmFnZXJSZXN0Lmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">节点管理器<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50L2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50LWNvcmUvTWFwcmVkQXBwTWFzdGVyUmVzdC5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50L2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50LWhzL0hpc3RvcnlTZXJ2ZXJSZXN0Lmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">历史服务器<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvVGltZWxpbmVTZXJ2ZXIuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServer.html">时间轴服务器v1 REST API<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvVGltZWxpbmVTZXJ2aWNlVjIuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">时间轴服务v2 REST API<i class="fa fa-external-link"></i></span></li></ul><h4 id="政策-4"><a href="#政策-4" class="headerlink" title="政策"></a>政策</h4><p>上面文本中注释稳定的API保留了至少一个主要版本的兼容性，并且可能在主要版本中由较新版本的REST API弃用。、</p><h3 id="度量-JMX"><a href="#度量-JMX" class="headerlink" title="度量/ JMX"></a>度量/ JMX</h3><p>虽然Metrics API兼容性受Java API兼容性的约束，但Hadoop公开的实际度量标准需要兼容，以便用户能够自动使用它们（脚本等）。添加其他指标是兼容的。修改（例如，更改单位或测量）或删除现有指标会破坏兼容性。同样，对JMX MBean对象名称的更改也会破坏兼容性。</p><h4 id="政策-5"><a href="#政策-5" class="headerlink" title="政策"></a>政策</h4><p>度量标准应保留主要版本中的兼容性。</p><h3 id="文件格式和元数据"><a href="#文件格式和元数据" class="headerlink" title="文件格式和元数据"></a>文件格式和元数据</h3><p>用户和系统级数据（包括元数据）存储在不同格式的文件中。对元数据或用于存储数据/元数据的文件格式的更改可能导致版本之间不兼容。</p><h4 id="用户级文件格式"><a href="#用户级文件格式" class="headerlink" title="用户级文件格式"></a>用户级文件格式</h4><p>对最终用户用于存储其数据的格式的更改可能会阻止他们在以后的版本中访问数据，因此保持这些文件格式兼容非常重要。人们总是可以添加一种改进现有格式的“新”格式。这些格式的示例包括har，war，SequenceFileFormat等。</p><h5 id="政策-6"><a href="#政策-6" class="headerlink" title="政策"></a>政策</h5><ul><li>非正向兼容的用户文件格式更改仅限于主要版本。当用户文件格式发生变化时，预计新版本将读取现有格式，但可能会以与先前版本不兼容的格式写入数据。此外，社区应优先创建程序必须选择的新格式，而不是对现有格式进行不兼容的更改。</li></ul><h4 id="系统内部文件格式"><a href="#系统内部文件格式" class="headerlink" title="系统内部文件格式"></a>系统内部文件格式</h4><p>Hadoop内部数据也存储在文件中，再次更改这些格式可能会导致不兼容。虽然此类更改不像用户级文件格式那样具有破坏性，但是关于何时可以破坏兼容性的策略很重要。</p><h5 id="MapReduce的"><a href="#MapReduce的" class="headerlink" title="MapReduce的"></a>MapReduce的</h5><p>MapReduce使用I-File等格式来存储特定于MapReduce的数据。</p><h5 id="政策-7"><a href="#政策-7" class="headerlink" title="政策"></a>政策</h5><p>MapReduce内部格式（如IFile）在主要版本中保持兼容性。对这些格式的更改可能导致正在进行的作业失败，因此我们应该确保较新的客户端能够以兼容的方式从旧服务器获取随机数据。</p><h5 id="HDFS元数据"><a href="#HDFS元数据" class="headerlink" title="HDFS元数据"></a>HDFS元数据</h5><p>HDFS以特定格式保存元数据（图像和编辑日志）。对格式或元数据的不兼容更改会阻止后续版本读取旧元数据。此类不兼容的更改可能需要HDFS“升级”才能转换元数据以使其可访问。某些更改可能需要多个此类“升级”。</p><p>根据更改中的不兼容程度，可能会出现以下可能的情况：</p><ul><li>自动：图像自动升级，无需显式“升级”。</li><li>直接：图像可升级，但可能需要一个显式版本“升级”。</li><li>间接：图像可升级，但可能需要先升级到中间版本。</li><li>无法升级：图像无法升级。</li></ul><h5 id="政策-8"><a href="#政策-8" class="headerlink" title="政策"></a>政策</h5><ul><li>版本升级必须允许群集回滚到旧版本及其旧磁盘格式。回滚需要还原原始数据，但不需要还原更新的数据。</li><li>HDFS元数据更改必须可通过任何升级路径进行升级 - 自动，直接或间接。</li><li>尚未考虑基于升级类型的更详细的策略。</li></ul><h3 id="命令行界面（CLI）"><a href="#命令行界面（CLI）" class="headerlink" title="命令行界面（CLI）"></a>命令行界面（CLI）</h3><p>Hadoop命令行程序可以直接通过系统shell或shell脚本使用。更改命令的路径，删除或重命名命令行选项，参数的顺序，或命令返回代码和输出中断兼容性，并可能对用户产生负面影响。</p><h4 id="政策-9"><a href="#政策-9" class="headerlink" title="政策"></a>政策</h4><p>在将一个主要版本删除或在后续主要版本中进行不兼容修改之前，将弃用CLI命令（使用时发出警告）。</p><h3 id="Web-UI"><a href="#Web-UI" class="headerlink" title="Web UI"></a>Web UI</h3><p>Web UI，尤其是网页的内容和布局，更改可能会干扰屏幕抓取网页信息的尝试。</p><h4 id="政策-10"><a href="#政策-10" class="headerlink" title="政策"></a>政策</h4><p>网页并不意味着被删除，因此允许随时对它们进行不兼容的更改。期望用户使用REST API来获取任何信息。</p><h3 id="Hadoop配置文件"><a href="#Hadoop配置文件" class="headerlink" title="Hadoop配置文件"></a>Hadoop配置文件</h3><p>用户使用（1）Hadoop定义的属性来配置和提供Hadoop的提示，以及（2）自定义属性以将信息传递给作业。因此，配置属性的兼容性是双重的：</p><ul><li>修改Hadoop定义的属性的键名，值单位和默认值。</li><li>自定义配置属性键不应与Hadoop定义的属性的命名空间冲突。通常，用户应避免使用Hadoop使用的前缀：hadoop，io，ipc，fs，net，file，ftp，s3，kfs，ha，file，dfs，mapred，mapreduce，yarn。</li></ul><h4 id="政策-11"><a href="#政策-11" class="headerlink" title="政策"></a>政策</h4><ul><li>Hadoop定义的属性至少在一个主要版本中被弃用，然后才会被删除。不允许修改现有属性的单位。</li><li>Hadoop定义的属性的默认值可以在次要/主要版本中更改，但在次要版本中的点版本中保持不变。</li><li>目前，没有明确的政策可以添加/删除新的前缀，以及自定义配置属性要避免的前缀列表。但是，如上所述，用户应避免使用Hadoop使用的前缀：hadoop，io，ipc，fs，net，file，ftp，s3，kfs，ha，file，dfs，mapred，mapreduce，yarn。</li></ul><h3 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h3><p>源代码，工件（源和测试），用户日志，配置文件，输出和作业历史记录都存储在本地文件系统或HDFS的磁盘上。更改这些用户可访问文件的目录结构会破坏兼容性，即使在通过符号链接保留原始路径的情况下（例如，如果路径由配置为不遵循符号链接的servlet访问）。</p><h4 id="政策-12"><a href="#政策-12" class="headerlink" title="政策"></a>政策</h4><ul><li>源代码和构建工件的布局可以随时更改，尤其是在主要版本中。在主要版本中，开发人员将尝试（不保证）保留目录结构; 但是，可以添加/移动/删除单个文件。确保补丁与代码保持同步的最佳方法是将它们提交到Apache源代码树。</li><li>配置文件，用户日志和作业历史记录的目录结构将在主要版本中的次要版本和点版本中保留。</li></ul><h3 id="Java-Classpath"><a href="#Java-Classpath" class="headerlink" title="Java Classpath"></a>Java Classpath</h3><p>针对Hadoop构建的用户应用程序可能会将所有Hadoop jar（包括Hadoop的库依赖项）添加到应用程序的类路径中。添加新依赖项或更新现有依赖项的版本可能会干扰应用程序类路径中的依赖项。</p><h4 id="政策-13"><a href="#政策-13" class="headerlink" title="政策"></a>政策</h4><p>目前，没有关于Hadoop的依赖关系何时可以改变的政策。</p><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>用户和相关项目通常使用导出的环境变量（例如HADOOP_CONF_DIR），因此删除或重命名环境变量是一种不兼容的更改。</p><h4 id="政策-14"><a href="#政策-14" class="headerlink" title="政策"></a>政策</h4><p>目前，没有关于环境变量何时可以改变的政策。开发人员尝试限制对主要版本的更改。</p><h3 id="构建工件"><a href="#构建工件" class="headerlink" title="构建工件"></a>构建工件</h3><p>Hadoop使用maven进行项目管理，更改工件可能会影响现有的用户工作流程。</p><h4 id="政策-15"><a href="#政策-15" class="headerlink" title="政策"></a>政策</h4><ul><li>测试工件：生成的测试jar严格用于内部使用，预计不会在Hadoop之外使用，类似于注释@Private，@ Unstable的API。</li><li>构建工件：hadoop-client工件（maven groupId：artifactId）在主要版本中保持兼容，而其他工件可以以不兼容的方式更改。</li></ul><h3 id="硬件-软件要求"><a href="#硬件-软件要求" class="headerlink" title="硬件/软件要求"></a>硬件/软件要求</h3><p>为了跟上硬件，操作系统，JVM和其他软件的最新进展，新的Hadoop版本或其某些功能可能需要更高版本的版本。对于特定环境，升级Hadoop可能需要升级其他相关软件组件。</p><h4 id="政策-16"><a href="#政策-16" class="headerlink" title="政策"></a>政策</h4><ul><li>硬件<ul><li>架构：社区没有计划将Hadoop限制为特定架构，但可以进行特定于系列的优化。</li><li>最小资源：虽然无法保证Hadoop守护程序所需的最低资源，但社区尝试不在次要版本中增加要求。</li></ul></li><li>操作系统：社区将尝试在次要版本中维护相同的操作系统要求（操作系统内核版本）。目前，GNU / Linux和Microsoft Windows是社区正式支持的操作系统，而Apache Hadoop在其他操作系统（如Apple MacOSX，Solaris等）上运行良好。</li><li>除非所讨论的JVM版本不受支持，否则JVM要求不会在同一次要版本中的点版本之间发生更改。次要/主要版本可能需要更高版本的JVM用于部分/全部受支持的操作系统。</li><li>其他软件：社区尝试维护Hadoop所需的其他软件的最低版本。例如，ssh，kerberos等。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以下是与该主题相关的一些相关JIRA和页面：</p><ul><li>该文件的演变 - <span class="exturl" data-url="aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQURPT1AtOTUxNw==" title="https://issues.apache.org/jira/browse/HADOOP-9517">HADOOP-9517<i class="fa fa-external-link"></i></span></li><li>hadoop-1.x和hadoop-2.x之间MapReduce最终用户应用程序的二进制兼容性 - <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50L2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50LWNvcmUvTWFwUmVkdWNlX0NvbXBhdGliaWxpdHlfSGFkb29wMV9IYWRvb3AyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">MapReduce hadoop-1.x和hadoop-2.x之间的兼容性<i class="fa fa-external-link"></i></span></li><li>根据接口分类计划的接口注释 - <span class="exturl" data-url="aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQURPT1AtNzM5MQ==" title="https://issues.apache.org/jira/browse/HADOOP-7391">HADOOP-7391 <i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9JbnRlcmZhY2VDbGFzc2lmaWNhdGlvbi5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/InterfaceClassification.html">Hadoop接口分类<i class="fa fa-external-link"></i></span></li><li>兼容Hadoop 1.x版本 - <span class="exturl" data-url="aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQURPT1AtNTA3MQ==" title="https://issues.apache.org/jira/browse/HADOOP-5071">HADOOP-5071<i class="fa fa-external-link"></i></span></li><li>在<span class="exturl" data-url="aHR0cDovL3dpa2kuYXBhY2hlLm9yZy9oYWRvb3AvUm9hZG1hcA==" title="http://wiki.apache.org/hadoop/Roadmap">Hadoop的路线图<i class="fa fa-external-link"></i></span>，捕捉其他发行政策页</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h2&gt;&lt;p&gt;本文档介绍了Apache Hadoop项目的兼容性目标。枚举了影响Hadoop开发人员，下游项目和最终用户的Hadoop版本之间的不同类型的
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>FileSystem Shell</title>
    <link href="https://tangguangen.com/2018/11/25/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FShell/"/>
    <id>https://tangguangen.com/2018/11/25/文件系统Shell/</id>
    <published>2018-11-25T11:39:27.000Z</published>
    <updated>2018-11-26T12:08:58.689Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>文件系统（FS）shell包括各种类似shell的命令，这些命令直接与Hadoop分布式文件系统（HDFS）以及Hadoop支持的其他文件系统交互，例如本地FS，HFTP FS，S3 FS等。FS shell由以下方式调用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hadoop fs &lt;args&gt;</span><br></pre></td></tr></table></figure><p>所有FS shell命令都将路径URI作为参数。URI格式为<code>scheme：// authority / path</code>。对于HDFS，方案是<code>hdfs</code>，对于本地FS，方案是<code>文件</code>。该计划和权限是可选的。如果未指定，则使用配置中指定的默认方案。可以将HDFS文件或目录（例如/ parent / child）指定为<code>hdfs：// namenodehost / parent / child</code>或简单地指定为<code>/ parent / child</code>（假设您的配置设置为指向<code>hdfs：// namenodehost</code>）。</p><p>FS shell中的大多数命令都表现得像对应的Unix命令。使用每个命令描述差异。错误信息发送到stderr，输出发送到stdout。</p><p>如果正在使用HDFS，则<code>hdfs dfs</code>是同义词。</p><p>可以使用相对路径。对于HDFS，当前工作目录是HDFS主目录<code>/ user / &lt;username&gt;</code>，通常必须手动创建。也可以隐式访问HDFS主目录，例如，当使用HDFS垃圾文件夹时，主目录中的<code>.Trash</code>目录。</p><p>有关通用shell选项，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9Db21tYW5kc01hbnVhbC5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CommandsManual.html">命令手册<i class="fa fa-external-link"></i></span>。</p><h2 id="appendToFile"><a href="#appendToFile" class="headerlink" title="appendToFile"></a>appendToFile</h2><p>用法：<code>hadoop fs -appendToFile &lt;localsrc&gt; ... &lt;dst&gt;</code></p><p>将单个src或多个srcs从本地文件系统附加到目标文件系统。还从stdin读取输入并附加到目标文件系统。</p><ul><li><code>hadoop fs -appendToFile localfile /user/hadoop/hadoopfile</code></li><li><code>hadoop fs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile</code></li><li><code>hadoop fs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile</code></li><li><code>hadoop fs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile</code> Reads the input from stdin.</li></ul><p>退出代码：</p><p>成功时返回0，错误时返回1。</p><h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h2><p>用法：<code>hadoop fs -cat [-ignoreCrc] URI [URI ...]</code></p><p>将源路径复制到stdout。</p><p>选项</p><ul><li>该<code>-ignoreCrc</code>选项禁用checkshum验证。</li></ul><p>例：</p><ul><li><code>hadoop fs -cat hdfs：//nn1.example.com/file1 hdfs：//nn2.example.com/file2</code></li><li><code>hadoop fs -cat file：/// file3 / user / hadoop / file4</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="checksum"><a href="#checksum" class="headerlink" title="checksum"></a>checksum</h2><p>用法：<code>hadoop fs -checksum URI</code></p><p>返回文件的校验和信息。</p><p>例：</p><ul><li><code>hadoop fs -checksum hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -checksum file：/// etc / hosts</code></li></ul><h2 id="chgrp"><a href="#chgrp" class="headerlink" title="chgrp"></a>chgrp</h2><p>用法：<code>hadoop fs -chgrp [-R] GROUP URI [URI ...]</code></p><p>更改文件的组关联。用户必须是文件的所有者，否则必须是超级用户。其他信息在“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1Blcm1pc3Npb25zR3VpZGUuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">权限指南”中<i class="fa fa-external-link"></i></span>。</p><p>选项</p><ul><li>-R选项将通过目录结构递归地进行更改。</li></ul><h2 id="CHMOD"><a href="#CHMOD" class="headerlink" title="CHMOD"></a>CHMOD</h2><p>用法：<code>hadoop fs -chmod [-R] &lt;MODE [，MODE] ... | OCTALMODE&gt; URI [URI ...]</code></p><p>更改文件的权限。使用-R，通过目录结构递归更改。用户必须是文件的所有者，否则必须是超级用户。其他信息在“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1Blcm1pc3Npb25zR3VpZGUuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">权限指南”中<i class="fa fa-external-link"></i></span>。</p><p>选项</p><ul><li>-R选项将通过目录结构递归地进行更改。</li></ul><h2 id="CHOWN"><a href="#CHOWN" class="headerlink" title="CHOWN"></a>CHOWN</h2><p>用法：<code>hadoop fs -chown [-R] [OWNER] [：[GROUP]] URI [URI]</code></p><p>更改文件的所有者。用户必须是超级用户。其他信息在“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1Blcm1pc3Npb25zR3VpZGUuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">权限指南”中<i class="fa fa-external-link"></i></span>。</p><p>选项</p><ul><li>-R选项将通过目录结构递归地进行更改。</li></ul><h2 id="copyFromLocal"><a href="#copyFromLocal" class="headerlink" title="copyFromLocal"></a>copyFromLocal</h2><p>用法：<code>hadoop fs -copyFromLocal &lt;localsrc&gt; URI</code></p><p>与<code>fs -put</code>命令类似，但源仅限于本地文件引用。</p><p>选项：</p><ul><li><code>-p</code>：保留访问和修改时间，所有权和权限。（假设权限可以跨文件系统传播）</li><li><code>-f</code>：覆盖目标（如果已存在）。</li><li><code>-l</code>：允许DataNode懒惰地将文件持久保存到磁盘，强制复制因子为1.此标志将导致持久性降低。小心使用。</li><li><code>-d</code>：使用后缀<code>._COPYING_</code>跳过创建临时文件。</li></ul><h2 id="copyToLocal"><a href="#copyToLocal" class="headerlink" title="copyToLocal"></a>copyToLocal</h2><p>用法：<code>hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;</code></p><p>与get命令类似，但目标仅限于本地文件引用。</p><h2 id="count"><a href="#count" class="headerlink" title="count"></a>count</h2><p>用法：<code>hadoop fs -count [-q] [-h] [-v] [-x] [-t [&lt;存储类型&gt;]] [-u] &lt;路径&gt;</code></p><p>计算与指定文件模式匹配的路径下的目录，文件和字节数。获取配额和使用情况。-count的输出列为：DIR_COUNT，FILE_COUNT，CONTENT_SIZE，PATHNAME</p><p>-u和-q选项控制输出包含的列。-q表示显示配额，-u限制输出以仅显示配额和使用情况。</p><p>-count -q的输出列为：QUOTA，REMAINING_QUOTA，SPACE_QUOTA，REMAINING_SPACE_QUOTA，DIR_COUNT，FILE_COUNT，CONTENT_SIZE，PATHNAME</p><p>-count -u的输出列为：QUOTA，REMAINING_QUOTA，SPACE_QUOTA，REMAINING_SPACE_QUOTA，PATHNAME</p><p>-t选项显示每种存储类型的配额和使用情况。如果未给出-u或-q选项，则忽略-t选项。可以在-t选项中使用的可能参数列表（除参数“”之外不区分大小写）：“”，“all”，“ram_disk”，“ssd”，“disk”或“archive”。</p><p>-h选项以人类可读格式显示大小。</p><p>-v选项显示标题行。</p><p>-x选项从结果计算中排除快照。如果没有-x选项（默认），则始终从所有INode计算结果，包括给定路径下的所有快照。如果给出-u或-q选项，则忽略-x选项。</p><p>例：</p><ul><li><code>hadoop fs -count hdfs：//nn1.example.com/file1 hdfs：//nn2.example.com/file2</code></li><li><code>hadoop fs -count -q hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -q -h hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -q -h -v hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -u hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -u -h hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -u -h -v hdfs：//nn1.example.com/file1</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1</p><h2 id="CP"><a href="#CP" class="headerlink" title="CP"></a>CP</h2><p>用法：<code>hadoop fs -cp [-f] [-p | -p [topax]] URI [URI ...] &lt;dest&gt;</code></p><p>将文件从源复制到目标。此命令也允许多个源，在这种情况下，目标必须是目录。</p><p>如果（1）源文件系统和目标文件系统支持它们（仅限HDFS），并且（2）所有源和目标路径名都在/.reserved/raw层次结构中，则保留’raw。<em>‘命名空间扩展属性。是否保留raw。</em> namespace xattrs的确定与-p（保留）标志无关。</p><p>选项：</p><ul><li>如果目标已存在，则-f选项将覆盖目标。</li><li>-p选项将保留文件属性[topx]（时间戳，所有权，权限，ACL，XAttr）。如果指定了-p而没有<em>arg</em>，则保留时间戳，所有权和权限。如果指定了-pa，则还保留权限，因为ACL是一组超级权限。确定是否保留原始命名空间扩展属性与-p标志无关。</li></ul><p>例：</p><ul><li><code>hadoop fs -cp / user / hadoop / file1 / user / hadoop / file2</code></li><li><code>hadoop fs -cp / user / hadoop / file1 / user / hadoop / file2 / user / hadoop / dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="createSnapshot"><a href="#createSnapshot" class="headerlink" title="createSnapshot"></a>createSnapshot</h2><p>请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1NuYXBzaG90cy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">HDFS快照指南”<i class="fa fa-external-link"></i></span>。</p><h2 id="deleteSnapshot"><a href="#deleteSnapshot" class="headerlink" title="deleteSnapshot"></a>deleteSnapshot</h2><p>请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1NuYXBzaG90cy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">HDFS快照指南”<i class="fa fa-external-link"></i></span>。</p><h2 id="df"><a href="#df" class="headerlink" title="df"></a>df</h2><p>用法：<code>hadoop fs -df [-h] URI [URI ...]</code></p><p>显示可用空间。</p><p>选项：</p><ul><li>-h选项将以“人类可读”的方式格式化文件大小（例如64.0m而不是67108864）</li></ul><p>例：</p><ul><li><code>hadoop dfs -df / user / hadoop / dir1</code></li></ul><h2 id="du"><a href="#du" class="headerlink" title="du"></a>du</h2><p>用法：<code>hadoop fs -du [-s] [-h] [-x] URI [URI ...]</code></p><p>显示给定目录中包含的文件和目录的大小或文件的长度，以防它只是一个文件。</p><p>选项：</p><ul><li>-s选项将导致显示文件长度的汇总摘要，而不是单个文件。如果没有-s选项，则通过从给定路径向上移动1级来完成计算。</li><li>-h选项将以“人类可读”的方式格式化文件大小（例如64.0m而不是67108864）</li><li>-x选项将从结果计算中排除快照。如果没有-x选项（默认），则始终从所有INode计算结果，包括给定路径下的所有快照。</li></ul><p>du返回三列，格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">size disk_space_consumed_with_all_replicas full_path_name</span><br></pre></td></tr></table></figure><p>例：</p><ul><li><code>hadoop fs -du / user / hadoop / dir1 / user / hadoop / file1 hdfs：//nn.example.com/user/hadoop/dir1</code></li></ul><p>退出代码：成功时返回0，错误时返回-1。</p><h2 id="dus"><a href="#dus" class="headerlink" title="dus"></a>dus</h2><p>用法：<code>hadoop fs -dus &lt;args&gt;</code></p><p>显示文件长度的摘要。</p><p><strong>注意：</strong>不推荐使用此命令。而是使用<code>hadoop fs -du -s</code>。</p><h2 id="expunge"><a href="#expunge" class="headerlink" title="expunge"></a>expunge</h2><p>用法：<code>hadoop fs -expunge</code></p><p>从trash目录中永久删除早于保留阈值的检查点中的文件，并创建新的检查点。</p><p>创建检查点时，垃圾箱中最近删除的文件将移动到检查点下。早于<code>fs.trash.interval的</code>检查点中的文件将在下次调用<code>-expunge</code>命令时被永久删除。</p><p>如果文件系统支持该功能，则用户可以配置为通过存储为<code>fs.trash.checkpoint.interval</code>的参数（在core-site.xml中）定期创建和删除检查点。该值应小于或等于<code>fs.trash.interval</code>。</p><p>有关<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc0Rlc2lnbi5odG1sI0ZpbGVfRGVsZXRlc19hbmRfVW5kZWxldGVz" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes">HDFS<i class="fa fa-external-link"></i></span>垃圾功能的更多信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc0Rlc2lnbi5odG1sI0ZpbGVfRGVsZXRlc19hbmRfVW5kZWxldGVz" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes">HDFS体系结构指南<i class="fa fa-external-link"></i></span>。</p><h2 id="find"><a href="#find" class="headerlink" title="find"></a>find</h2><p>用法：<code>hadoop fs -find &lt;path&gt; ... &lt;expression&gt; ...</code></p><p>查找与指定表达式匹配的所有文件，并将选定的操作应用于它们。如果未指定<em>路径</em>，则默认为当前工作目录。如果未指定表达式，则默认为-print。</p><p>识别以下主要表达式：</p><ul><li><p>-name pattern<br>-iname pattern</p><p>如果文件的基名与使用标准文件系统通配符的模式匹配，则求值为true。如果使用-iname，则匹配不区分大小写。</p></li><li><p>-print<br>-print0</p><p>始终评估为true。导致将当前路径名写入标准输出。如果使用-print0表达式，则附加ASCII NULL字符。</p></li></ul><p>识别以下运算符：</p><ul><li><p>表达式-a表达式<br>表达式和表达式<br>表达式表达式</p><p>用于连接两个表达式的逻辑AND运算符。如果两个子表达式都返回true，则返回true。由两个表达式的并置所暗示，因此不需要明确指定。如果第一个表达式失败，则不会应用第二个表达式。</p></li></ul><p>例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -find / -name <span class="built_in">test</span> -<span class="built_in">print</span></span><br></pre></td></tr></table></figure><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="get"><a href="#get" class="headerlink" title="get"></a>get</h2><p>用法：<code>hadoop fs -get [-ignorecrc] [-crc] [-p] [-f] &lt;src&gt; &lt;localdst&gt;</code></p><p>将文件复制到本地文件系统。可以使用-ignorecrc选项复制CRC校验失败的文件。可以使用-crc选项复制文件和CRC。</p><p>例：</p><ul><li><code>hadoop fs -get / user / hadoop / file localfile</code></li><li><code>hadoop fs -get hdfs：//nn.example.com/user/hadoop/file localfile</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><p>选项：</p><ul><li><code>-p</code>：保留访问和修改时间，所有权和权限。（假设权限可以跨文件系统传播）</li><li><code>-f</code>：覆盖目标（如果已存在）。</li><li><code>-ignorecrc</code>：对下载的文件进行跳过CRC校验。</li><li><code>-crc</code>：为下载的文件写入CRC校验和。</li></ul><h2 id="getfacl"><a href="#getfacl" class="headerlink" title="getfacl"></a>getfacl</h2><p>用法：<code>hadoop fs -getfacl [-R] &lt;path&gt;</code></p><p>显示文件和目录的访问控制列表（ACL）。如果目录具有默认ACL，则getfacl还会显示默认ACL。</p><p>选项：</p><ul><li>-R：递归列出所有文件和目录的ACL。</li><li><em>path</em>：要列出的文件或目录。</li></ul><p>例子：</p><ul><li><code>hadoop fs -getfacl / file</code></li><li><code>hadoop fs -getfacl -R / dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="getfattr"><a href="#getfattr" class="headerlink" title="getfattr"></a>getfattr</h2><p>用法：<code>hadoop fs -getfattr [-R] -n name | -d [-e en] &lt;path&gt;</code></p><p>显示文件或目录的扩展属性名称和值（如果有）。</p><p>选项：</p><ul><li>-R：递归列出所有文件和目录的属性。</li><li>-n name：转储指定的扩展属性值。</li><li>-d：转储与pathname关联的所有扩展属性值。</li><li>-e <em>encoding</em>：检索后<em>对代码</em>值进行编码。有效编码为“text”，“hex”和“base64”。编码为文本字符串的值用双引号（“）括起来，编码为十六进制和base64的值分别以0x和0为前缀。</li><li><em>path</em>：文件或目录。</li></ul><p>例子：</p><ul><li><code>hadoop fs -getfattr -d / file</code></li><li><code>hadoop fs -getfattr -R -n user.myAttr / dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="getmerge"><a href="#getmerge" class="headerlink" title="getmerge"></a>getmerge</h2><p>用法：<code>hadoop fs -getmerge [-nl] &lt;src&gt; &lt;localdst&gt;</code></p><p>将源目录和目标文件作为输入，并将src中的文件连接到目标本地文件。可选地，-nl可以设置为在每个文件的末尾添加换行符（LF）。-skip-empty-file可用于在空文件的情况下避免不需要的换行符。</p><p>例子：</p><ul><li><code>hadoop fs -getmerge -nl / src /opt/output.txt</code></li><li><code>hadoop fs -getmerge -nl /src/file1.txt /src/file2.txt /output.txt</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="help"><a href="#help" class="headerlink" title="help"></a>help</h2><p>用法：<code>hadoop fs -help</code></p><p>返回使用输出。</p><h2 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h2><p>用法：<code>hadoop fs -ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] &lt;args&gt;</code></p><p>选项：</p><ul><li>-C：仅显示文件和目录的路径。</li><li>-d：目录列为纯文件。</li><li>-h：以人类可读的方式格式化文件大小（例如64.0m而不是67108864）。</li><li>-q：打印？而不是不可打印的字符。</li><li>-R：递归列出遇到的子目录。</li><li>-t：按修改时间排序输出（最近的第一个）。</li><li>-S：按文件大小排序输出。</li><li>-r：反转排序顺序。</li><li>-u：使用访问时间而不是修改时间进行显示和排序。</li></ul><p>对于文件，ls使用以下格式返回文件的stat：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permissions number_of_replicas userid groupid filesize modification_date modification_time filename</span><br></pre></td></tr></table></figure><p>对于目录，它返回其直接子节点的列表，如在Unix中。目录列为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permissions userid groupid modification_date modification_time dirname</span><br></pre></td></tr></table></figure><p>默认情况下，目录中的文件按文件名排序。</p><p>例：</p><ul><li><code>hadoop fs -ls / user / hadoop / file1</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="lsr"><a href="#lsr" class="headerlink" title="lsr"></a>lsr</h2><p>用法：<code>hadoop fs -lsr &lt;args&gt;</code></p><p>ls的递归版本。</p><p><strong>注意：</strong>不推荐使用此命令。而是使用<code>hadoop fs -ls -R</code></p><h2 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h2><p>用法：<code>hadoop fs -mkdir [-p] &lt;paths&gt;</code></p><p>将路径uri作为参数并创建目录。</p><p>选项：</p><ul><li>-p选项行为与Unix mkdir -p非常相似，沿路径创建父目录。</li></ul><p>例：</p><ul><li><code>hadoop fs -mkdir / user / hadoop / dir1 / user / hadoop / dir2</code></li><li><code>hadoop fs -mkdir hdfs：//nn1.example.com/user/hadoop/dir hdfs：//nn2.example.com/user/hadoop/dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1</p><h2 id="moveFromLocal"><a href="#moveFromLocal" class="headerlink" title="moveFromLocal"></a>moveFromLocal</h2><p>用法：<code>hadoop fs -moveFromLocal &lt;localsrc&gt; &lt;dst&gt;</code></p><p>与put命令类似，只是在复制后删除了源localsrc。</p><h2 id="moveToLocal"><a href="#moveToLocal" class="headerlink" title="moveToLocal"></a>moveToLocal</h2><p>用法：<code>hadoop fs -moveToLocal [-crc] &lt;src&gt; &lt;dst&gt;</code></p><p>显示“尚未实现”消息。</p><h2 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h2><p>用法：<code>hadoop fs -mv URI [URI ...] &lt;dest&gt;</code></p><p>将文件从源移动到目标。此命令也允许多个源，在这种情况下，目标需要是目录。不允许跨文件系统移动文件。</p><p>例：</p><ul><li><code>hadoop fs -mv / user / hadoop / file1 / user / hadoop / file2</code></li><li><code>hadoop fs -mv hdfs：//nn.example.com/file1 hdfs：//nn.example.com/file2 hdfs：//nn.example.com/file3 hdfs：//nn.example.com/dir1</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="put"><a href="#put" class="headerlink" title="put"></a>put</h2><p>用法：<code>hadoop fs -put [-f] [-p] [-l] [-d] [ - | &lt;localsrc1&gt; ..]。&lt;DST&gt;</code></p><p>将单个src或多个srcs从本地文件系统复制到目标文件系统。如果源设置为“ - ”，还从stdin读取输入并写入目标文件系统</p><p>如果文件已存在，则复制失败，除非给出-f标志。</p><p>选项：</p><ul><li><code>-p</code>：保留访问和修改时间，所有权和权限。（假设权限可以跨文件系统传播）</li><li><code>-f</code>：覆盖目标（如果已存在）。</li><li><code>-l</code>：允许DataNode懒惰地将文件持久保存到磁盘，强制复制因子为1.此标志将导致持久性降低。小心使用。</li><li><code>-d</code>：使用后缀<code>._COPYING_</code>跳过创建临时文件。</li></ul><p>例子：</p><ul><li><code>hadoop fs -put localfile / user / hadoop / hadoopfile</code></li><li><code>hadoop fs -put -f localfile1 localfile2 / user / hadoop / hadoopdir</code></li><li><code>hadoop fs -put -d localfile hdfs：//nn.example.com/hadoop/hadoopfile</code></li><li><code>hadoop fs -put - hdfs：//nn.example.com/hadoop/hadoopfile</code>从stdin读取输入。</li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="renameSnapshot"><a href="#renameSnapshot" class="headerlink" title="renameSnapshot"></a>renameSnapshot</h2><p>请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1NuYXBzaG90cy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">HDFS快照指南”<i class="fa fa-external-link"></i></span>。</p><h2 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h2><p>用法：<code>hadoop fs -rm [-f] [-r | -R] [-skipTrash] [-safely] URI [URI ...]</code></p><p>删除指定为args的文件。</p><p>如果启用了垃圾箱，则文件系统会将已删除的文件移动到垃圾箱目录（由<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2FwaS9vcmcvYXBhY2hlL2hhZG9vcC9mcy9GaWxlU3lzdGVtLmh0bWw=" title="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html">FileSystem＃getTrashRoot提供<i class="fa fa-external-link"></i></span>）。</p><p>目前，默认情况下禁用垃圾箱功能。用户可以通过为参数<code>fs.trash.interval</code>（在core-site.xml中）设置大于零的值来启用垃圾。</p><p>请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9GaWxlU3lzdGVtU2hlbGwuaHRtbCNleHB1bmdl" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge">删除<i class="fa fa-external-link"></i></span>有关删除垃圾箱中文件的信息。</p><p>选项：</p><ul><li>如果文件不存在，-f选项将不显示诊断消息或修改退出状态以反映错误。</li><li>-R选项以递归方式删除目录及其下的任何内容。</li><li>-r选项等效于-R。</li><li>-skipTrash选项将绕过垃圾桶（如果已启用），并立即删除指定的文件。当需要从超配额目录中删除文件时，这非常有用。</li><li>-safely选项在删除目录之前需要安全确认，文件总数大于<code>hadoop.shell.delete.limit.num.files</code>（在core-site.xml中，默认值：100）。它可以与-skipTrash一起使用，以防止意外删除大目录。当递归地遍历大目录以计算在确认之前要删除的文件的数量时，预期延迟。</li></ul><p>例：</p><ul><li><code>hadoop fs -rm hdfs：//nn.example.com/file / user / hadoop / emptydir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir"></a>rmdir</h2><p>用法：<code>hadoop fs -rmdir [--ignore-fail-on-non-empty] URI [URI ...]</code></p><p>删除目录。</p><p>选项：</p><ul><li><code>--ignore-fail-on-non-empty</code>：使用通配符时，如果目录仍包含文件，请不要失败。</li></ul><p>例：</p><ul><li><code>hadoop fs -rmdir / user / hadoop / emptydir</code></li></ul><h2 id="rmr"><a href="#rmr" class="headerlink" title="rmr"></a>rmr</h2><p>用法：<code>hadoop fs -rmr [-skipTrash] URI [URI ...]</code></p><p>删除的递归版本。</p><p><strong>注意：</strong>不推荐使用此命令。而是使用<code>hadoop fs -rm -r</code></p><h2 id="setfacl"><a href="#setfacl" class="headerlink" title="setfacl"></a>setfacl</h2><p>用法：<code>hadoop fs -setfacl [-R] [-b | -k -m | -x &lt;acl_spec&gt; &lt;path&gt;] | [ - set &lt;acl_spec&gt; &lt;path&gt;]</code></p><p>设置文件和目录的访问控制列表（ACL）。</p><p>选项：</p><ul><li>-b：删除除基本ACL条目之外的所有条目。保留用户，组和其他条目以与权限位兼容。</li><li>-k：删除默认ACL。</li><li>-R：递归地对所有文件和目录应用操作。</li><li>-m：修改ACL。新条目将添加到ACL，并保留现有条目。</li><li>-x：删除指定的ACL条目。保留其他ACL条目。</li><li><code>--set</code>：完全替换ACL，丢弃所有现有条目。所述<em>acl_spec</em>必须包括用户，组条目和其他用于与权限位兼容性。</li><li><em>acl_spec</em>：以逗号分隔的ACL条目列表。</li><li><em>path</em>：要修改的文件或目录。</li></ul><p>例子：</p><ul><li><code>hadoop fs -setfacl -m user：hadoop：rw- / file</code></li><li><code>hadoop fs -setfacl -x user：hadoop / file</code></li><li><code>hadoop fs -setfacl -b / file</code></li><li><code>hadoop fs -setfacl -k / dir</code></li><li><code>hadoop fs -setfacl --set user :: rw-，user：hadoop：rw-，group :: r - ，other :: r-- / file</code></li><li><code>hadoop fs -setfacl -R -m user：hadoop：rx / dir</code></li><li><code>hadoop fs -setfacl -m default：user：hadoop：rx / dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="setfattr"><a href="#setfattr" class="headerlink" title="setfattr"></a>setfattr</h2><p>用法：<code>hadoop fs -setfattr -n name [-v value] | -x name &lt;path&gt;</code></p><p>设置文件或目录的扩展属性名称和值。</p><p>选项：</p><ul><li>-n name：扩展属性名称。</li><li>-v value：扩展属性值。该值有三种不同的编码方法。如果参数用双引号括起来，那么值就是引号内的字符串。如果参数的前缀为0x或0X，则将其视为十六进制数。如果参数以0或0S开头，则将其视为base64编码。</li><li>-x name：删除扩展属性。</li><li><em>path</em>：文件或目录。</li></ul><p>例子：</p><ul><li><code>hadoop fs -setfattr -n user.myAttr -v myValue / file</code></li><li><code>hadoop fs -setfattr -n user.noValue / file</code></li><li><code>hadoop fs -setfattr -x user.myAttr / file</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="setrep"><a href="#setrep" class="headerlink" title="setrep"></a>setrep</h2><p>用法：<code>hadoop fs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt;</code></p><p>更改文件的复制因子。如果<em>path</em>是目录，则命令以递归方式更改以<em>path为</em>根的目录树下的所有文件的复制因子。</p><p>选项：</p><ul><li>-w标志请求命令等待复制完成。这可能需要很长时间。</li><li>接受-R标志是为了向后兼容。它没有效果。</li></ul><p>例：</p><ul><li><code>hadoop fs -setrep -w 3 / user / hadoop / dir1</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="stat"><a href="#stat" class="headerlink" title="stat"></a>stat</h2><p>用法：<code>hadoop fs -stat [格式] &lt;路径&gt; ...</code></p><p>以指定格式打印有关<path></path>的文件/目录的统计信息。格式接受八进制（％a）和符号（％A），文件大小（字节）（％b），类型（％F），所有者组名（％g），名称（％n），块大小（％o）的权限），复制（％r），所有者的用户名（％u），访问日期（％x，％X）和修改日期（％y，％Y）。％x和％y将UTC日期显示为“yyyy-MM-dd HH：mm：ss”，％X和％Y显示自1970年1月1日UTC以来的毫秒数。如果未指定格式，则默认使用％y。</p><p>例：</p><ul><li><code>hadoop fs -stat“type：％F perm：％a％u：％g size：％b mtime：％y atime：％x name：％n”/ file</code></li></ul><p>退出代码：成功时返回0，错误时返回-1。</p><h2 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h2><p>用法：<code>hadoop fs -tail [-f] URI</code></p><p>显示文件的最后一千字节到stdout。</p><p>选项：</p><ul><li>-f选项将在文件增长时输出附加数据，如在Unix中一样。</li></ul><p>例：</p><ul><li><code>hadoop fs -tail路径名</code></li></ul><p>退出代码：成功时返回0，错误时返回-1。</p><h2 id="test"><a href="#test" class="headerlink" title="test"></a>test</h2><p>用法：<code>hadoop fs -test - [defsz] URI</code></p><p>选项：</p><ul><li>-d：f路径是目录，返回0。</li><li>-e：如果路径存在，则返回0。</li><li>-f：如果路径是文件，则返回0。</li><li>-s：如果路径不为空，则返回0。</li><li>-r：如果路径存在且授予读权限，则返回0。</li><li>-w：如果路径存在且授予写入权限，则返回0。</li><li>-z：如果文件长度为零，则返回0。</li></ul><p>例：</p><ul><li><code>hadoop fs -test -e filename</code></li></ul><h2 id="text"><a href="#text" class="headerlink" title="text"></a>text</h2><p>用法：<code>hadoop fs -text &lt;src&gt;</code></p><p>获取源文件并以文本格式输出文件。允许的格式是zip和TextRecordInputStream。</p><h2 id="touchz"><a href="#touchz" class="headerlink" title="touchz"></a>touchz</h2><p>用法：<code>hadoop fs -touchz URI [URI ...]</code></p><p>创建一个零长度的文件。如果文件存在非零长度，则返回错误。</p><p>例：</p><ul><li><code>hadoop fs -touchz pathname</code></li></ul><p>退出代码：成功时返回0，错误时返回-1。</p><h2 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h2><p>用法：<code>hadoop fs -truncate [-w] &lt;length&gt; &lt;paths&gt;</code></p><p>将与指定文件模式匹配的所有文件截断为指定的长度。</p><p>选项：</p><ul><li>该<code>-w</code>标志的要求，对块恢复命令如有必要，等待完成。如果没有-w标志，则在恢复过程中文件可能会保持一段时间不闭合。在此期间，无法重新打开文件以进行追加。</li></ul><p>例：</p><ul><li><code>hadoop fs -truncate 55 / user / hadoop / file1 / user / hadoop / file2</code></li><li><code>hadoop fs -truncate -w 127 hdfs：//nn1.example.com/user/hadoop/file1</code></li></ul><h3 id="usage"><a href="#usage" class="headerlink" title="usage"></a>usage</h3><p>用法：<code>hadoop fs -usage命令</code></p><p>返回单个命令的帮助。</p><h2 id="使用对象存储"><a href="#使用对象存储" class="headerlink" title="使用对象存储"></a>使用对象存储</h2><p>Hadoop FileSystem shell可与Object Stores（如Amazon S3，Azure WASB和OpenStack Swift）配合使用。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">＃创建一个目录</span><br><span class="line">hadoop fs -mkdir s3a：// bucket / datasets /</span><br><span class="line"></span><br><span class="line">＃从群集文件系统上传文件</span><br><span class="line">hadoop fs -put /datasets/example.orc s3a：// bucket / datasets /</span><br><span class="line"></span><br><span class="line">＃触摸文件</span><br><span class="line">hadoop fs -touchz wasb：//yourcontainer@youraccount.blob.core.windows.net/touched</span><br></pre></td></tr></table></figure><p>与普通文件系统不同，重命名对象存储中的文件和目录通常需要与被操作对象的大小成比例的时间。由于许多文件系统shell操作使用重命名作为操作的最后阶段，因此跳过该阶段可以避免长时间的延迟。</p><p>特别是，<code>put</code>和<code>copyFromLocal</code>命令都应该为直接上载设置<code>-d</code>选项。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Upload a file from the cluster filesystem</span></span><br><span class="line">hadoop fs -put -d /datasets/example.orc s3a://bucket/datasets/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Upload a file from under the user's home directory in the local filesystem.</span></span><br><span class="line"><span class="comment"># Note it is the shell expanding the "~", not the hadoop fs command</span></span><br><span class="line">hadoop fs -copyFromLocal -d -f ~/datasets/devices.orc s3a://bucket/datasets/</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a file from stdin</span></span><br><span class="line"><span class="comment"># the special "-" source means "use stdin"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello"</span> | hadoop fs -put -d -f - wasb://yourcontainer@youraccount.blob.core.windows.net/hello.txt</span><br></pre></td></tr></table></figure><p>可以下载和查看对象：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">＃将目录复制到本地文件系统</span><br><span class="line">hadoop fs -copyToLocal s3a：// bucket / datasets /</span><br><span class="line"></span><br><span class="line">＃将文件从对象库复制到集群文件系统。</span><br><span class="line">hadoop fs -get wasb：//yourcontainer@youraccount.blob.core.windows.net/hello.txt / examples</span><br><span class="line"></span><br><span class="line">＃打印对象</span><br><span class="line">hadoop fs -cat wasb：//yourcontainer@youraccount.blob.core.windows.net/hello.txt</span><br><span class="line"></span><br><span class="line">＃打印对象，必要时解压缩</span><br><span class="line">hadoop fs -text wasb：//yourcontainer@youraccount.blob.core.windows.net/hello.txt</span><br><span class="line"></span><br><span class="line"><span class="comment">##将日志文件下载到本地文件中</span></span><br><span class="line">hadoop fs -getmerge wasb：//yourcontainer@youraccount.blob.core.windows.net/logs \ * log.txt</span><br></pre></td></tr></table></figure><p>列出许多文件的命令往往比使用HDFS或其他文件系统时慢得多</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -count s3a://bucket/</span><br><span class="line">hadoop fs -du s3a://bucket/</span><br></pre></td></tr></table></figure><p>其他慢速命令包括<code>find</code>，<code>mv</code>，<code>cp</code>和<code>rm</code>。</p><h2 id="Find"><a href="#Find" class="headerlink" title="Find"></a><strong>Find</strong></h2><p>在提供路径下有许多目录的大型商店中，这可能会非常慢。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># enumerate all files in the object store's container.</span></span><br><span class="line">hadoop fs -find s3a://bucket/ -<span class="built_in">print</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># remember to escape the wildcards to stop the shell trying to expand them first</span></span><br><span class="line">hadoop fs -find s3a://bucket/datasets/ -name \*.txt -<span class="built_in">print</span></span><br></pre></td></tr></table></figure><h3 id="Rename"><a href="#Rename" class="headerlink" title="Rename"></a><strong>Rename</strong></h3><p>重命名文件的时间取决于其大小。</p><p>重命名目录的时间取决于该目录下所有文件的数量和大小。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv s3a：// bucket / datasets s3a：// bucket / historical</span><br></pre></td></tr></table></figure><p>如果操作中断，则对象存储将处于未定义状态。</p><h3 id="Copy"><a href="#Copy" class="headerlink" title="Copy"></a><strong>Copy</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp s3a://bucket/datasets s3a://bucket/historical</span><br></pre></td></tr></table></figure><p>复制操作读取每个文件，然后将其写回对象存储区; 完成的时间取决于要复制的数据量，以及本地计算机和对象存储库之间双向带宽。</p><p><strong>计算机离对象存储器越远，复制所用的时间越长</strong></p><h2 id="删除对象"><a href="#删除对象" class="headerlink" title="删除对象"></a>删除对象</h2><p>该<code>RM</code>命令删除对象和目录满对象。如果对象存储<em>最终</em>是<em>一致的</em>，则<code>fs ls</code>命令和其他访问器可能会暂时返回现在删除的对象的详细信息; 这是对象存储的工件，无法避免。</p><p>如果文件系统客户端配置为将文件复制到废纸篓目录，则该文件系统将位于存储桶中; 然后，<code>rm</code>操作将花费与数据大小成比例的时间。此外，删除的文件将继续产生存储成本。</p><p>要避免这种情况，请使用<code>-skipTrash</code>选项。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm -skipTrash s3a：// bucket / dataset</span><br></pre></td></tr></table></figure><p>可以使用<code>expunge</code>命令清除移动到<code>.Trash</code>目录的数据。由于此命令仅适用于默认文件系统，因此必须将其配置为使默认文件系统成为目标对象库。<code></code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -expunge -D fs.defaultFS = s3a：// bucket /</span><br></pre></td></tr></table></figure><h2 id="覆盖对象"><a href="#覆盖对象" class="headerlink" title="覆盖对象"></a>覆盖对象</h2><p>如果对象存储<em>最终</em>是<em>一致的</em>，则任何覆盖现有对象的操作可能不会立即对所有客户端/查询可见。即：稍后查询相同对象的状态或内容的操作可以获得前一个对象。在读取单个对象时，这有时可以在同一客户端中显示。</p><p>避免使用一系列覆盖对象的命令，然后立即处理更新的数据; 存在以下风险：将使用先前的数据。</p><h2 id="时间戳"><a href="#时间戳" class="headerlink" title="时间戳"></a>时间戳</h2><p>对象存储中对象和目录的时间戳可能不遵循HDFS中文件和目录的行为。</p><ol><li>对象的创建和初始修改时间将是它在对象库上创建的时间; 这将是写入过程的结束，而不是开始。</li><li>时间戳将取自对象存储基础架构的时钟，而不是客户端的时钟。</li><li>如果覆盖了对象，则将更新修改时间。</li><li>目录可能有也可能没有有效的时间戳。当更新下面的对象时，他们不太可能更新修改时间。</li><li>该<code>atime的</code>访问时间特征不被任何在Apache Hadoop的代码库中找到的对象存储的支持。</li></ol><p>有关这可能如何影响<code>distcp -update</code>操作的详细信息，请参阅<code>DistCp</code>文档。</p><h2 id="安全模型和操作"><a href="#安全模型和操作" class="headerlink" title="安全模型和操作"></a>安全模型和操作</h2><p>对象存储的安全性和权限模型通常与Unix风格的文件系统非常不同; 查询或操纵权限的操作通常不受支持。</p><p>适用的操作包括：<code>chgrp</code>，<code>chmod</code>，<code>chown</code>，<code>getfacl</code>和<code>setfacl</code>。相关属性命令<code>getfattr</code>和<code>setfattr</code>通常也不可用。</p><ul><li>列出权限和用户/组详细信息的文件系统命令通常模拟这些详细信息。</li><li>尝试保留权限的操作（例如<code>fs -put -p</code>）不会因此原因保留权限。（特例：<code>wasb：//</code>，它保留权限但不强制执行）。</li></ul><p>当与只读对象存储交互时，“list”和“stat”命令中的权限可以指示用户具有写访问权限，而实际上他们没有。</p><p>对象存储通常具有自己的权限模型，模型可以通过特定于商店的工具进行操作。请注意，对象存储可能提供的某些权限（例如只写路径或根路径上的不同权限）可能与Hadoop文件系统客户端不兼容。这些往往需要对它们写入数据的整个对象存储桶/容器进行完全读写访问。</p><p>作为如何模拟权限的示例，这里是亚马逊的公共，只读桶Landsat图像的列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls s3a://landsat-pds/</span><br><span class="line">Found 10 items</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/L8</span><br><span class="line">-rw-rw-rw-   1 mapred      23764 2015-01-28 18:13 s3a://landsat-pds/index.html</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/landsat-pds_stats</span><br><span class="line">-rw-rw-rw-   1 mapred        105 2016-08-19 18:12 s3a://landsat-pds/robots.txt</span><br><span class="line">-rw-rw-rw-   1 mapred         38 2016-09-26 12:16 s3a://landsat-pds/run_info.json</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/runs</span><br><span class="line">-rw-rw-rw-   1 mapred   27458808 2016-09-26 12:16 s3a://landsat-pds/scene_list.gz</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/tarq</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/tarq_corrupt</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/<span class="built_in">test</span></span><br></pre></td></tr></table></figure><ol><li>所有文件都列为具有完全读/写权限。</li><li>所有目录似乎都具有完整的<code>rwx</code>权限。</li><li>所有文件的复制计数为“1”。</li><li>所有文件和目录的所有者被声明为当前用户（<code>mapred</code>）。</li><li>所有目录的时间戳实际上是执行<code>-ls</code>操作的时间戳。这是因为这些目录不是商店中的实际对象; 它们是基于路径下对象存在的模拟目录。</li></ol><p>当尝试删除其中一个文件时，操作失败 - 尽管<code>ls</code>命令显示的权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -rm s3a://landsat-pds/scene_list.gz</span><br><span class="line">rm: s3a://landsat-pds/scene_list.gz: delete on s3a://landsat-pds/scene_list.gz:</span><br><span class="line">  com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3;</span><br><span class="line">  Status Code: 403; Error Code: AccessDenied; Request ID: 1EF98D5957BCAB3D),</span><br><span class="line">  S3 Extended Request ID: wi3veOXFuFqWBUCJgV3Z+NQVj9gWgZVdXlPU4KBbYMsw/gA+hyhRXcaQ+PogOsDgHh31HlTCebQ=</span><br></pre></td></tr></table></figure><p>这表明列出的权限不能作为写访问的证据; 只有对象操作才能确定这一点。</p><p>请注意，Microsoft Azure WASB文件系统允许设置和检查权限，但实际上并未强制实施权限。此功能提供了使用DistCp备份HDFS目录树的功能，其权限得以保留，权限可在将目录复制回HDFS时恢复。但是，为了保护对对象存储中数据的访问，<span class="exturl" data-url="aHR0cHM6Ly9henVyZS5taWNyb3NvZnQuY29tL2VuLXVzL2RvY3VtZW50YXRpb24vYXJ0aWNsZXMvc3RvcmFnZS1zZWN1cml0eS1ndWlkZS8=" title="https://azure.microsoft.com/en-us/documentation/articles/storage-security-guide/">必须使用<i class="fa fa-external-link"></i></span> Azure <span class="exturl" data-url="aHR0cHM6Ly9henVyZS5taWNyb3NvZnQuY29tL2VuLXVzL2RvY3VtZW50YXRpb24vYXJ0aWNsZXMvc3RvcmFnZS1zZWN1cml0eS1ndWlkZS8=" title="https://azure.microsoft.com/en-us/documentation/articles/storage-security-guide/">自己的模型和工具<i class="fa fa-external-link"></i></span>。</p><h2 id="Commands-of-limited-value"><a href="#Commands-of-limited-value" class="headerlink" title="Commands of limited value"></a>Commands of limited value</h2><p>以下是通常没有效果的shell命令列表 - 实际上可能会失败。</p><table><thead><tr><th>command</th><th>limitations</th></tr></thead><tbody><tr><td><code>appendToFile</code></td><td>generally unsupported</td></tr><tr><td><code>checksum</code></td><td>the usual checksum is “NONE”</td></tr><tr><td><code>chgrp</code></td><td>generally unsupported permissions model; no-op</td></tr><tr><td><code>chmod</code></td><td>generally unsupported permissions model; no-op</td></tr><tr><td><code>chown</code></td><td>generally unsupported permissions model; no-op</td></tr><tr><td><code>createSnapshot</code></td><td>generally unsupported</td></tr><tr><td><code>deleteSnapshot</code></td><td>generally unsupported</td></tr><tr><td><code>df</code></td><td>default values are normally displayed</td></tr><tr><td><code>getfacl</code></td><td>may or may not be supported</td></tr><tr><td><code>getfattr</code></td><td>generally supported</td></tr><tr><td><code>renameSnapshot</code></td><td>generally unsupported</td></tr><tr><td><code>setfacl</code></td><td>generally unsupported permissions model</td></tr><tr><td><code>setfattr</code></td><td>generally unsupported permissions model</td></tr><tr><td><code>setrep</code></td><td>has no effect</td></tr><tr><td><code>truncate</code></td><td>generally unsupported</td></tr></tbody></table><p>不同的对象存储客户端<em>可能</em>支持这些命令：请查阅文档并针对目标存储进行测试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概览&quot;&gt;&lt;a href=&quot;#概览&quot; class=&quot;headerlink&quot; title=&quot;概览&quot;&gt;&lt;/a&gt;概览&lt;/h2&gt;&lt;p&gt;文件系统（FS）shell包括各种类似shell的命令，这些命令直接与Hadoop分布式文件系统（HDFS）以及Hadoop支持的其他文件系
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop集群设置</title>
    <link href="https://tangguangen.com/2018/11/24/Hadoop%E9%9B%86%E7%BE%A4%E8%AE%BE%E7%BD%AE/"/>
    <id>https://tangguangen.com/2018/11/24/Hadoop集群设置/</id>
    <published>2018-11-24T07:34:01.000Z</published>
    <updated>2018-11-26T12:08:36.507Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>本文档描述了如何安装和配置Hadoop集群，范围从几个节点到具有数千个节点的极大集群。要使用Hadoop，您可能首先要将其安装在一台计算机上（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>）。</p><p>本文档不包括<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TZWN1cmVNb2RlLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html">安全性<i class="fa fa-external-link"></i></span>或高可用性等高级主题。</p><h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><ul><li>安装Java。有关已知的好版本，请参阅<span class="exturl" data-url="aHR0cDovL3dpa2kuYXBhY2hlLm9yZy9oYWRvb3AvSGFkb29wSmF2YVZlcnNpb25z" title="http://wiki.apache.org/hadoop/HadoopJavaVersions">Hadoop Wiki<i class="fa fa-external-link"></i></span>。</li><li>从Apache镜像下载稳定版本的Hadoop。</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>安装Hadoop集群通常涉及在集群中的所有计算机上解压缩软件，或者通过适合您的操作系统的打包系统进行安装。将硬件划分为几个功能非常重要。</p><p>通常，群集中的<strong>一台计算机被指定为NameNode</strong>，而<strong>一台计算机则被指定为ResourceManager</strong>。这些都是<strong>masters</strong>。其他服务（例如Web App Proxy Server和MapReduce Job History server）通常在专用硬件或共享基础架构上运行，具体取决于负载。</p><p>集群中的<strong>其余计算机充当DataNode和NodeManager</strong>。这些是<strong>slaves</strong>。</p><h2 id="在非安全模式下配置Hadoop"><a href="#在非安全模式下配置Hadoop" class="headerlink" title="在非安全模式下配置Hadoop"></a>在非安全模式下配置Hadoop</h2><p>Hadoop的 Java 配置由两种类型的重要配置文件驱动：</p><ul><li><strong>只读默认配置</strong>  - <code>core-default.xml</code>, <code>hdfs-default.xml</code>, <code>yarn-default.xml</code> 和<code>mapred-default.xml</code>.</li><li><strong>具体站点的配置</strong>- <code>etc/hadoop/core-site.xml</code>, <code>etc/hadoop/hdfs-site.xml</code>, <code>etc/hadoop/yarn-site.xml</code> 和<code>etc/hadoop/mapred-site.xml</code>.</li></ul><p>此外，您可以通过<code>etc/hadoop/hadoop-env.sh</code>和<code>etc/hadoop/yarn-env.sh</code>设置特定于站点的值来控制分发的 bin/ 目录中的Hadoop脚本。</p><p>要配置Hadoop集群，您需要配置Hadoop后台进程执行的<code>environment</code>以及Hadoop后台进程的<code>configuration parameters</code>。</p><p><strong>HDFS</strong>后台进程是 <strong>NameNode</strong>，<strong>SecondaryNameNode</strong> 和 <strong>DataNode</strong>。<strong>YARN</strong>后台进程是<strong>ResourceManager</strong>，<strong>NodeManager</strong>和<strong>WebAppProxy</strong>。如果要使用MapReduce，则MapReduce Job History Server也将运行。对于大型安装，这些通常运行在不同的主机上。</p><h3 id="Hadoop的环境配置"><a href="#Hadoop的环境配置" class="headerlink" title="Hadoop的环境配置"></a>Hadoop的环境配置</h3><p>管理员应该使用<code>etc/hadoop/hadoop-env.sh</code>和<code>etc/hadoop/mapred-env.sh</code>以及<code>etc/hadoop/yarn-env.sh</code>脚本来对Hadoop环境进行特定于站点的自定义。</p><p>至少，您必须指定<code>JAVA_HOME，</code>以便在每个远程节点上正确定义它。</p><p>管理员可以使用下表中显示的配置选项配置各个后台进程：</p><table><thead><tr><th>Daemon</th><th>Environment Variable</th></tr></thead><tbody><tr><td>NameNode</td><td>HADOOP_NAMENODE_OPTS</td></tr><tr><td>DataNode</td><td>HADOOP_DATANODE_OPTS</td></tr><tr><td>Secondary NameNode</td><td>HADOOP_SECONDARYNAMENODE_OPTS</td></tr><tr><td>ResourceManager</td><td>YARN_RESOURCEMANAGER_OPTS</td></tr><tr><td>NodeManager</td><td>YARN_NODEMANAGER_OPTS</td></tr><tr><td>WebAppProxy</td><td>YARN_PROXYSERVER_OPTS</td></tr><tr><td>Map Reduce Job History Server</td><td>HADOOP_JOB_HISTORYSERVER_OPTS</td></tr></tbody></table><p>例如，要将NameNode配置为使用parallelGC，应在hadoop-env.sh中添加以下语句：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_NAMENODE_OPTS=<span class="string">"-XX:+UseParallelGC"</span></span><br></pre></td></tr></table></figure><p>有关其他示例，请参阅 etc/hadoop/hadoop-env.sh</p><p>您可以自定义的其他有用配置参数包括：</p><ul><li><code>HADOOP_PID_DIR</code> - 存储后台进程的进程标识文件的目录。</li><li><code>HADOOP_LOG_DIR</code> - 存储后台程序日志文件的目录。如果日志文件不存在，则会自动创建日志文件。</li><li><code>HADOOP_HEAPSIZE</code> / <code>YARN_HEAPSIZE</code> - 要使用的最大堆大小，以MB为单位，例如，如果varibale设置为1000，则堆将设置为1000MB。这用于配置守护程序的堆大小。默认情况下，该值为1000.如果要为每个可以使用的守护程序单独配置值。</li></ul><p>在大多数情况下，您应该指定<code>HADOOP_PID_DIR</code>和<code>HADOOP_LOG_DIR</code>目录，以便它们只能由将要运行hadoop守护程序的用户写入。否则就有可能发生符号链接攻击。</p><p>在系统范围的shell环境配置中配置<code>HADOOP_PREFIX</code>也是传统的。例如，在<code>/etc/profile.d中</code>有一个简单的脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_PREFIX=/path/to/hadoop</span><br><span class="line">export HADOOP_PREFIX</span><br></pre></td></tr></table></figure><table><thead><tr><th>Daemon</th><th>Environment Variable</th></tr></thead><tbody><tr><td>ResourceManager</td><td>YARN_RESOURCEMANAGER_HEAPSIZE</td></tr><tr><td>NodeManager</td><td>YARN_NODEMANAGER_HEAPSIZE</td></tr><tr><td>WebAppProxy</td><td>YARN_PROXYSERVER_HEAPSIZE</td></tr><tr><td>Map Reduce Job History Server</td><td>HADOOP_JOB_HISTORYSERVER_HEAPSIZE</td></tr></tbody></table><h3 id="配置Hadoop"><a href="#配置Hadoop" class="headerlink" title="配置Hadoop"></a>配置Hadoop</h3><p>本节介绍在给定配置文件中指定的重要参数：</p><ul><li><code>etc/hadoop/core-site.xml</code></li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>fs.defaultFS</code></td><td>NameNode URI</td><td><span class="exturl" data-url="aGRmczovL2hvc3Q6cG9ydC8=" title="hdfs://host:port/">hdfs://host:port/<i class="fa fa-external-link"></i></span></td></tr><tr><td><code>io.file.buffer.size</code></td><td>131072</td><td>SequenceFiles中使用的读/写缓冲区的大小。</td></tr></tbody></table><ul><li><code>etc/hadoop/hdfs-site.xml</code></li><li>NameNode的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>dfs.namenode.name.dir</code></td><td>NameNode存储名称空间和持续事务日志的本地文件系统上的路径。</td><td>如果这是一个以逗号分隔的目录列表，那么名称表将复制到所有目录中，以实现冗余。</td></tr><tr><td><code>dfs.hosts</code> / <code>dfs.hosts.exclude</code></td><td>List of permitted/excluded DataNodes.</td><td>If necessary, use these files to control the list of allowable datanodes.</td></tr><tr><td><code>dfs.blocksize</code></td><td>268435456</td><td>HDFS blocksize of 256MB for large file-systems.</td></tr><tr><td><code>dfs.namenode.handler.count</code></td><td>100</td><td>More NameNode server threads to handle RPCs from large number of DataNodes.</td></tr></tbody></table><ul><li>Configurations for DataNode:</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>dfs.datanode.data.dir</code></td><td>Comma separated list of paths on the local filesystem of a <code>DataNode</code> where it should store its blocks.</td><td>If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices.</td></tr></tbody></table><ul><li><code>etc/hadoop/yarn-site.xml</code></li><li>ResourceManager和NodeManager的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.acl.enable</code></td><td><code>true</code> / <code>false</code></td><td>Enable ACLs? Defaults to <em>false</em>.</td></tr><tr><td><code>yarn.admin.acl</code></td><td>Admin ACL</td><td>ACL to set admins on the cluster. ACLs are of for <em>comma-separated-usersspacecomma-separated-groups</em>. Defaults to special value of <strong>*</strong> which means <em>anyone</em>. Special value of just <em>space</em> means no one has access.</td></tr><tr><td><code>yarn.log-aggregation-enable</code></td><td><em>false</em></td><td>Configuration to enable or disable log aggregation</td></tr></tbody></table><ul><li>ResourceManager的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.resourcemanager.address</code></td><td><code>ResourceManager</code>host:port for clients to submit jobs.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.scheduler.address</code></td><td><code>ResourceManager</code>host:port for ApplicationMasters to talk to Scheduler to obtain resources.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.resource-tracker.address</code></td><td><code>ResourceManager</code>host:port for NodeManagers.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.admin.address</code></td><td><code>ResourceManager</code>host:port for administrative commands.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.webapp.address</code></td><td><code>ResourceManager</code>web-ui host:port.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.hostname</code></td><td><code>ResourceManager</code>host.</td><td><em>host</em> Single hostname that can be set in place of setting all <code>yarn.resourcemanager*address</code> resources. Results in default ports for ResourceManager components.</td></tr><tr><td><code>yarn.resourcemanager.scheduler.class</code></td><td><code>ResourceManager</code>Scheduler class.</td><td><code>CapacityScheduler</code> (recommended), <code>FairScheduler</code> (also recommended), or <code>FifoScheduler</code>. Use a fully qualified class name, e.g., <code>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</code>.</td></tr><tr><td><code>yarn.scheduler.minimum-allocation-mb</code></td><td>Minimum limit of memory to allocate to each container request at the <code>Resource Manager</code>.</td><td>In MBs</td></tr><tr><td><code>yarn.scheduler.maximum-allocation-mb</code></td><td>Maximum limit of memory to allocate to each container request at the <code>Resource Manager</code>.</td><td>In MBs</td></tr><tr><td><code>yarn.resourcemanager.nodes.include-path</code> / <code>yarn.resourcemanager.nodes.exclude-path</code></td><td>List of permitted/excluded NodeManagers.</td><td>If necessary, use these files to control the list of allowable NodeManagers.</td></tr></tbody></table><ul><li>NodeManager的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.nodemanager.resource.memory-mb</code></td><td>Resource i.e. available physical memory, in MB, for given <code>NodeManager</code></td><td>Defines total available resources on the <code>NodeManager</code> to be made available to running containers</td></tr><tr><td><code>yarn.nodemanager.vmem-pmem-ratio</code></td><td>Maximum ratio by which virtual memory usage of tasks may exceed physical memory</td><td>The virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio.</td></tr><tr><td><code>yarn.nodemanager.local-dirs</code></td><td>Comma-separated list of paths on the local filesystem where intermediate data is written.</td><td>Multiple paths help spread disk i/o.</td></tr><tr><td><code>yarn.nodemanager.log-dirs</code></td><td>Comma-separated list of paths on the local filesystem where logs are written.</td><td>Multiple paths help spread disk i/o.</td></tr><tr><td><code>yarn.nodemanager.log.retain-seconds</code></td><td><em>10800</em></td><td>Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled.</td></tr><tr><td><code>yarn.nodemanager.remote-app-log-dir</code></td><td><em>/logs</em></td><td>HDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled.</td></tr><tr><td><code>yarn.nodemanager.remote-app-log-dir-suffix</code></td><td><em>logs</em></td><td>Suffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled.</td></tr><tr><td><code>yarn.nodemanager.aux-services</code></td><td>mapreduce_shuffle</td><td>Shuffle service that needs to be set for Map Reduce applications.</td></tr></tbody></table><ul><li>Configurations for History Server (Needs to be moved elsewhere):</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.log-aggregation.retain-seconds</code></td><td><em>-1</em></td><td>How long to keep aggregation logs before deleting them. -1 disables. Be careful, set this too small and you will spam the name node.</td></tr><tr><td><code>yarn.log-aggregation.retain-check-interval-seconds</code></td><td><em>-1</em></td><td>Time between checks for aggregated log retention. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful, set this too small and you will spam the name node.</td></tr></tbody></table><ul><li><code>etc/hadoop/mapred-site.xml</code></li><li>MapReduce应用程序的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>mapreduce.framework.name</code></td><td>yarn</td><td>Execution framework set to Hadoop YARN.</td></tr><tr><td><code>mapreduce.map.memory.mb</code></td><td>1536</td><td>Larger resource limit for maps.</td></tr><tr><td><code>mapreduce.map.java.opts</code></td><td>-Xmx1024M</td><td>Larger heap-size for child jvms of maps.</td></tr><tr><td><code>mapreduce.reduce.memory.mb</code></td><td>3072</td><td>Larger resource limit for reduces.</td></tr><tr><td><code>mapreduce.reduce.java.opts</code></td><td>-Xmx2560M</td><td>Larger heap-size for child jvms of reduces.</td></tr><tr><td><code>mapreduce.task.io.sort.mb</code></td><td>512</td><td>Higher memory-limit while sorting data for efficiency.</td></tr><tr><td><code>mapreduce.task.io.sort.factor</code></td><td>100</td><td>More streams merged at once while sorting files.</td></tr><tr><td><code>mapreduce.reduce.shuffle.parallelcopies</code></td><td>50</td><td>Higher number of parallel copies run by reduces to fetch outputs from very large number of maps.</td></tr></tbody></table><ul><li>Configurations for MapReduce JobHistory Server:</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>mapreduce.jobhistory.address</code></td><td>MapReduce JobHistory Server <em>host:port</em></td><td>Default port is 10020.</td></tr><tr><td><code>mapreduce.jobhistory.webapp.address</code></td><td>MapReduce JobHistory Server Web UI <em>host:port</em></td><td>Default port is 19888.</td></tr><tr><td><code>mapreduce.jobhistory.intermediate-done-dir</code></td><td>/mr-history/tmp</td><td>Directory where history files are written by MapReduce jobs.</td></tr><tr><td><code>mapreduce.jobhistory.done-dir</code></td><td>/mr-history/done</td><td>Directory where history files are managed by the MR JobHistory Server.</td></tr></tbody></table><h2 id="监控NodeManager的健康状况"><a href="#监控NodeManager的健康状况" class="headerlink" title="监控NodeManager的健康状况"></a>监控NodeManager的健康状况</h2><p>Hadoop提供了一种机制，管理员可以通过该机制定期运行管理员提供的脚本以确定节点是否健康。</p><p>管理员可以通过在脚本中执行对其选择的任何检查来确定节点是否处于正常状态。如果脚本检测到节点处于不健康状态，则必须以字符串ERROR开头的标准输出行。NodeManager定期生成脚本并检查其输出。如果脚本的输出包含字符串ERROR，如上所述，节点的状态将报告为运行状况<code>不佳</code>并且ResourceManager将节点列入黑名单。不会为此节点分配其他任务。但是，NodeManager继续运行脚本，因此如果节点再次变得健康，它将自动从ResourceManager上的黑名单节点中删除。如果节点不健康，则可以在ResourceManager Web界面中为管理员提供节点的运行状况以及脚本的输出。自节点健康以来的时间也显示在Web界面上。</p><p>以下参数可用于控制<code>etc / hadoop / yarn-site.xml中</code>的节点运行状况监视脚本。</p><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.nodemanager.health-checker.script.path</code></td><td>Node health script</td><td>Script to check for node’s health status.</td></tr><tr><td><code>yarn.nodemanager.health-checker.script.opts</code></td><td>Node health script options</td><td>Options for script to check for node’s health status.</td></tr><tr><td><code>yarn.nodemanager.health-checker.interval-ms</code></td><td>Node health script interval</td><td>Time interval for running health script.</td></tr><tr><td><code>yarn.nodemanager.health-checker.script.timeout-ms</code></td><td>Node health script timeout interval</td><td>Timeout for health script execution.</td></tr></tbody></table><p>如果只有部分本地磁盘变坏，则运行状况检查程序脚本不应该给出错误。NodeManager能够定期检查本地磁盘的运行状况（具体检查nodemanager-local-dirs和nodemanager-log-dirs），并在达到配置属性yarn.nodemanager设置的错误目录数阈值后.disk-health-checker.min-healthy-disks，整个节点被标记为运行状况不佳，此信息也会发送给资源管理器。引导磁盘被突袭或健康检查程序脚本识别引导磁盘中的故障。</p><h2 id="Slaves-文件"><a href="#Slaves-文件" class="headerlink" title="Slaves 文件"></a>Slaves 文件</h2><p>列出<code>etc / hadoop / slaves</code>文件中的所有从属主机名或IP地址，每行一个。Helper脚本（如下所述）将使用<code>etc / hadoop / slaves</code>文件一次在多个主机上运行命令。它不用于任何基于Java的Hadoop配置。为了使用此功能，必须为用于运行Hadoop的帐户建立ssh信任（通过无密码ssh或其他方式，如Kerberos）。</p><h2 id="Hadoop机架意识"><a href="#Hadoop机架意识" class="headerlink" title="Hadoop机架意识"></a>Hadoop机架意识</h2><p>许多Hadoop组件都具有机架感知功能，并利用网络拓扑结构提高性能和安全性。Hadoop守护程序通过调用管理员配置的模块来获取集群中从站的机架信息。有关更多具体信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9SYWNrQXdhcmVuZXNzLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness<i class="fa fa-external-link"></i></span>文档。</p><p>强烈建议在启动HDFS之前配置机架感知。</p><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p>Hadoop 通过Apache Commons Logging框架使用<span class="exturl" data-url="aHR0cDovL2xvZ2dpbmcuYXBhY2hlLm9yZy9sb2c0ai8yLngv" title="http://logging.apache.org/log4j/2.x/">Apache log4j<i class="fa fa-external-link"></i></span>进行日志记录。编辑<code>etc / hadoop / log4j.properties</code>文件以自定义Hadoop守护程序的日志记录配置（日志格式等）。</p><h2 id="操作Hadoop集群"><a href="#操作Hadoop集群" class="headerlink" title="操作Hadoop集群"></a>操作Hadoop集群</h2><p>完成所有必要的配置后，将文件分发到所有计算机上的<code>HADOOP_CONF_DIR</code>目录。这应该是所有计算机上的相同目录。</p><p>通常，建议HDFS和YARN作为单独的用户运行。在大多数安装中，HDFS进程以’hdfs’的形式执行。YARN通常使用’yarn’帐户。、</p><h3 id="Hadoop启动"><a href="#Hadoop启动" class="headerlink" title="Hadoop启动"></a>Hadoop启动</h3><p>要启动Hadoop集群，您需要启动HDFS和YARN集群。</p><p>第一次启动HDFS时，必须对其进行格式化。将新的分布式文件系统格式化为<em>hdfs</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format &lt;cluster_name&gt;</span><br></pre></td></tr></table></figure><p>在指定节点上使用以下命令以<em>hdfs</em>启动HDFS NameNode ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> --script hdfs start namenode</span><br></pre></td></tr></table></figure><p>使用以下命令在每个指定节点上以<em>hdfs</em>启动HDFS DataNode ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemons.sh --config <span class="variable">$HADOOP_CONF_DIR</span> --script hdfs start datanode</span><br></pre></td></tr></table></figure><p>如果配置了<code>etc/hadoop/slaves</code>和ssh trusted access（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>），则可以使用<strong>实用程序脚本启动所有HDFS进程</strong>。作为<em>hdfs</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>使用以下命令启动YARN，在指定的ResourceManager上以<em>yarn形式运行</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start resourcemanager</span><br></pre></td></tr></table></figure><p>运行脚本以在每个指定的主机上启动NodeManager作为<em>yarn</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemons.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start nodemanager</span><br></pre></td></tr></table></figure><p>启动独立的WebAppProxy服务器。以<em>纱线形式</em>在WebAppProxy服务器上运行。如果使用多个服务器进行负载平衡，则应在每个服务器上运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start proxyserver</span><br></pre></td></tr></table></figure><p>如果配置了<code>etc/hadoop/slaves</code>和ssh trusted access（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>），则可以使用实用程序脚本启动所有YARN进程。作为 As <em>yarn</em>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>使用以下命令启动MapReduce JobHistory Server，在指定的服务器上以<em>mapred运行</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[mapred]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/mr-jobhistory-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start historyserver</span><br></pre></td></tr></table></figure><h3 id="Hadoop关闭"><a href="#Hadoop关闭" class="headerlink" title="Hadoop关闭"></a>Hadoop关闭</h3><p>使用以下命令停止NameNode，在指定的NameNode上运行为<em>hdfs</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs] $ $ HADOOP_PREFIX / sbin / hadoop-daemon.sh --config $ HADOOP_CONF_DIR --script hdfs stop namenode</span><br></pre></td></tr></table></figure><p>运行脚本以将DataNode作为<em>hdfs</em>停止：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs] $ $ HADOOP_PREFIX / sbin / hadoop-daemons.sh --config $ HADOOP_CONF_DIR --script hdfs stop datanode</span><br></pre></td></tr></table></figure><p>如果配置了<code>etc / hadoop / slaves</code>和ssh trusted access（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>），则可以使用实用程序脚本停止所有HDFS进程。作为<em>hdfs</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs] $ $ HADOOP_PREFIX / sbin / stop-dfs.sh</span><br></pre></td></tr></table></figure><p>使用以下命令停止ResourceManager，在指定的ResourceManager上作为<em>yarn运行</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn] $ $ HADOOP_YARN_HOME / sbin / yarn-daemon.sh --config $ HADOOP_CONF_DIR stop resourcemanager</span><br></pre></td></tr></table></figure><p>运行脚本以将从站上的NodeManager作为<em>yarn</em>停止：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn] $ $ HADOOP_YARN_HOME / sbin / yarn-daemons.sh --config $ HADOOP_CONF_DIR stop nodemanager</span><br></pre></td></tr></table></figure><p>如果配置了<code>etc / hadoop / slaves</code>和ssh trusted access（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>），则可以使用实用程序脚本停止所有YARN进程。As <em>hdfs</em>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn] $ $ HADOOP_PREFIX / sbin / stop-yarn.sh</span><br></pre></td></tr></table></figure><p>停止WebAppProxy服务器。以<em>纱线形式</em>在WebAppProxy服务器上运行。如果使用多个服务器进行负载平衡，则应在每个服务器上运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn] $ $ HADOOP_YARN_HOME / sbin / yarn-daemon.sh --config $ HADOOP_CONF_DIR stop proxyserver</span><br></pre></td></tr></table></figure><p>使用以下命令停止MapReduce JobHistory Server，在指定的服务器上以<em>mapred运行</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[mapred] $ $ HADOOP_PREFIX / sbin / mr-jobhistory-daemon.sh --config $ HADOOP_CONF_DIR stop historyserver</span><br></pre></td></tr></table></figure><h2 id="Web界面"><a href="#Web界面" class="headerlink" title="Web界面"></a>Web界面</h2><p>一旦Hadoop集群启动并运行，请检查组件的web-ui，如下所述：</p><table><thead><tr><th>Daemon</th><th>Web Interface</th><th>Notes</th></tr></thead><tbody><tr><td>NameNode</td><td><span class="exturl" data-url="aHR0cDovL25uX2hvc3Q6cG9ydC8=" title="http://nn_host:port/">http://nn_host:port/<i class="fa fa-external-link"></i></span></td><td>Default HTTP port is 50070.</td></tr><tr><td>ResourceManager</td><td><span class="exturl" data-url="aHR0cDovL3JtX2hvc3Q6cG9ydC8=" title="http://rm_host:port/">http://rm_host:port/<i class="fa fa-external-link"></i></span></td><td>Default HTTP port is 8088.</td></tr><tr><td>MapReduce JobHistory Server</td><td><span class="exturl" data-url="aHR0cDovL2poc19ob3N0OnBvcnQv" title="http://jhs_host:port/">http://jhs_host:port/<i class="fa fa-external-link"></i></span></td><td>Default HTTP port is 19888.</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h2&gt;&lt;p&gt;本文档描述了如何安装和配置Hadoop集群，范围从几个节点到具有数千个节点的极大集群。要使用Hadoop，您可能首先要将其安装在一台计算机上
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop: 设置单个节点集群</title>
    <link href="https://tangguangen.com/2018/11/23/Hadoop-%E8%AE%BE%E7%BD%AE%E5%8D%95%E4%B8%AA%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4/"/>
    <id>https://tangguangen.com/2018/11/23/Hadoop-设置单个节点集群/</id>
    <published>2018-11-23T06:44:41.000Z</published>
    <updated>2018-11-26T12:08:44.307Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>本文档介绍如何设置和配置单节点Hadoop安装，以便您可以使用Hadoop MapReduce和Hadoop分布式文件系统（HDFS）快速执行简单操作。</p><h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><h3 id="支持的平台"><a href="#支持的平台" class="headerlink" title="支持的平台"></a>支持的平台</h3><ul><li>支持GNU / Linux作为开发和生产平台。已经在具有2000个节点的GNU / Linux集群上演示了Hadoop。</li><li>Windows也是受支持的平台，但以下步骤仅适用于Linux。要在Windows上设置Hadoop，请参阅<span class="exturl" data-url="aHR0cDovL3dpa2kuYXBhY2hlLm9yZy9oYWRvb3AvSGFkb29wMk9uV2luZG93cw==" title="http://wiki.apache.org/hadoop/Hadoop2OnWindows">Wiki页面<i class="fa fa-external-link"></i></span>。</li></ul><h3 id="必备软件"><a href="#必备软件" class="headerlink" title="必备软件"></a>必备软件</h3><p>Linux所需的软件包括：</p><ol><li>必须安装Java™。<span class="exturl" data-url="aHR0cDovL3dpa2kuYXBhY2hlLm9yZy9oYWRvb3AvSGFkb29wSmF2YVZlcnNpb25z" title="http://wiki.apache.org/hadoop/HadoopJavaVersions">HadoopJavaVersions<i class="fa fa-external-link"></i></span>描述了推荐的Java版本。</li><li>必须安装ssh并且必须运行sshd才能使用管理远程Hadoop守护程序的Hadoop脚本。</li></ol><h3 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h3><p>如果您的群集没有必需的软件，则需要安装它。</p><p>例如在Ubuntu Linux上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh</span><br><span class="line">$ sudo apt-get install rsync</span><br></pre></td></tr></table></figure><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>要获得Hadoop发行版，请从其中一个<span class="exturl" data-url="aHR0cDovL3d3dy5hcGFjaGUub3JnL2R5bi9jbG9zZXIuY2dpL2hhZG9vcC9jb21tb24v" title="http://www.apache.org/dyn/closer.cgi/hadoop/common/">Apache下载镜像<i class="fa fa-external-link"></i></span>下载最新的稳定版本。</p><h2 id="准备启动Hadoop集群"><a href="#准备启动Hadoop集群" class="headerlink" title="准备启动Hadoop集群"></a>准备启动Hadoop集群</h2><p>解压缩下载的Hadoop发行版。在分发中，编辑文件<code>etc / hadoop / hadoop-env.sh</code>以定义一些参数，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">＃设置为Java安装的根目录</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME = / usr / java / latest</span><br></pre></td></tr></table></figure><p>请尝试以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin / hadoop</span><br></pre></td></tr></table></figure><p>这将显示hadoop脚本的使用文档。</p><p>现在，您已准备好以三种支持模式之一启动Hadoop集群：</p><ul><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjU3RhbmRhbG9uZV9PcGVyYXRpb24=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation">本地（独立）模式<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjUHNldWRvLURpc3RyaWJ1dGVkX09wZXJhdGlvbg==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation">伪分布式模式<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjRnVsbHktRGlzdHJpYnV0ZWRfT3BlcmF0aW9u" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Fully-Distributed_Operation">完全分布式模式<i class="fa fa-external-link"></i></span></li></ul><h2 id="独立操作"><a href="#独立操作" class="headerlink" title="独立操作"></a>独立操作</h2><p>默认情况下，Hadoop配置为以非分布式模式运行，作为单个Java进程。这对调试很有用。</p><p>以下示例复制解压缩的conf目录以用作输入，然后查找并显示给定正则表达式的每个匹配项。输出将写入给定的输出目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir input</span><br><span class="line">$ cp etc/hadoop/*.xml input</span><br><span class="line">$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></span><br><span class="line">$ cat output/*</span><br></pre></td></tr></table></figure><h2 id="伪分布式操作"><a href="#伪分布式操作" class="headerlink" title="伪分布式操作"></a>伪分布式操作</h2><p>Hadoop还可以在伪分布式模式下在单节点上运行，其中每个Hadoop后台进程在单独的Java进程中运行。</p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>如下：</p><p>etc/hadoop/core-site.xml:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>etc/hadoop/hdfs-site.xml:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="设置passphraseless-ssh"><a href="#设置passphraseless-ssh" class="headerlink" title="设置passphraseless ssh"></a>设置passphraseless ssh</h3><p>现在检查您是否可以在没有密码的情况下ssh到localhost：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh localhost</span><br></pre></td></tr></table></figure><p>如果在没有密码短语的情况下无法ssh到localhost，请执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P<span class="string">''</span> -  f~ / .ssh / id_rsa</span><br><span class="line">$ cat~ / .ssh / id_rsa.pub &gt;&gt;〜/ .ssh / authorized_keys</span><br><span class="line">$ chmod 0600~ / .ssh / authorized_keys</span><br></pre></td></tr></table></figure><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><p>以下说明是在本地运行MapReduce作业。如果要在YARN上执行作业，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjWUFSTl9vbl9TaW5nbGVfTm9kZQ==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_Single_Node">单节点<i class="fa fa-external-link"></i></span>上的YARN 。</p><ol><li><p>格式化文件系统：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs namenode -format</span><br></pre></td></tr></table></figure></li><li><p>启动NameNode守护程序和DataNode守护程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>hadoop后台进程日志输出将写入<code>$ HADOOP_LOG_DIR</code>目录（默认为<code>$ HADOOP_HOME / logs</code>）。</p></li><li><p>浏览NameNode的Web界面; 默认情况下，它可用于：</p><ul><li>NameNode - <code>http：// localhost：50070 /</code></li></ul></li><li><p>创建执行MapReduce作业所需的HDFS目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -mkdir /user</span><br><span class="line">$ bin/hdfs dfs -mkdir /user/&lt;username&gt;</span><br></pre></td></tr></table></figure></li><li><p>将输入文件复制到分布式文件系统中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -put etc/hadoop input</span><br></pre></td></tr></table></figure></li><li><p>运行一些提供的示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></span><br></pre></td></tr></table></figure></li><li><p>检查输出文件：将输出文件从分布式文件系统复制到本地文件系统并检查它们：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -get output output</span><br><span class="line">$ cat output/*</span><br></pre></td></tr></table></figure><p>或者</p><p>查看分布式文件系统上的输出文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -cat output/*</span><br></pre></td></tr></table></figure></li><li><p>完成后，停止守护进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure></li></ol><h3 id="YARN在单个节点上"><a href="#YARN在单个节点上" class="headerlink" title="YARN在单个节点上"></a>YARN在单个节点上</h3><p>您可以通过设置一些参数并运行ResourceManager守护程序和NodeManager守护程序，以伪分布式模式在YARN上运行MapReduce作业。</p><p>以下说明假定已执行<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjRXhlY3V0aW9u" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Execution">上述指令的<i class="fa fa-external-link"></i></span> 1.~4步骤。</p><ol><li><p>配置参数如下:<code>etc/hadoop/mapred-site.xml</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><code>etc/hadoop/yarn-site.xml</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>启动ResourceManager守护程序和NodeManager守护程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>浏览ResourceManager的Web界面; 默认情况下，它可用于：</p><ul><li>ResourceManager - <code>http：// localhost：8088 /</code></li></ul></li><li><p>运行MapReduce作业。</p></li><li><p>完成后，停止后台进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure></li></ol><h2 id="全分布式操作"><a href="#全分布式操作" class="headerlink" title="全分布式操作"></a>全分布式操作</h2><p>有关设置完全分布式，非平凡群集的信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9DbHVzdGVyU2V0dXAuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html">群集设置<i class="fa fa-external-link"></i></span>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h2&gt;&lt;p&gt;本文档介绍如何设置和配置单节点Hadoop安装，以便您可以使用Hadoop MapReduce和Hadoop分布式文件系统（HDFS）快速执
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop命令指南</title>
    <link href="https://tangguangen.com/2018/11/22/Hadoop%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97/"/>
    <id>https://tangguangen.com/2018/11/22/Hadoop命令指南/</id>
    <published>2018-11-22T11:28:54.000Z</published>
    <updated>2018-11-26T12:08:23.645Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>所有hadoop命令都由<code>bin / hadoop</code>脚本调用。不带任何参数运行hadoop脚本会打印所有命令的描述。</p><p>用法: <code>hadoop [--config confdir] [--loglevel loglevel] [COMMAND] [GENERIC_OPTIONS] [COMMAND_OPTIONS]</code></p><table><thead><tr><th>FIELD</th><th>Description</th></tr></thead><tbody><tr><td><code>--config confdir</code></td><td>Overwrites the default Configuration directory. Default is <code>${HADOOP_HOME}/conf</code>.</td></tr><tr><td><code>--loglevel loglevel</code></td><td>Overwrites the log level. Valid log levels are FATAL, ERROR, WARN, INFO, DEBUG, and TRACE. Default is INFO.</td></tr><tr><td>GENERIC_OPTIONS</td><td>The common set of options supported by multiple commands.</td></tr><tr><td>COMMAND_OPTIONS</td><td>Various commands with their options are described in this documention for the Hadoop common sub-project. HDFS and YARN are covered in other documents.</td></tr></tbody></table><h3 id="通用选项"><a href="#通用选项" class="headerlink" title="通用选项"></a>通用选项</h3><p>许多子命令都支持一组通用的配置选项来改变它们的行为：</p><table><thead><tr><th>GENERIC_OPTION</th><th>Description</th></tr></thead><tbody><tr><td><code>-archives &lt;comma separated list of archives&gt;</code></td><td>Specify comma separated archives to be unarchived on the compute machines. Applies only to job.</td></tr><tr><td><code>-conf &lt;configuration file&gt;</code></td><td>Specify an application configuration file.</td></tr><tr><td><code>-D &lt;property&gt;=&lt;value&gt;</code></td><td>Use value for given property.</td></tr><tr><td><code>-files &lt;comma separated list of files&gt;</code></td><td>Specify comma separated files to be copied to the map reduce cluster. Applies only to job.</td></tr><tr><td><code>-fs &lt;file:///&gt; or &lt;hdfs://namenode:port&gt;</code></td><td>Specify default filesystem URL to use. Overrides ‘fs.defaultFS’ property from configurations.</td></tr><tr><td><code>-jt &lt;local&gt; or &lt;resourcemanager:port&gt;</code></td><td>Specify a ResourceManager. Applies only to job.</td></tr><tr><td><code>-libjars &lt;comma seperated list of jars&gt;</code></td><td>Specify comma separated jar files to include in the classpath. Applies only to job.</td></tr></tbody></table><h1 id="Hadoop常用命令"><a href="#Hadoop常用命令" class="headerlink" title="Hadoop常用命令"></a>Hadoop常用命令</h1><p>所有这些命令都是从<code>hadoop</code> shell命令执行的。它们已分解为<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9Db21tYW5kc01hbnVhbC5odG1sI1VzZXJfQ29tbWFuZHM=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CommandsManual.html#User_Commands"><strong>用户命令</strong><i class="fa fa-external-link"></i></span>和<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9Db21tYW5kc01hbnVhbC5odG1sI0FkbWluaXN0cmF0aW9uX0NvbW1hbmRz" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CommandsManual.html#Administration_Commands"><strong>管理命令</strong><i class="fa fa-external-link"></i></span>。</p><h2 id="用户命令"><a href="#用户命令" class="headerlink" title="用户命令"></a>用户命令</h2><p>对hadoop集群的用户有用的命令。</p><h3 id="档案"><a href="#档案" class="headerlink" title="档案"></a><code>档案</code></h3><p>创建一个hadoop存档。有关更多信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1hcmNoaXZlcy9IYWRvb3BBcmNoaXZlcy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-archives/HadoopArchives.html">Hadoop Archives Guide<i class="fa fa-external-link"></i></span>。</p><h3 id="checknative"><a href="#checknative" class="headerlink" title="checknative"></a><code>checknative</code></h3><p>用法：<code>hadoop checknative [-a] [-h]</code></p><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td><code>-a</code></td><td>Check all libraries are available.</td></tr><tr><td><code>-h</code></td><td>print help</td></tr></tbody></table><p>此命令检查Hadoop本机代码的可用性。有关更多信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9OYXRpdmVMaWJyYXJpZXMuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libaries<i class="fa fa-external-link"></i></span>。默认情况下，此命令仅检查libhadoop的可用性。</p><h3 id="classpath"><a href="#classpath" class="headerlink" title="classpath"></a><code>classpath</code></h3><p>Usage: <code>hadoop classpath [--glob |--jar &lt;path&gt; |-h |--help]</code></p><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td><code>--glob</code></td><td>expand wildcards</td></tr><tr><td><code>--jar</code> <em>path</em></td><td>write classpath as manifest in jar named <em>path</em></td></tr><tr><td><code>-h</code>, <code>--help</code></td><td>print help</td></tr></tbody></table><p>打印获取Hadoop jar和所需库所需的类路径。如果不带参数调用，则打印由命令脚本设置的类路径，该脚本可能在类路径条目中包含通配符。其他选项在通配符扩展后打印类路径，或将类路径写入jar文件的清单中。后者在无法使用通配符且扩展类路径超过支持的最大命令行长度的环境中非常有用。</p><h3 id="凭据"><a href="#凭据" class="headerlink" title="凭据"></a><code>凭据</code></h3><p>用法: <code>hadoop credential &lt;subcommand&gt; [options]</code></p><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td>create <em>alias</em> [-provider <em>provider-path</em>] [-strict] [-value <em>credential-value</em>]</td><td>提示用户将凭据存储为给定别名。所述<em>hadoop.security.credential.provider.path</em>芯site.xml文件内将被使用，除非一个<code>-provider</code>被指示。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。使用<code>-value</code>标志提供凭据值（也就是别名密码）而不是提示</td></tr><tr><td>delete <em>alias</em> [-provider <em>provider-path</em>] [-strict] [-f]</td><td>使用提供的别名删除凭据。所述<em>hadoop.security.credential.provider.path</em>芯site.xml文件内将被使用，除非一个<code>-provider</code>被指示。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。除非指定了<code>-f</code>，否则该命令会要求确认</td></tr><tr><td>list [-provider <em>provider-path</em>] [-strict]</td><td>列出所有的凭证的别名<em>hadoop.security.credential.provider.path</em>芯site.xml文件内将被使用，除非一个<code>-provider</code>被指示。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。</td></tr></tbody></table><p>用于管理凭据提供程序中的凭据，密码和机密的命令。</p><p>Hadoop中的CredentialProvider API允许分离应用程序以及它们如何存储所需的密码/秘密。为了指示特定的提供程序类型和位置，用户必须在core-site.xml中提供<em>hadoop.security.credential.provider.path</em>配置元素，或者对以下每个命令使用命令行选项<code>-provider</code>。此提供程序路径是以逗号分隔的URL列表，用于指示应查阅的提供程序列表的类型和位置。例如，以下路径：<code>user：///，jceks：//file/tmp/test.jceks,jceks：//hdfs@nn1.example.com/my/path/test.jceks</code></p><p>表示应通过用户提供程序查询当前用户的凭证文件，位于<code>/tmp/test.jceks</code>的本地文件是Java密钥库提供程序，该文件位于HDFS中的<code>nn1.example.com/my/path/ test.jceks</code>也是Java Keystore Provider的商店。</p><p>当使用凭证命令时，它通常用于向特定凭证存储提供商提供密码或秘密。为了明确指出要使用哪个提供者存储，应该使用<code>-provider</code>选项。否则，给定多个提供者的路径，将使用第一个非瞬态提供者。这可能是也可能不是你想要的那个。</p><p>提供商经常要求提供密码或其他秘密。如果提供程序需要密码而无法找到密码，则它将使用默认密码并发出警告消息，指出正在使用默认密码。如果提供了<code>-strict</code>标志，则警告消息将成为错误消息，并且该命令会立即返回错误状态。</p><p>示例：<code>hadoop凭证列表-provider jceks：//file/tmp/test.jceks</code></p><h3 id="distcp"><a href="#distcp" class="headerlink" title="distcp"></a><code>distcp</code></h3><p>递归复制文件或目录。有关详细信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1kaXN0Y3AvRGlzdENwLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-distcp/DistCp.html">Hadoop DistCp指南<i class="fa fa-external-link"></i></span>。</p><h3 id="fs"><a href="#fs" class="headerlink" title="fs"></a><code>fs</code></h3><p>“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9GaWxlU3lzdGVtU2hlbGwuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html">文件系统Shell指南”<i class="fa fa-external-link"></i></span>中介绍了此命令。当HDFS正在使用时，它是<code>hdfs dfs</code>的同义词。</p><h3 id="jar"><a href="#jar" class="headerlink" title="jar"></a><code>jar</code></h3><p>Usage: <code>hadoop jar &lt;jar&gt; [mainClass] args...</code></p><p>Runs a jar file.</p><p>Use <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvWWFybkNvbW1hbmRzLmh0bWwjamFy" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YarnCommands.html#jar"><code>yarn jar</code><i class="fa fa-external-link"></i></span> to launch YARN applications instead.</p><h3 id="key"><a href="#key" class="headerlink" title="key"></a><code>key</code></h3><p>Usage: <code>hadoop key &lt;subcommand&gt; [options]</code></p><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td>create <em>keyname</em> [-cipher <em>cipher</em>] [-size <em>size</em>] [-description <em>description</em>] [-attr <em>attribute=value</em>] [-provider <em>provider</em>] [-strict] [-help]</td><td>创建由指定的名称一个新的密钥<em>键名</em>由指定的提供者中的说法<code>-provider</code>说法。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。您可以使用<code>-cipher</code>参数指定密码。默认密码当前为“AES / CTR / NoPadding”。默认密钥大小为128.您可以使用<code>-size</code>参数指定请求的密钥长度。可以使用<code>-attr</code>参数指定任意属性=值样式属性。<code>-attr</code>可以多次指定，每个属性一次。</td></tr><tr><td>roll <em>keyname</em> [-provider <em>provider</em>] [-strict] [-help]</td><td>使用<code>-provider</code>参数为指示的提供程序中的指定键创建新版本。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。</td></tr><tr><td>delete <em>keyname</em> [-provider <em>provider</em>] [-strict] [-f] [-help]</td><td>从<code>-provider</code>指定的提供程序中删除<em>keyname</em>参数指定的所有密钥版本。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。除非指定了<code>-f，</code>否则该命令会要求用户确认。</td></tr><tr><td>list [-provider <em>provider</em>] [-strict] [-metadata] [-help]</td><td>显示在core-site.xml中配置或使用<code>-provider</code>参数指定的特定提供程序中包含的键名。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。<code>-metadata</code>显示元数据。</td></tr><tr><td>-help</td><td>打印此命令的用法</td></tr></tbody></table><p>通过KeyProvider管理密钥。有关KeyProviders的详细信息，请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvVHJhbnNwYXJlbnRFbmNyeXB0aW9uLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">透明加密指南”<i class="fa fa-external-link"></i></span>。</p><p>提供商经常要求提供密码或其他秘密。如果提供程序需要密码而无法找到密码，则它将使用默认密码并发出警告消息，指出正在使用默认密码。如果提供了<code>-strict</code>标志，则警告消息将成为错误消息，并且该命令会立即返回错误状态。</p><p>注意：某些KeyProviders（例如org.apache.hadoop.crypto.key.JavaKeyStoreProvider）不支持大写键名称。</p><p>注意：某些KeyProviders不直接执行密钥删除（例如，执行软删除，或延迟实际删除，以防止错误）。在这些情况下，删除后创建/删除具有相同名称的密钥时可能会遇到错误。有关详细信息，请查看基础KeyProvider。</p><h3 id="trace"><a href="#trace" class="headerlink" title="trace"></a><code>trace</code></h3><p>查看和修改Hadoop跟踪设置。请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9UcmFjaW5nLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/Tracing.html">跟踪指南”<i class="fa fa-external-link"></i></span>。</p><h3 id="version"><a href="#version" class="headerlink" title="version"></a><code>version</code></h3><p>用法：<code>hadoop版本</code></p><p>打印版本。</p><h3 id="CLASSNAME"><a href="#CLASSNAME" class="headerlink" title="CLASSNAME"></a><code>CLASSNAME</code></h3><p>用法：<code>hadoop CLASSNAME</code></p><p>运行名为<code>CLASSNAME</code>的类。</p><h3 id="envvars"><a href="#envvars" class="headerlink" title="envvars"></a><code>envvars</code></h3><p>用法：<code>hadoop envvars</code></p><p>显示计算的Hadoop环境变量。</p><h2 id="管理命令"><a href="#管理命令" class="headerlink" title="管理命令"></a>管理命令</h2><p>对hadoop集群的管理员有用的命令。</p><h3 id="daemonlog"><a href="#daemonlog" class="headerlink" title="daemonlog"></a><code>daemonlog</code></h3><p>Usage:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop daemonlog -getlevel &lt;host:port&gt; &lt;classname&gt; [-protocol (http|https)]</span><br><span class="line">hdoop daemonlog -setlevel &lt;host:port&gt; &lt;classname&gt; &lt;level&gt; [-protocol (http|https)]</span><br></pre></td></tr></table></figure><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td><code>-getlevel</code> <em>host:port</em> <em>classname</em>[-protocol (http\</td><td>https)]</td><td>在<em>host：port</em>运行的守护程序中打印由限定<em>类名</em>标识的日志的日志级别。所述<code>-protocol</code>标志指定用于连接的协议。</td></tr><tr><td><code>-setlevel</code> <em>host:port</em> <em>classname**level</em> [-protocol (http\</td><td>https)]</td><td>设置在<em>host：port</em>运行的守护程序中由限定<em>类名</em>标识的日志的日志级别。所述<code>-protocol</code>标志指定用于连接的协议。</td></tr></tbody></table><p>获取/设置守护程序中由限定类名称标识的日志的日志级别。默认情况下，该命令发送HTTP请求，但可以使用参数<code>-protocol https</code>来覆盖此请求以发送HTTPS请求。</p><p>例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop daemonlog -setlevel 127.0.0.1:50070 org.apache.hadoop.hdfs.server.namenode.NameNode DEBUG</span><br><span class="line">$ bin/hadoop daemonlog -getlevel 127.0.0.1:50470 org.apache.hadoop.hdfs.server.namenode.NameNode DEBUG -protocol https</span><br></pre></td></tr></table></figure><p>请注意，该设置不是永久性的，并且会在重新启动守护程序时重置。此命令通过向守护程序的内部Jetty servlet发送HTTP / HTTPS请求来工作，因此它支持以下守护程序：</p><ul><li>HDFS<ul><li>名称节点</li><li>辅助名称节点</li><li>数据节点</li><li>期刊节点</li></ul></li><li>YARN<ul><li>资源经理</li><li>节点管理员</li><li>时间线服务器</li></ul></li></ul><p>但是，该命令不支持KMS服务器，因为它的Web界面基于Tomcat，后者不支持servlet。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;所有hadoop命令都由&lt;code&gt;bin / hadoop&lt;/code&gt;脚本调用。不带任何参数运行hadoop脚本会打印所有命令的描述。&lt;
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>CS22n笔记01：Introduction to NLP and Deep Learning</title>
    <link href="https://tangguangen.com/2018/11/20/CS22n%E7%AC%94%E8%AE%B001-Introduction-to-NLP-and-Deep-Learning/"/>
    <id>https://tangguangen.com/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/</id>
    <published>2018-11-20T07:49:46.000Z</published>
    <updated>2018-11-28T09:07:49.185Z</updated>
    
    <content type="html"><![CDATA[<p>主要参考自<span class="exturl" data-url="aHR0cDovL3d3dy5oYW5rY3MuY29tL25scC9jczIyNG4taW50cm9kdWN0aW9uLXRvLW5scC1hbmQtZGVlcC1sZWFybmluZy5odG1s" title="http://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html">http://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html<i class="fa fa-external-link"></i></span></p><h2 id="什么是自然语言处理"><a href="#什么是自然语言处理" class="headerlink" title="什么是自然语言处理"></a>什么是自然语言处理</h2><p>自然语言处理是<strong>计算机科学</strong>、<strong>人工智能</strong>和<strong>语言学</strong>的交叉学科。虽然语言只是人工智能的一部分（人工智能还包括计算机视觉等），但它是非常独特的一部分。这个星球上有许多生物拥有超过人类的视觉系统，但只有人类才拥有这么高级的语言。</p><p>自然语言处理的目标是让计算机处理或说“<strong>理解</strong>”自然语言，以完成有意义的任务，比如订机票购物或QA等。<strong>完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。</strong></p><h3 id="自然语言处理涉及的几个层次"><a href="#自然语言处理涉及的几个层次" class="headerlink" title="自然语言处理涉及的几个层次"></a>自然语言处理涉及的几个层次</h3><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/1.PNG"><p>作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词（事实上，跳过分词虽然理所当然地不能做句法分析，但字符级也可以直接做不少应用）。接下来是形态学，援引《统计自然语言处理》中的定义：</p><blockquote><p>形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科［Matthews,2000］。</p></blockquote><p>下面的是<strong>句法分析</strong>和<strong>语义分析</strong>，最后面的在中文中似乎翻译做“<strong>语篇处理</strong>”，需要根据上文语境理解下文。</p><p>这门课主要关注画圈的三个部分，其中中间的两个是重中之重，虽然深度学习在语音识别上的发力最大。</p><h3 id="NLP应用"><a href="#NLP应用" class="headerlink" title="NLP应用"></a>NLP应用</h3><p>NLP中有不同级别的任务，从<strong>语音处理</strong>（speech processing）到<strong>语义解释</strong>（semantic interpretation）和<strong>语篇处理</strong>（ discourse processing）。 NLP的目标是设让计算机“理解”自然语言以执行某些任务的算法。 示例任务从简单到复杂有：</p><p><strong>Easy</strong></p><ul><li>Spell Checking</li><li>Keyword Search</li><li>Finding Synonyms</li></ul><p><strong>Medium</strong></p><ul><li>Parsing information from websites, documents, etc.</li></ul><p><strong>Hard</strong></p><ul><li>Machine Translation (e.g. Translate Chinese text to English)</li><li>Semantic Analysis (What is the meaning of query statement?)</li><li>Coreference (e.g. What does “he” or “it” refer to given a document?)</li><li>Question Answering (e.g. Answering Jeopardy questions).</li></ul><p>在工业界从搜索到广告投放、自动\辅助翻译、情感舆情分析、语音识别、聊天机器人\管家等等五花八门。</p><h3 id="人类语言的特殊之处"><a href="#人类语言的特殊之处" class="headerlink" title="人类语言的特殊之处"></a>人类语言的特殊之处</h3><p>人类（自然）语言有什么特别之处？ 人类语言是专门用于传达有意义的信息的，这种传输连小孩子都能很快学会（amazingly!）。人类语言是<strong>离散</strong>的、<strong>明确</strong>的<strong>符号系统</strong>。但又允许出现各种变种，比如颜文字，随意的错误拼写“I loooove it”。这种自由性可能是因为语言的可靠性。所以说语言文字绝对不是形式逻辑或传统AI的产物。</p><p>语言符号有多种形式（声音、手势、书写），在这些不同的形式中，其意义保持不变：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/2.PNG"><p>虽然人类语言是明确的符号系统，但符号传输到大脑的过程是通过连续的声学光学信号，大脑编码似乎是连续的激活值上的模式。另外巨大的词表也导致<strong>数据稀疏</strong>，不利于机器学习。这构成一种动机，是不是应该用<strong>连续的信号</strong>而不是离散的符号去处理语言。</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/3.PNG"><h2 id="什么是深度学习"><a href="#什么是深度学习" class="headerlink" title="什么是深度学习"></a>什么是深度学习</h2><p>深度学习是机器学习的一个子集。传统机器学习中，人类需要对专业问题理解非常透彻，才能手工设计特征。比如地名和机构名识别的特征模板：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/4.PNG"><p>然后把特征交给某个机器学习算法，比如线性分类器。机器为这些特征调整找到合适的权值，将误差优化到最小。</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/5.PNG"><p>下面这张图很好地展示了这个过程中的比例：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/6.PNG"><p>而深度学习是表示学习的一部分，用来学习原始输入的多层特征表示，输入的元数据可能是声音、字符、单词……</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/7.PNG"><h3 id="“深度学习”的历史"><a href="#“深度学习”的历史" class="headerlink" title="“深度学习”的历史"></a>“深度学习”的历史</h3><p>虽然这个术语大部分时候指代利用各种各样<strong>多层的神经网络</strong>进行<strong>表示学习</strong>，有时候也有一些<strong>概率图</strong>模型参与。统计学家会说，哦，不过是一些<strong>逻辑斯谛回归单元</strong>的堆砌而已。也许的确如此，但这还是以偏概全的说法（电子计算机还是一堆半导体的堆砌呢，大脑还是一堆神经元的堆砌呢）。这门课不会回顾历史（像Hinton老爷子那样博古通今），而只会专注当前在NLP领域大放异彩的方法。</p><h3 id="为什么需要研究深度学习"><a href="#为什么需要研究深度学习" class="headerlink" title="为什么需要研究深度学习"></a>为什么需要研究深度学习</h3><ul><li>手工特征耗时耗力，还不易拓展</li><li>自动特征学习快，方便拓展</li><li>深度学习提供了一种通用的学习框架，可用来表示世界、视觉和语言学信息</li><li>深度学习既可以无监督学习，也可以监督学习</li></ul><p>深度学习可追溯到八九十年代，但在2010年左右才崛起（最先是语音与图像，后来才是NLP），那之前为什么没有呢？</p><p>与Hinton介绍的一样，无非是以前<strong>数据量不够</strong>，<strong>计算力太弱</strong>。当然，最近也的确有许多新模型，新算法。</p><blockquote><ul><li><p>Large amounts of training data favor deep learning</p></li><li><p>Faster machines and multicore CPU/GPUs favor Deep Learning</p></li><li>New models, algorithms, ideas<br>• Better, more flexible learning of intermediate representations<br>• Effective end-to-end joint system learning<br>• Effective learning methods for using contexts and transferring between tasks</li></ul></blockquote><h3 id="语音识别中的深度学习"><a href="#语音识别中的深度学习" class="headerlink" title="语音识别中的深度学习"></a>语音识别中的深度学习</h3><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/8.PNG"><p>深度学习上突破性的研究发生在语音识别领域，来自Hinton老爷子的学生，具体参考：<span class="exturl" data-url="aHR0cDovL3d3dy5oYW5rY3MuY29tL21sL2hpbnRvbi1kZWVwLW5ldXJhbC1uZXRzLXdpdGgtZ2VuZXJhdGl2ZS1wcmUtdHJhaW5pbmcuaHRtbCNoMy0xMQ==" title="http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html#h3-11">http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html#h3-11<i class="fa fa-external-link"></i></span></p><h3 id="计算机视觉中的深度学习"><a href="#计算机视觉中的深度学习" class="headerlink" title="计算机视觉中的深度学习"></a>计算机视觉中的深度学习</h3><p>大多数深度学习的研究都集中在计算机视觉（至少到两年前）</p><p>突破性进展还是来自Hinton的学生。</p><blockquote><p>ImageNet Classification with Deep Convolutional Neural Networks by Krizhevsky, Sutskever, &amp; Hinton, 2012, U. Toronto.</p></blockquote><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/9.PNG"><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/10.PNG"><h2 id="课程相关"><a href="#课程相关" class="headerlink" title="课程相关"></a>课程相关</h2><p>有4次编程练习，会用到TensorFlow。</p><h2 id="为什么NLP难"><a href="#为什么NLP难" class="headerlink" title="为什么NLP难"></a>为什么NLP难</h2><p>人类语言是充满歧义的，不像编程语言那样明确。编程语言中有各种变量名，但人类语言中只有少数几个代词可以用，你得思考到底指代的是谁……</p><p>人类语言的解读依赖于现实世界、常识以及上下文。由于说话速度书写速度阅读速度的限制，人类语言非常简练，省略了大量背景知识。</p><p>接下来是几个英文的歧义例子，对native speaker而言很有趣。为了完整性只看一个：</p><blockquote><p>The Pope’s baby steps on gays</p></blockquote><p>主要歧义发生在baby上面，可以理解为“教皇的孩子踩了基佬”，也可以理解为“教皇在同性恋问题上裹足不前”。</p><p>旧版CS224d里面还有个更直观的例子，推特上关于电影明星“海瑟薇”的评论影响了保险公司哈撒韦的股价，因为两者拼写是一样的。</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/11.png"><p>说明某些“舆情系统”没做好命名实体识别。</p><h2 id="Deep-NLP-Deep-Learning-NLP"><a href="#Deep-NLP-Deep-Learning-NLP" class="headerlink" title="Deep NLP = Deep Learning + NLP"></a>Deep NLP = Deep Learning + NLP</h2><p>将<strong>自然语言处理</strong>的思想与<strong>表示学习</strong>结合起来，用<strong>深度学习</strong>的手法解决NLP目标。这提高了许多方面的效果：</p><ul><li>层次：语音、词汇、语法、语义</li><li>工具：词性标注、命名实体识别、句法\语义分析</li><li>应用：机器翻译、情感分析、客服系统、问答系统</li></ul><h3 id="Word-Vectors"><a href="#Word-Vectors" class="headerlink" title="Word Vectors"></a>Word Vectors</h3><p>所有NLP任务的第一个也是最重要的共同点是我们如何将<strong>单词</strong>表示为<strong>模型的输入</strong>。 为了在大多数NLP任务上表现良好，我们首先需要有一些相似性和单词之间差异的概念。 使用词向量，我们可以很容易地在向量本身中编码这种能力（如Jaccard，Cosine，Euclidean等距离测量方法）。</p><h3 id="NLP表示层次：形态级别"><a href="#NLP表示层次：形态级别" class="headerlink" title="NLP表示层次：形态级别"></a>NLP表示层次：形态级别</h3><p>传统方法在形态级别的表示是词素：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/12.PNG"><p>深度学习中把词素也作为向量，多个词素向量构成相同纬度语义更丰富的词向量。</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/13.PNG"><h3 id="NLP工具：句法分析"><a href="#NLP工具：句法分析" class="headerlink" title="NLP工具：句法分析"></a>NLP工具：句法分析</h3><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/14.PNG"><blockquote><p>我在<span class="exturl" data-url="aHR0cDovL3d3dy5oYW5rY3MuY29tL25scC9wYXJzaW5nL25ldXJhbC1uZXR3b3JrLWJhc2VkLWRlcGVuZGVuY3ktcGFyc2VyLmh0bWw=" title="http://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html">《基于神经网络的高性能依存句法分析器》<i class="fa fa-external-link"></i></span>中分析并<span class="exturl" data-url="aHR0cDovL2hhbmxwLmhhbmtjcy5jb20vP3NlbnRlbmNlPSVFNSVCRSU5MCVFNSU4NSU4OCVFNyU5NCU5RiVFOCVCRiU5OCVFNSU4NSVCNyVFNCVCRCU5MyVFNSVCOCVBRSVFNSU4QSVBOSVFNCVCQiU5NiVFNyVBMSVBRSVFNSVBRSU5QSVFNCVCQSU4NiVFNiU4QSU4QSVFNyU5NCVCQiVFOSU5QiU4NCVFOSVCOSVCMCVFMyU4MCU4MSVFNiU5RCVCRSVFOSVCQyVBMCVFNSU5MiU4QyVFOSVCQSVCQiVFOSU5QiU4MCVFNCVCRCU5QyVFNCVCOCVCQSVFNCVCOCVCQiVFNiU5NCVCQiVFNyU5QiVBRSVFNiVBMCU4NyVFMyU4MCU4Mg==" title="http://hanlp.hankcs.com/?sentence=%E5%BE%90%E5%85%88%E7%94%9F%E8%BF%98%E5%85%B7%E4%BD%93%E5%B8%AE%E5%8A%A9%E4%BB%96%E7%A1%AE%E5%AE%9A%E4%BA%86%E6%8A%8A%E7%94%BB%E9%9B%84%E9%B9%B0%E3%80%81%E6%9D%BE%E9%BC%A0%E5%92%8C%E9%BA%BB%E9%9B%80%E4%BD%9C%E4%B8%BA%E4%B8%BB%E6%94%BB%E7%9B%AE%E6%A0%87%E3%80%82">移植的LTP句法分析器<i class="fa fa-external-link"></i></span>，参考的就是这里介绍的Danqi Chen的<span class="exturl" data-url="aHR0cDovL3d3dy5oYW5rY3MuY29tL3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDE1LzExL0ElMjBGYXN0JTIwYW5kJTIwQWNjdXJhdGUlMjBEZXBlbmRlbmN5JTIwUGFyc2VyJTIwdXNpbmclMjBOZXVyYWwlMjBOZXR3b3Jrcy5wZGY=" title="http://www.hankcs.com/wp-content/uploads/2015/11/A%20Fast%20and%20Accurate%20Dependency%20Parser%20using%20Neural%20Networks.pdf">A Fast and Accurate Dependency Parser using Neural Networks.pdf<i class="fa fa-external-link"></i></span>。原来她是这门课的TA：</p></blockquote><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/15.PNG"><h3 id="NLP语义层面的表示"><a href="#NLP语义层面的表示" class="headerlink" title="NLP语义层面的表示"></a>NLP语义层面的表示</h3><p>传统方法是手写大量的规则函数，叫做Lambda calculus：</p><p>• Carefully engineered functions<br>• Take as inputs specific other functions<br>• No notion of similarity or fuzziness of language</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/16.PNG"><p>在深度学习中，每个句子、短语和逻辑表述都是向量。神经网络负责它们的合并。</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/17.PNG"><h3 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h3><p>传统方法是请一两百个工人，手工搜集“情感极性词典”在词袋模型上做分类器。</p><p>深度学习复用了RNN来解决这个问题，它可以识别“反话”的情感极性：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/18.PNG"><p>注意这只是为了方便理解的示意图，并不是RNN的工作流程。私以为这张图放在这里不合适，可能会误导一部分人，以为神经网络就是这样的基于规则的“决策树”模型。</p><h3 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h3><p>传统方法是手工编写大量的逻辑规则，比如正则表达式之类：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/19.PNG"><p>深度学习依然使用了类似的学习框架，把事实储存在向量里：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/20.PNG"><h3 id="客服系统"><a href="#客服系统" class="headerlink" title="客服系统"></a>客服系统</h3><p>最著名的例子得数GMail的自动回复：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/smartreply1.gif"><p>这是<strong>Neural Language Models</strong>的又一次成功应用，Neural Language Models是基于<strong>RNN</strong>的：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/21.PNG"><h3 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h3><p>传统方法在许多层级上做了尝试，词语、语法、语义之类。这类方法试图找到一种世界通用的“国际语”（Interlingua）来作为原文和译文的桥梁。</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/22.PNG"><p>而<strong>Neural Machine Translation</strong>将原文映射为<strong>向量</strong>，由<strong>向量构建译文</strong>。也许可以说Neural Machine Translation的“国际语”是向量。</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/23.PNG"><h3 id="结论：所有层级的表示都是向量"><a href="#结论：所有层级的表示都是向量" class="headerlink" title="结论：所有层级的表示都是向量"></a>结论：所有层级的表示都是向量</h3><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/24.PNG"><p>这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高阶的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。</p><p>下面两次课会详细地讲解向量表示，希望能带来新的体会。</p><blockquote><p>这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高阶的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。</p><p>旧版视频中Socher还顺便广告了一下他的创业公司MetaMind（已被收购，人生赢家）：</p><img src="/2018/11/20/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/25.PNG"><p>这个demo让我非常惊讶，因为普通NLP演示页面都是让人手工选择要执行的任务的。而这个demo竟然支持用一句话表示自己要执行的意图。不光可以执行情感分析、句法分析之类的常规任务，还可以输入一段话做推理任务。更让我惊讶的是，据说后台所有任务用的都是同一种模型，真乃神机也。据说这种模型是<span class="exturl" data-url="aHR0cHM6Ly9tZXRhbWluZC5pby9yZXNlYXJjaC9uZXctZGVlcC1sZWFybmluZy1tb2RlbC11bmRlcnN0YW5kcy1hbmQtYW5zd2Vycy1xdWVzdGlvbnM=" title="https://metamind.io/research/new-deep-learning-model-understands-and-answers-questions">Dynamic Memory Network<i class="fa fa-external-link"></i></span>。另外，他们又发了篇<span class="exturl" data-url="aHR0cHM6Ly9tZXRhbWluZC5pby9yZXNlYXJjaC9tdWx0aXBsZS1kaWZmZXJlbnQtbmF0dXJhbC1sYW5ndWFnZS1wcm9jZXNzaW5nLXRhc2tzLWluLWEtc2luZ2xlLWRlZXAtbW9kZWw=" title="https://metamind.io/research/multiple-different-natural-language-processing-tasks-in-a-single-deep-model">A Joint Many-Task Model:Growing a Neural Network for Multiple NLP Tasks<i class="fa fa-external-link"></i></span>，不知道两者有什么联系没有。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;主要参考自&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cDovL3d3dy5oYW5rY3MuY29tL25scC9jczIyNG4taW50cm9kdWN0aW9uLXRvLW5scC1hbmQtZGVlcC1sZWFybmluZy5odG1s&quot;
      
    
    </summary>
    
      <category term="NLP" scheme="https://tangguangen.com/categories/NLP/"/>
    
    
      <category term="CS224n" scheme="https://tangguangen.com/tags/CS224n/"/>
    
      <category term="NLP" scheme="https://tangguangen.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>实体和关系的联合抽取研究</title>
    <link href="https://tangguangen.com/2018/11/19/%E5%AE%9E%E4%BD%93%E5%92%8C%E5%85%B3%E7%B3%BB%E7%9A%84%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0/"/>
    <id>https://tangguangen.com/2018/11/19/实体和关系的联合抽取研究综述/</id>
    <published>2018-11-19T12:54:25.000Z</published>
    <updated>2018-11-20T01:54:07.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>实体关系联合抽取旨在识别出非结构化文本中的实体, 并同时提取出实体之间隐含的语义关系。它是信息抽取任务中一个非常重要研究方向。本文总结了该领域的主要研究方向和最新进展。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>实体关系联合抽取旨在识别出非结构化文本中的实体, 并同时提取出实体之间隐含的语义关系。如图一所示</p><img src="/2018/11/19/实体和关系的联合抽取研究综述/图1.PNG"><p>与<strong>开放信息抽取 (Open IE)</strong> (<em>Banko et al., 2007</em>) 不同的是，实体关系联合抽取任务中的关系词是从预定义的关系集合中抽取出来的，它可能不会出现在给定的句子当中。这是<strong>知识提取（knowledge extraction）</strong>和<strong>知识库（knowledge base）</strong>自动构建中的一个重要问题。</p><p>实体关系抽取任务的传统方法采用<strong>流水线方式（pipelined manner）</strong>，例如：首先提取出<strong>实体</strong>(<em>Nadeau and Sekine, 2007</em>)，然后对实体间的关系进行<strong>分类</strong>(<em>Rink, 2010</em>)。这种框架将实体的识别和关系的分类分为两个子任务，使得实体和关系的抽取任务变得简单，每个组件都很灵活。由于每一个子任务都是一个独立的模型，忽略了这两个子任务之间的相关性，实体识别的结果会对关系分类的性能造成影响，产生<strong>误差传递</strong>(<em>Li and Ji, 2014</em>)。</p><p><strong>联合学习</strong>方法是使用一个模型同时抽取实体和关系。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]张晓斌,陈福才,黄瑞阳.基于CNN和双向LSTM融合的实体关系抽取[J].网络与信息安全学报,2018(09):44-51.</p><p>[2]Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJCAI. volume 7, pages 2670–2676.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;实体关系联合抽取旨在识别出非结构化文本中的实体, 并同时提取出实体之间隐含的语义关系。它是信
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>云服务器安装远程ipython notebook</title>
    <link href="https://tangguangen.com/2018/09/28/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85%E8%BF%9C%E7%A8%8Bipython-notebook/"/>
    <id>https://tangguangen.com/2018/09/28/云服务器安装远程ipython-notebook/</id>
    <published>2018-09-28T07:22:48.000Z</published>
    <updated>2018-09-30T01:20:53.200Z</updated>
    
    <content type="html"><![CDATA[<p>Ipython Notebook非常适合用来做数据分析，在windows下安装anaconda就可以直接使用Ipython Notebook了。但是如果是要在云服务器的Linux环境下安装Ipython Notebook，并且在本地远程访问，就需要自己动手配置了。</p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>服务器：阿里云ECS，Ubuntu16.04 64位</p><p>Xshell5</p><h3 id="1-安装anaconda"><a href="#1-安装anaconda" class="headerlink" title="1.安装anaconda"></a>1.安装anaconda</h3><p>Anaconda指的是一个开源的<span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS9QeXRob24=" title="https://baike.baidu.com/item/Python">Python<i class="fa fa-external-link"></i></span>发行版本，其包含了conda、Python等180多个科学包及其依赖项。</p><p>首先在<span class="exturl" data-url="aHR0cHM6Ly9taXJyb3JzLnR1bmEudHNpbmdodWEuZWR1LmNuL2FuYWNvbmRhL2FyY2hpdmUv" title="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/">清华大学开源软件镜像站<i class="fa fa-external-link"></i></span>下载Anaconda3，下载最新版本就行了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update <span class="comment">#执行这条命令更新一下</span></span><br><span class="line">sudo wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Linux-x86_64.sh<span class="comment">#这里我选择的版本是3-5.2.0，64位。</span></span><br></pre></td></tr></table></figure><p>下载好后，进入下载的anaconda 安装包的目录下进行安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-5.2.0-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>最后一部是询问是否将python加入环境变量，选择yes，当然你也可以在安装完后手动添加到bash中。 </p><p>安装完成之后要重启终端，anaconda才能生效。</p><p>输入python进行测试安装是否成功，显示python版本这表示安装正确。</p><img src="/2018/09/28/云服务器安装远程ipython-notebook/python测试.PNG"><h2 id="2-配置ipython-jupyter"><a href="#2-配置ipython-jupyter" class="headerlink" title="2. 配置ipython jupyter"></a>2. 配置ipython jupyter</h2><h3 id="2-1-生成配置文件"><a href="#2-1-生成配置文件" class="headerlink" title="2.1.生成配置文件"></a>2.1.生成配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成配置文件</span></span><br><span class="line">jupyter notebook --generate-config</span><br><span class="line"><span class="comment"># 此时生成配置文件：</span></span><br><span class="line"><span class="comment"># Writing default config to: /home/python/.jupyter/jupyter_notebook_config.py</span></span><br><span class="line"><span class="comment"># 这里 /home/python/... 中的python指的是用户名是python，将其改成自己的用户名就好了，下面的情况也是一样的。</span></span><br><span class="line"><span class="comment"># 创建登录密码</span></span><br><span class="line"><span class="comment"># 打开ipython，生成密钥，这一步也可以不做，如果不需要设置密码的话。</span></span><br><span class="line">$ ipython</span><br><span class="line">from notebook.auth import passwd</span><br><span class="line">passwd()</span><br><span class="line">Enter password:</span><br><span class="line">Verify password:</span><br><span class="line">Out[2]: <span class="string">'sha1:6f6193fcfbd5:614c4ba185334868fc8bbce2e9890b3ef7d1a79b'</span>  </span><br><span class="line"><span class="comment"># 我这里创建的密码是123456，对应的密钥是sha1xxxx的那一串</span></span><br><span class="line"><span class="comment"># 然后退出ipython</span></span><br></pre></td></tr></table></figure><h3 id="2-2-创建自签名的证书"><a href="#2-2-创建自签名的证书" class="headerlink" title="2.2.创建自签名的证书"></a>2.2.创建自签名的证书</h3><p>这一步使用openssl创建一个自签名证书，如果不担心安全问题，不使用ssl速度会快一些。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># 在linux下执行，遇到询问的地方一路回车即可</span></span><br><span class="line">openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 会在当前文件夹下生成 mycert.pem，我将它移到.jupyter/secret文件夹下面，方便管理</span></span><br><span class="line"><span class="comment"># 先创建.secret文件夹</span></span><br><span class="line"><span class="built_in">cd</span> .jupyter</span><br><span class="line">mkdir secret  </span><br><span class="line"><span class="comment"># 移动</span></span><br><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mv mycert.pem .jupyter/secret/</span><br></pre></td></tr></table></figure><h3 id="2-3-修改配置文件"><a href="#2-3-修改配置文件" class="headerlink" title="2.3.修改配置文件"></a>2.3.修改配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开刚才创建的.jupyter/jupyter_notebook_config.py，先备份源文件，然后再修改</span></span><br><span class="line"><span class="comment"># 备份</span></span><br><span class="line">$ cp .jupyter/jupyter_notebook_config.py .jupyter/jupyter_notebook_config.py_bak</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改如下，可以先删除里面的内容添加，也可以修改，或者直接在头部添加，反正里面的原先的内容都是注释掉的：</span></span><br><span class="line">vi /home/python/.jupyter/jupyter_notebook_config.py</span><br><span class="line"></span><br><span class="line">c = get_config()</span><br><span class="line"><span class="comment"># Kernel config</span></span><br><span class="line">c.IPKernelApp.pylab = <span class="string">'inline'</span>  <span class="comment"># if you want plotting support always</span></span><br><span class="line"></span><br><span class="line">c.NotebookApp.ip = <span class="string">'*'</span>  <span class="comment"># 就是设置所有ip皆可访问，在144行</span></span><br><span class="line">c.NotebookApp.open_browser = False  <span class="comment"># 禁止自动打开浏览器</span></span><br><span class="line"><span class="comment"># 密钥，在194行。该密钥就是2.1步生成的</span></span><br><span class="line">c.NotebookApp.password = <span class="string">'sha1:74d233d59da1:50d7ef60a58456e2016dc427547fb42cdd971cea'</span></span><br><span class="line">c.NotebookApp.port = 6868  <span class="comment"># 访问端口，在197行</span></span><br><span class="line"><span class="comment"># 自签名证书位置，如果不使用ssl，可以不设置</span></span><br><span class="line">c.NotebookNotary.secret_file = <span class="string">'/home/python/.jupyter/secret/mycert.pem'</span></span><br><span class="line">c.NotebookApp.keyfile = <span class="string">'/home/python/.jupyter/.secret/mykey.key'</span></span><br><span class="line"><span class="comment"># 设置目录，存放创建的ipython notebook文件</span></span><br><span class="line">c.NotebookApp.notebook_dir = <span class="string">'/home/python/ipython'</span></span><br></pre></td></tr></table></figure><h2 id="3-XShell配置远程访问"><a href="#3-XShell配置远程访问" class="headerlink" title="3.XShell配置远程访问"></a>3.XShell配置远程访问</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动ipython jupyter，不使用ssl</span></span><br><span class="line">jupyter notebook</span><br><span class="line"><span class="comment"># 或者开启ssl</span></span><br><span class="line"><span class="comment"># jupyter notebook --certfile=mycert.pem --keyfile mykey.key</span></span><br><span class="line">jupyter notebook --certfile=/home/zhenyu/.jupyter/secret/mycert.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出，看倒数第二行，显示的是IP地址和端口号，记下来</span></span><br><span class="line">[I 15:56:59.916 NotebookApp] JupyterLab beta preview extension loaded from /home/python/anaconda3/lib/python3.6/site-packages/jupyterlab</span><br><span class="line">[I 15:56:59.916 NotebookApp] JupyterLab application directory is /home/python/anaconda3/share/jupyter/lab</span><br><span class="line">[I 15:56:59.921 NotebookApp] Serving notebooks from <span class="built_in">local</span> directory: /home/python/ipython</span><br><span class="line">[I 15:56:59.921 NotebookApp] 0 active kernels</span><br><span class="line">[I 15:56:59.921 NotebookApp] The Jupyter Notebook is running at:</span><br><span class="line">[I 15:56:59.921 NotebookApp] http://127.0.0.1:6868/</span><br><span class="line">[I 15:56:59.921 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span><br></pre></td></tr></table></figure><p>这时候退出XShell连接，再打开XShell，进入会话连接属性，添加一条转义规则，就可以在本地window机器上远程访问jupyter notebook使用python进行数据分析了。</p><img src="/2018/09/28/云服务器安装远程ipython-notebook/端口转发.PNG"><p>打开浏览器，在地址栏输入<span class="exturl" data-url="aHR0cDovL2xvY2FsaG9zdDo1ODU45bCx6IO96L+c56iL6K6/6Zeu5LqR5pyN5Yqh5Zmo5Lit55qENjg2OOerr+WPo+S6huOAgg==" title="http://localhost:5858就能远程访问云服务器中的6868端口了。">http://localhost:5858就能远程访问云服务器中的6868端口了。<i class="fa fa-external-link"></i></span></p><img src="/2018/09/28/云服务器安装远程ipython-notebook/本地远程访问.PNG"><p>linux 下安装anaconda<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0MTQ1ODkvYXJ0aWNsZS9kZXRhaWxzLzUxMzAzNTAy" title="https://blog.csdn.net/u010414589/article/details/51303502">https://blog.csdn.net/u010414589/article/details/51303502<i class="fa fa-external-link"></i></span></p><p>Linux 远程Ipython Notebook<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1enl1MTIzNDUvYXJ0aWNsZS9kZXRhaWxzLzUxMDM3OTA1" title="https://blog.csdn.net/suzyu12345/article/details/51037905">https://blog.csdn.net/suzyu12345/article/details/51037905<i class="fa fa-external-link"></i></span></p><p>XShell–SSH端口转发<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MDM5MzE1L2FydGljbGUvZGV0YWlscy83NzUxMDkyMw==" title="https://blog.csdn.net/qq_34039315/article/details/77510923">https://blog.csdn.net/qq_34039315/article/details/77510923<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Ipython Notebook非常适合用来做数据分析，在windows下安装anaconda就可以直接使用Ipython Notebook了。但是如果是要在云服务器的Linux环境下安装Ipython Notebook，并且在本地远程访问，就需要自己动手配置了。&lt;/p&gt;

      
    
    </summary>
    
      <category term="技术" scheme="https://tangguangen.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="anaconda" scheme="https://tangguangen.com/tags/anaconda/"/>
    
      <category term="云服务器" scheme="https://tangguangen.com/tags/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop在云服务器上的安装教程</title>
    <link href="https://tangguangen.com/2018/09/28/Hadoop%E5%9C%A8%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    <id>https://tangguangen.com/2018/09/28/Hadoop在云服务器上的安装教程/</id>
    <published>2018-09-28T02:59:15.000Z</published>
    <updated>2018-09-30T01:13:06.599Z</updated>
    
    <content type="html"><![CDATA[<p>XShell–SSH端口转发<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MDM5MzE1L2FydGljbGUvZGV0YWlscy83NzUxMDkyMw==" title="https://blog.csdn.net/qq_34039315/article/details/77510923">https://blog.csdn.net/qq_34039315/article/details/77510923<i class="fa fa-external-link"></i></span></p><p>修改Linux终端命令行中的用户名和主机名<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpbmdkdTAwNy9hcnRpY2xlL2RldGFpbHMvNTE0MzQxNzI=" title="https://blog.csdn.net/qingdu007/article/details/51434172">https://blog.csdn.net/qingdu007/article/details/51434172<i class="fa fa-external-link"></i></span></p><p>本教程<strong>转载</strong>自<span class="exturl" data-url="aHR0cDovL2RibGFiLnhtdS5lZHUuY24v" title="http://dblab.xmu.edu.cn/">厦门大学数据库实验室<i class="fa fa-external-link"></i></span> / <span class="exturl" data-url="aHR0cDovL3d3dy5wb3dlcnhpbmcuY29tLw==" title="http://www.powerxing.com/">给力星<i class="fa fa-external-link"></i></span></p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>本教程使用 <strong>Ubuntu 14.04 64位</strong> 作为系统环境（Ubuntu 12.04，Ubuntu16.04 也行，32位、64位均可），请自行安装系统（可参考<span class="exturl" data-url="aHR0cDovL2RibGFiLnhtdS5lZHUuY24vYmxvZy8zMzctMi8=" title="http://dblab.xmu.edu.cn/blog/337-2/">使用VirtualBox安装Ubuntu<i class="fa fa-external-link"></i></span>）。</p><p>如果用的是 CentOS/RedHat 系统，请查看相应的<span class="exturl" data-url="aHR0cDovL2RibGFiLnhtdS5lZHUuY24vYmxvZy9pbnN0YWxsLWhhZG9vcC1pbi1jZW50b3Mv" title="http://dblab.xmu.edu.cn/blog/install-hadoop-in-centos/">CentOS安装Hadoop教程_单机伪分布式配置<i class="fa fa-external-link"></i></span>。</p><p>本教程基于原生 Hadoop 2，在 <strong>Hadoop 2.6.0 (stable)</strong> 版本下验证通过，可适合任何 Hadoop 2.x.y 版本，如 Hadoop 2.7.1、2.6.3、2.4.1等。</p><p>使用本教程请确保系统处于联网状态下，部分高校使用星网锐捷连接网络，可能导致虚拟机无法联网，那么建议您使用双系统安装ubuntu,然后再使用本教程！</p><p>Hadoop版本</p><p>Hadoop 有两个主要版本，Hadoop 1.x.y 和 Hadoop 2.x.y 系列，比较老的教材上用的可能是 0.20 这样的版本。Hadoop 2.x 版本在不断更新，本教程均可适用。如果需安装 0.20，1.2.1这样的版本，本教程也可以作为参考，主要差别在于配置项，配置请参考官网教程或其他教程。</p><p>新版是兼容旧版的，书上旧版本的代码应该能够正常运行（我自己没验证，欢迎验证反馈）。</p><p>装好了 Ubuntu 系统之后，在安装 Hadoop 前还需要做一些必备工作。</p><h2 id="创建hadoop用户"><a href="#创建hadoop用户" class="headerlink" title="创建hadoop用户"></a>创建hadoop用户</h2><p>如果你安装 Ubuntu 的时候不是用的 “hadoop” 用户，那么需要增加一个名为 hadoop 的用户。</p><p>首先按 <strong>ctrl+alt+t</strong> 打开终端窗口，输入如下命令创建新用户 :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd -m hadoop -s /bin/bash</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>这条命令创建了可以登陆的 hadoop 用户，并使用 /bin/bash 作为 shell。</p><p>sudo命令</p><p>本文中会大量使用到sudo命令。sudo是ubuntu中一种权限管理机制，管理员可以授权给一些普通用户去执行一些需要root权限执行的操作。当使用sudo命令时，就需要输入您当前用户的密码.</p><p>密码</p><p>在Linux的终端中输入密码，终端是不会显示任何你当前输入的密码，也不会提示你已经输入了多少字符密码。而在windows系统中,输入密码一般都会以“*”表示你输入的密码字符</p><p>输入法中英文切换</p><p>ubuntu中终端输入的命令一般都是使用英文输入。linux中英文的切换方式是使用键盘“shift”键来切换，也可以点击顶部菜单的输入法按钮进行切换。ubuntu自带的Sunpinyin中文输入法已经足够读者使用。</p><p>Ubuntu终端复制粘贴快捷键</p><p>在Ubuntu终端窗口中，复制粘贴的快捷键需要加上 shift，即粘贴是 ctrl+shift+v。</p><p>接着使用如下命令设置密码，可简单设置为 hadoop，按提示输入两次密码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo passwd hadoop</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>可为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较棘手的权限问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo adduser hadoop sudo</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>最后注销当前用户（点击屏幕右上角的齿轮，选择注销），返回登陆界面。在登陆界面中选择刚创建的 hadoop 用户进行登陆。</p><h2 id="更新apt"><a href="#更新apt" class="headerlink" title="更新apt"></a>更新apt</h2><p>用 hadoop 用户登录后，我们先更新一下 apt，后续我们使用 apt 安装软件，如果没更新可能有一些软件安装不了。按 ctrl+alt+t 打开终端窗口，执行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>若出现如下 “Hash校验和不符” 的提示，可通过更改软件源来解决。若没有该问题，则不需要更改。从软件源下载某些软件的过程中，可能由于网络方面的原因出现没法下载的情况，那么建议更改软件源。在学习Hadoop过程中，即使出现“Hash校验和不符”的提示，也不会影响Hadoop的安装。</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-01-apt-hash.png" alt="Ubuntu更新软件源时遇到Hash校验和不符的问题">Ubuntu更新软件源时遇到Hash校验和不符的问题</p><p>点击查看：如何更改软件源</p><p>后续需要更改一些配置文件，我比较喜欢用的是 vim（vi增强版，基本用法相同），建议安装一下（如果你实在还不会用 vi/vim 的，请将后面用到 vim 的地方改为 gedit，这样可以使用文本编辑器进行修改，并且每次文件更改完成后请关闭整个 gedit 程序，否则会占用终端）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install vim</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>安装软件时若需要确认，在提示处输入 y 即可。</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-07-apt-install.png" alt="通过命令行安装软件">通过命令行安装软件</p><p>点击查看：vim简单操作指南</p><h2 id="安装SSH、配置SSH无密码登陆"><a href="#安装SSH、配置SSH无密码登陆" class="headerlink" title="安装SSH、配置SSH无密码登陆"></a>安装SSH、配置SSH无密码登陆</h2><p>集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>安装后，可以使用如下命令登陆本机：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>此时会有如下提示(SSH首次登陆提示)，输入 yes 。然后按提示输入密码 hadoop，这样就登陆到本机了。</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-08-ssh-continue.png" alt="SSH首次登陆提示">SSH首次登陆提示</p><p>但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。</p><p>首先退出刚才的 ssh，就回到了我们原先的终端窗口，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exit</span>                           <span class="comment"># 退出刚才的 ssh localhostcd ~/.ssh/                     # 若没有该目录，请先执行一次ssh localhostssh-keygen -t rsa              # 会有提示，都按回车就可以cat ./id_rsa.pub &gt;&gt; ./authorized_keys  # 加入授权</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>~的含义</p><p>在 Linux 系统中，~ 代表的是用户的主文件夹，即 “/home/用户名” 这个目录，如你的用户名为 hadoop，则 ~ 就代表 “/home/hadoop/”。 此外，命令中的 # 后面的文字是注释，只需要输入前面命令即可。</p><p>此时再用 <code>ssh localhost</code> 命令，无需输入密码就可以直接登陆了，如下图所示。</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-09-ssh-localhost.png" alt="SSH无密码登录">SSH无密码登录</p><h2 id="安装Java环境"><a href="#安装Java环境" class="headerlink" title="安装Java环境"></a>安装Java环境</h2><p>Java环境可选择 Oracle 的 JDK，或是 OpenJDK，按中说的，新版本在 OpenJDK 1.7 下是没问题的。为图方便，这边直接通过命令安装 OpenJDK 7。<br>下面有两种安装JDK的方式，可以任选一种，如果第1种失败，就选择第2种。推荐直接使用第2种安装方式。<br>（1）第1种安装JDK方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openjdk-7-jre openjdk-7-jdk</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>安装好 OpenJDK 后，需要找到相应的安装路径，这个路径是用于配置 JAVA_HOME 环境变量的。执行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dpkg -L openjdk-7-jdk | grep <span class="string">'/bin/javac'</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>该命令会输出一个路径，除去路径末尾的 “/bin/javac”，剩下的就是正确的路径了。如输出路径为 /usr/lib/jvm/java-7-openjdk-amd64/bin/javac，则我们需要的路径为 /usr/lib/jvm/java-7-openjdk-amd64。</p><p>接着需要配置一下 JAVA_HOME 环境变量，为方便，我们在 ~/.bashrc 中进行设置（扩展阅读: <span class="exturl" data-url="aHR0cDovL2RibGFiLnhtdS5lZHUuY24vYmxvZy9saW51eC1lbnZpcm9ubWVudC12YXJpYWJsZS8=" title="http://dblab.xmu.edu.cn/blog/linux-environment-variable/">设置Linux环境变量的方法和区别<i class="fa fa-external-link"></i></span>）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>在文件最前面添加如下单独一行（注意 = 号前后不能有空格），将“JDK安装路径”改为上述命令得到的路径，并保存：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=JDK安装路径</span><br></pre></td></tr></table></figure><p>Shell</p><p>如下图所示（该文件原本可能不存在，内容为空，这不影响）：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-10-set-java-home.png" alt="配置JAVA_HOME变量">配置JAVA_HOME变量</p><p>接着还需要让该环境变量生效，执行如下代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc    <span class="comment"># 使变量设置生效</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>设置好后我们来检验一下是否设置正确：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span>     <span class="comment"># 检验变量值java -version$JAVA_HOME/bin/java -version  # 与直接执行 java -version 一样</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>如果设置正确的话，<code>$JAVA_HOME/bin/java -version</code> 会输出 java 的版本信息，且和 <code>java -version</code> 的输出结果一样，如下图所示：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-11-echo-java-home.png" alt="成功配置JAVA_HOME变量">成功配置JAVA_HOME变量</p><p>这样，Hadoop 所需的 Java 运行环境就安装好了。</p><p>（2）第2种安装JDK方式<br>根据大量电脑安装Java环境的情况我们发现，部分电脑按照上述的第一种安装方式会出现安装失败的情况，这时，可以采用这里介绍的另外一种安装方式，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install default-jre default-jdk</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>上述安装过程需要访问网络下载相关文件，请保持联网状态。安装结束以后，需要配置JAVA_HOME环境变量，请在Linux终端中输入下面命令打开当前登录用户的环境变量配置文件.bashrc：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>在文件最前面添加如下单独一行（注意，等号“=”前后不能有空格），然后保存退出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/default-java</span><br></pre></td></tr></table></figure><p>接下来，要让环境变量立即生效，请执行如下代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc    <span class="comment"># 使变量设置生效</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>执行上述命令后，可以检验一下是否设置正确：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span>     <span class="comment"># 检验变量值java -version$JAVA_HOME/bin/java -version  # 与直接执行java -version一样</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>至此，就成功安装了Java环境。下面就可以进入Hadoop的安装。</p><h2 id="安装-Hadoop-2"><a href="#安装-Hadoop-2" class="headerlink" title="安装 Hadoop 2"></a>安装 Hadoop 2</h2><p>Hadoop 2 可以通过 <span class="exturl" data-url="aHR0cDovL21pcnJvci5iaXQuZWR1LmNuL2FwYWNoZS9oYWRvb3AvY29tbW9uLw==" title="http://mirror.bit.edu.cn/apache/hadoop/common/">http://mirror.bit.edu.cn/apache/hadoop/common/<i class="fa fa-external-link"></i></span> 或者 <span class="exturl" data-url="aHR0cDovL21pcnJvcnMuY25uaWMuY24vYXBhY2hlL2hhZG9vcC9jb21tb24v" title="http://mirrors.cnnic.cn/apache/hadoop/common/">http://mirrors.cnnic.cn/apache/hadoop/common/<i class="fa fa-external-link"></i></span> 下载，一般选择下载最新的稳定版本，即下载 “stable” 下的 <strong>hadoop-2.x.y.tar.gz</strong> 这个格式的文件，这是编译好的，另一个包含 src 的则是 Hadoop 源代码，需要进行编译才可使用。</p><p>截止到2015年12月9日，Hadoop官方网站已经更新到2.7.1版本。对于2.6.0以上版本的Hadoop，仍可以参照此教程学习，可放心下载官网最新版本的Hadoop。</p><ol><li>如果读者是使用虚拟机方式安装Ubuntu系统的用户，请用虚拟机中的Ubuntu自带firefox浏览器访问本指南，再点击下面的地址，才能把hadoop文件下载虚拟机ubuntu中。请不要使用Windows系统下的浏览器下载，文件会被下载到Windows系统中，虚拟机中的Ubuntu无法访问外部Windows系统的文件，造成不必要的麻烦。</li><li>如果读者是使用双系统方式安装Ubuntu系统的用户，请进去Ubuntu系统，在Ubuntu系统打开firefox浏览器访问本指南，再点击下面的地址下载：<span class="exturl" data-url="aHR0cDovL21pcnJvcnMuaHVzdC5lZHUuY24vYXBhY2hlL2hhZG9vcC9jb21tb24vaGFkb29wLTIuNy4xL2hhZG9vcC0yLjcuMS50YXIuZ3o=" title="http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz">hadoop-2.7.1下载地址<i class="fa fa-external-link"></i></span></li></ol><p>下载完 Hadoop 文件后一般就可以直接使用。但是如果网络不好，可能会导致下载的文件缺失，可以使用 md5 等检测工具可以校验文件是否完整。</p><p>点击查看：如何校验下载的文件是否完整</p><p>我们选择将 Hadoop 安装至 /usr/local/ 中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf ~/下载/hadoop-2.6.0.tar.gz -C /usr/<span class="built_in">local</span>    <span class="comment"># 解压到/usr/local中cd /usr/local/sudo mv ./hadoop-2.6.0/ ./hadoop            # 将文件夹名改为hadoopsudo chown -R hadoop ./hadoop       # 修改文件权限</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>Hadoop 解压后即可使用。输入如下命令来检查 Hadoop 是否可用，成功则会显示 Hadoop 版本信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop./bin/hadoop version</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>相对路径与绝对路径</p><p>请务必注意命令中的相对路径与绝对路径，本文后续出现的 <code>./bin/...</code>，<code>./etc/...</code> 等包含 ./ 的路径，均为相对路径，以 /usr/local/hadoop 为当前目录。例如在 /usr/local/hadoop 目录中执行 <code>./bin/hadoop version</code> 等同于执行 <code>/usr/local/hadoop/bin/hadoop version</code>。可以将相对路径改成绝对路径来执行，但如果你是在主文件夹 ~ 中执行 <code>./bin/hadoop version</code>，执行的会是 <code>/home/hadoop/bin/hadoop version</code>，就不是我们所想要的了。</p><h2 id="Hadoop单机配置-非分布式"><a href="#Hadoop单机配置-非分布式" class="headerlink" title="Hadoop单机配置(非分布式)"></a>Hadoop单机配置(非分布式)</h2><p>Hadoop 默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。</p><p>现在我们可以执行例子来感受下 Hadoop 的运行。Hadoop 附带了丰富的例子（运行 <code>./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar</code> 可以看到所有例子），包括 wordcount、terasort、join、grep 等。</p><p>在此我们选择运行 grep 例子，我们将 input 文件夹中的所有文件作为输入，筛选当中符合正则表达式 dfs[a-z.]+ 的单词并统计出现的次数，最后输出结果到 output 文件夹中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoopmkdir ./inputcp ./etc/hadoop/*.xml ./input   <span class="comment"># 将配置文件作为输入文件./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output 'dfs[a-z.]+'cat ./output/*          # 查看运行结果</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>执行成功后如下所示，输出了作业的相关信息，输出的结果是符合正则的单词 dfsadmin 出现了1次</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-13-grep-output.png" alt="Hadoop单机模式运行grep的输出结果">Hadoop单机模式运行grep的输出结果</p><p><strong>注意</strong>，Hadoop 默认不会覆盖结果文件，因此再次运行上面实例会提示出错，需要先将 <code>./output</code> 删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -r ./output</span><br></pre></td></tr></table></figure><p>Shell 命令</p><h2 id="Hadoop伪分布式配置"><a href="#Hadoop伪分布式配置" class="headerlink" title="Hadoop伪分布式配置"></a>Hadoop伪分布式配置</h2><p>Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。</p><p>Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件 <strong>core-site.xml</strong> 和 <strong>hdfs-site.xml</strong> 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。</p><p>修改配置文件 <strong>core-site.xml</strong> (通过 gedit 编辑会比较方便: <code>gedit ./etc/hadoop/core-site.xml</code>)，将当中的</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>XML</p><p>修改为下面配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span>    <span class="tag">&lt;<span class="name">property</span>&gt;</span>        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span>        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span>    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>    <span class="tag">&lt;<span class="name">property</span>&gt;</span>        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span>        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    <span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>XML</p><p>同样的，修改配置文件 <strong>hdfs-site.xml</strong>：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span>    <span class="tag">&lt;<span class="name">property</span>&gt;</span>        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span>        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>    <span class="tag">&lt;<span class="name">property</span>&gt;</span>        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>    <span class="tag">&lt;<span class="name">property</span>&gt;</span>        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    <span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>XML</p><p>Hadoop配置文件说明</p><p>Hadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。</p><p>此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 /tmp/hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。</p><p>配置完成后，执行 NameNode 的格式化:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>成功的话，会看到 “successfully formatted” 和 “Exitting with status 0” 的提示，若为 “Exitting with status 1” 则是出错。</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-14-namenode-format.png" alt="执行namenode格式化">执行namenode格式化</p><p>如果在这一步时提示 <strong>Error: JAVA_HOME is not set and could not be found.</strong> 的错误，则说明之前设置 JAVA_HOME 环境变量那边就没设置好，请按教程先设置好 JAVA_HOME 变量，否则后面的过程都是进行不下去的。如果已经按照前面教程在.bashrc文件中设置了JAVA_HOME，还是出现 <strong>Error: JAVA_HOME is not set and could not be found.</strong> 的错误，那么，请到hadoop的安装目录修改配置文件“/usr/local/hadoop/etc/hadoop/hadoop-env.sh”，在里面找到“export JAVA_HOME=${JAVA_HOME}”这行，然后，把它修改成JAVA安装路径的具体地址，比如，“export JAVA_HOME=/usr/lib/jvm/default-java”，然后，再次启动Hadoop。</p><p>接着开启 NameNode 和 DataNode 守护进程。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-dfs.sh  <span class="comment">#start-dfs.sh是个完整的可执行文件，中间没有空格</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>若出现如下SSH提示，输入yes即可。</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-15-ssh-continue.png" alt="启动Hadoop时的SSH提示">启动Hadoop时的SSH提示</p><p>启动时可能会出现如下 WARN 提示：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable WARN 提示可以忽略，并不会影响正常使用。</p><p>启动 Hadoop 时提示 Could not resolve hostname</p><p>如果启动 Hadoop 时遇到输出非常多“ssh: Could not resolve hostname xxx”的异常情况，如下图所示：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-15-resolve-hostname.png" alt="启动Hadoop时的异常提示">启动Hadoop时的异常提示</p><p>这个并不是 ssh 的问题，可通过设置 Hadoop 环境变量来解决。首先按键盘的 <strong>ctrl + c</strong> 中断启动，然后在 ~/.bashrc 中，增加如下两行内容（设置过程与 JAVA_HOME 变量一样，其中 HADOOP_HOME 为 Hadoop 的安装目录）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/local/hadoopexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br></pre></td></tr></table></figure><p>Shell</p><p>保存后，务必执行 <code>source ~/.bashrc</code> 使变量设置生效，然后再次执行 <code>./sbin/start-dfs.sh</code> 启动 Hadoop。</p><p>启动完成后，可以通过命令 <code>jps</code> 来判断是否成功启动，若成功启动则会列出如下进程: “NameNode”、”DataNode” 和 “SecondaryNameNode”（如果 SecondaryNameNode 没有启动，请运行 sbin/stop-dfs.sh 关闭进程，然后再次尝试启动尝试）。如果没有 NameNode 或 DataNode ，那就是配置不成功，请仔细检查之前步骤，或通过查看启动日志排查原因。</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-16-jps.png" alt="通过jps查看启动的Hadoop进程">通过jps查看启动的Hadoop进程</p><p>Hadoop无法正常启动的解决方法</p><p>一般可以查看启动日志来排查原因，注意几点：</p><ul><li>启动时会提示形如 “DBLab-XMU: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-DBLab-XMU.out”，其中 DBLab-XMU 对应你的机器名，但其实启动日志信息是记录在 /usr/local/hadoop/logs/hadoop-hadoop-namenode-DBLab-XMU.log 中，所以应该查看这个后缀为 <strong>.log</strong> 的文件；</li><li>每一次的启动日志都是追加在日志文件之后，所以得拉到最后面看，对比下记录的时间就知道了。</li><li>一般出错的提示在最后面，通常是写着 Fatal、Error、Warning 或者 Java Exception 的地方。</li><li>可以在网上搜索一下出错信息，看能否找到一些相关的解决方法。</li></ul><p>此外，<strong>若是 DataNode 没有启动</strong>，可尝试如下的方法（注意这会删除 HDFS 中原有的所有数据，如果原有的数据很重要请不要这样做）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 针对 DataNode 没法启动的解决方法./sbin/stop-dfs.sh   # 关闭rm -r ./tmp     # 删除 tmp 文件，注意这会删除 HDFS 中原有的所有数据./bin/hdfs namenode -format   # 重新格式化 NameNode./sbin/start-dfs.sh  # 重启</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>成功启动后，可以访问 Web 界面 <span class="exturl" data-url="aHR0cDovL2xvY2FsaG9zdDo1MDA3MC8=" title="http://localhost:50070/">http://localhost:50070<i class="fa fa-external-link"></i></span> 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-17-web-ui.png" alt="Hadoop的Web界面">Hadoop的Web界面</p><h2 id="运行Hadoop伪分布式实例"><a href="#运行Hadoop伪分布式实例" class="headerlink" title="运行Hadoop伪分布式实例"></a>运行Hadoop伪分布式实例</h2><p>上面的单机模式，grep 例子读取的是本地数据，伪分布式读取的则是 HDFS 上的数据。要使用 HDFS，首先需要在 HDFS 中创建用户目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -mkdir -p /user/hadoop</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>注意</p><p>教材《大数据技术原理与应用》的命令是以”./bin/hadoop dfs”开头的Shell命令方式，实际上有三种shell命令方式。<br>\1. hadoop fs<br>\2. hadoop dfs<br>\3. hdfs dfs</p><p>hadoop fs适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统<br>hadoop dfs只能适用于HDFS文件系统<br>hdfs dfs跟hadoop dfs的命令作用一样，也只能适用于HDFS文件系统</p><p>接着将 ./etc/hadoop 中的 xml 文件作为输入文件复制到分布式文件系统中，即将 /usr/local/hadoop/etc/hadoop 复制到分布式文件系统中的 /user/hadoop/input 中。我们使用的是 hadoop 用户，并且已创建相应的用户目录 /user/hadoop ，因此在命令中就可以使用相对路径如 input，其对应的绝对路径就是 /user/hadoop/input:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -mkdir input./bin/hdfs dfs -put ./etc/hadoop/*.xml input</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>复制完成后，可以通过如下命令查看文件列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -ls input</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件（可以将单机步骤中创建的本地 input 文件夹，输出结果 output 文件夹都删掉来验证这一点）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output <span class="string">'dfs[a-z.]+'</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>查看运行结果的命令（查看的是位于 HDFS 中的输出结果）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -cat output/*</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>结果如下，注意到刚才我们已经更改了配置文件，所以运行结果不同。</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2014/08/install-hadoop-18-grep-output.png" alt="Hadoop伪分布式运行grep结果">Hadoop伪分布式运行grep结果</p><p>我们也可以将运行结果取回到本地：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -r ./output    <span class="comment"># 先删除本地的 output 文件夹（如果存在）./bin/hdfs dfs -get output ./output     # 将 HDFS 上的 output 文件夹拷贝到本机cat ./output/*</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>Hadoop 运行程序时，输出目录不能存在，否则会提示错误 “org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/hadoop/output already exists” ，因此若要再次执行，需要执行如下命令删除 output 文件夹:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -rm -r output    <span class="comment"># 删除 output 文件夹</span></span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>运行程序时，输出目录不能存在</p><p>运行 Hadoop 程序时，为了防止覆盖结果，程序指定的输出目录（如 output）不能存在，否则会提示错误，因此运行前需要先删除输出目录。在实际开发应用程序时，可考虑在程序中加上如下代码，能在每次运行时自动删除输出目录，避免繁琐的命令行操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();Job job = <span class="keyword">new</span> Job(conf); <span class="comment">/* 删除输出目录 */</span>Path outputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);outputPath.getFileSystem(conf).delete(outputPath, <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure><p>Java</p><p>若要关闭 Hadoop，则运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure><p>Shell 命令</p><p>注意</p><p>下次启动 hadoop 时，无需进行 NameNode 的初始化，只需要运行 <code>./sbin/start-dfs.sh</code> 就可以！</p><h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><p>YARN 是 Hadoop 2.x 中的内容，使用林子雨编写的<span class="exturl" data-url="aHR0cDovL2RibGFiLnhtdS5lZHUuY24vcG9zdC9iaWdkYXRhLw==" title="http://dblab.xmu.edu.cn/post/bigdata/">大数据技术原理与应用<i class="fa fa-external-link"></i></span>教材的读者，可不用学习YARN内容。</p><p>如果对这方便的内容感兴趣，可点击下方查看。</p><p>点击查看：启动YARN</p><p>自此，你已经掌握 Hadoop 的配置和基本使用了。安装好的Hadoop项目中已经包含了第三章的HDFS，继续学习第3章HDFS文件系统，请参考如下学习指南：<span class="exturl" data-url="aHR0cDovL2RibGFiLnhtdS5lZHUuY24vYmxvZy8yOTAtMi8=" title="http://dblab.xmu.edu.cn/blog/290-2/">大数据技术原理与应用 第三章 学习指南<i class="fa fa-external-link"></i></span></p><h2 id="附加教程-配置PATH环境变量"><a href="#附加教程-配置PATH环境变量" class="headerlink" title="附加教程: 配置PATH环境变量"></a>附加教程: 配置PATH环境变量</h2><p>在这里额外讲一下 PATH 这个环境变量（可执行 <code>echo $PATH</code> 查看，当中包含了多个目录）。例如我们在主文件夹 ~ 中执行 <code>ls</code> 这个命令时，实际执行的是 <code>/bin/ls</code> 这个程序，而不是 <code>~/ls</code> 这个程序。系统是根据 PATH 这个环境变量中包含的目录位置，逐一进行查找，直至在这些目录位置下找到匹配的程序（若没有匹配的则提示该命令不存在）。</p><p>上面的教程中，我们都是先进入到 /usr/local/hadoop 目录中，再执行 <code>sbin/hadoop</code>，实际上等同于运行 <code>/usr/local/hadoop/sbin/hadoop</code>。我们可以将 Hadoop 命令的相关目录加入到 PATH 环境变量中，这样就可以直接通过 <code>start-dfs.sh</code> 开启 Hadoop，也可以直接通过 <code>hdfs</code> 访问 HDFS 的内容，方便平时的操作。</p><p>同样我们选择在 ~/.bashrc 中进行设置（<code>vim ~/.bashrc</code>，与 JAVA_HOME 的设置相似），在文件最前面加入如下单独一行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/usr/local/hadoop/sbin:/usr/local/hadoop/bin</span><br></pre></td></tr></table></figure><p>添加后执行 <code>source ~/.bashrc</code> 使设置生效，生效后，在任意目录中，都可以直接使用 <code>hdfs</code> 等命令了，读者不妨现在就执行 <code>hdfs dfs -ls input</code> 查看 HDFS 文件试试看。</p><h2 id="安装Hadoop集群"><a href="#安装Hadoop集群" class="headerlink" title="安装Hadoop集群"></a>安装Hadoop集群</h2><p>在平时的学习中，我们使用伪分布式就足够了。如果需要安装 Hadoop 集群，请查看<span class="exturl" data-url="aHR0cDovL2RibGFiLnhtdS5lZHUuY24vYmxvZy9pbnN0YWxsLWhhZG9vcC1jbHVzdGVyLw==" title="http://dblab.xmu.edu.cn/blog/install-hadoop-cluster/">Hadoop集群安装配置教程<i class="fa fa-external-link"></i></span>。</p><h2 id="相关教程"><a href="#相关教程" class="headerlink" title="相关教程"></a>相关教程</h2><ul><li><span class="exturl" data-url="aHR0cDovL2RibGFiLnhtdS5lZHUuY24vYmxvZy9oYWRvb3AtYnVpbGQtcHJvamVjdC11c2luZy1lY2xpcHNlLw==" title="http://dblab.xmu.edu.cn/blog/hadoop-build-project-using-eclipse/">使用Eclipse编译运行MapReduce程序<i class="fa fa-external-link"></i></span>: 使用 Eclipse 可以方便的开发、运行 MapReduce 程序，还可以直接管理 HDFS 中的文件。</li><li><span class="exturl" data-url="aHR0cDovL2RibGFiLnhtdS5lZHUuY24vYmxvZy9oYWRvb3AtYnVpbGQtcHJvamVjdC1ieS1zaGVsbC8=" title="http://dblab.xmu.edu.cn/blog/hadoop-build-project-by-shell/">使用命令行编译打包运行自己的MapReduce程序<i class="fa fa-external-link"></i></span>: 有时候需要直接通过命令来编译、打包 MapReduce 程序。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL3d3dy5jbmJsb2dzLmNvbS94aWE1MjBwaS9hcmNoaXZlLzIwMTIvMDUvMTYvMjUwMzk0OS5odG1s" title="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html">http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL3d3dy5taWNtaXUuY29tL2JpZ2RhdGEvaGFkb29wL2hhZG9vcC0yeC11YnVudHUtYnVpbGQv" title="http://www.micmiu.com/bigdata/hadoop/hadoop-2x-ubuntu-build/">http://www.micmiu.com/bigdata/hadoop/hadoop-2x-ubuntu-build/<i class="fa fa-external-link"></i></span></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;XShell–SSH端口转发&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MDM5MzE1L2FydGljbGUvZGV0YWlscy83NzUxMDkyMw==&quot; title=&quot;htt
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>什么是大数据</title>
    <link href="https://tangguangen.com/2018/09/27/%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    <id>https://tangguangen.com/2018/09/27/什么是大数据/</id>
    <published>2018-09-26T23:47:13.000Z</published>
    <updated>2018-09-27T00:19:58.972Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>根据IBM前首席执行官郭士纳的观点，IT领域每隔十五年就会迎来一次重大变革。</p><img src="/2018/09/27/什么是大数据/第三次信息化浪潮.PNG"><p>物联网改变了数据产生的方式，迎来了大数据时代。</p><p>大数据时代的技术支撑：存储、计算、网络能力的大幅提升。</p><p>大数据的四个特性：4V</p><ul><li><strong>数据量大</strong>：大数据摩尔定律、人类在最近两年产生的数据量相当于之前产生的全部数据量；</li><li><strong>数据类型多</strong>：大数据是由结构化数据和非结构化数据组成，90%都是非结构化数据，结构化数据是存储在关系型数据库中的数据。</li><li><strong>数据处理速度快</strong></li><li><strong>数据价值密度低</strong>，商业价值高</li></ul><h2 id="大数据的影响"><a href="#大数据的影响" class="headerlink" title="大数据的影响"></a>大数据的影响</h2><p>Jim Gray博士，98年图灵奖得主，提出了事务理论</p><p>四种研究方式：</p><ul><li>实验，通过实验解决科学问题。</li><li>理论</li><li>计算</li><li>数据，以数据为驱动的全新的科学研究时代。通过对大量数据的分析找出问题。考数据驱动，发现问题，解决问题。</li></ul><p>在思维方式方面大数据完全颠覆了传统的思维方式</p><ul><li>全样而非抽样，不需要像以前一样做抽样</li><li>效率而非精确，因为以前是做抽样分析，所以要最前高精确度，使误差尽量小。大数据时代是对全样分析，不纯在误差放大问题，就不需要最求精确度，所以这时候追求的是效率。</li><li>相关而非因果，大数据时代只关注相关性，而不关注因果性。</li></ul><h2 id="大数据关键技术"><a href="#大数据关键技术" class="headerlink" title="大数据关键技术"></a>大数据关键技术</h2><p>两大核心技术：</p><ul><li>分布式存储：解决海量数据的存储问题</li><li>分布式处理：解决海量数据的处理问题</li></ul><p>大数据技术一谷歌公司技术为代表</p><ul><li>分布式数据库Big Table</li><li>分布式文件系统GFS</li><li>分布式并行处理技术MapReduce</li></ul><p>不同的计算模式需要使用不同的产品</p><p>大数据产品服务的计算模式是不一样的，有些是用于批处理，有些是用于实时计算，有些是用于交互式计算。</p><p>批处理计算：</p><ul><li>解决问题：针对大规模数据的批量处理</li><li>MapReduce是批处理计算模式的典型代表，适用于批处理，不适用于实时计算</li><li>15年异军突起的产品Spark，实时性比MapReduce更好，而且它解决了MapReduce当中的一些缺点，MapReduce无法高效的执行迭代计算，但是Spark可以进行迭代计算。许多应用中需要做迭代计算，比如数据挖掘，这时就不能用MapReduce要用Spark</li></ul><p>流计算：</p><ul><li>解决问题：针对流数据的实时计算</li><li>流计算是专门针对流数据的实时计算，流数据需要实时处理，给出实时响应，否则分析结果就会失去商业价值，只能用流计算框架进行处理，它是实现秒级的针对实时数据流的响应。</li><li>流计算代表产品：S4，Storm，Flume</li></ul><p>图计算：</p><ul><li>解决问题：针对大规模图结构数据的处理</li><li>图计算代表软件：Google Pregel</li><li>社交网络数据就是图结构数据</li></ul><p>查询分析计算：</p><ul><li>解决问题：大规模数据的存储管理和查询分析</li><li>大数据查询分析软件，满足交互式查询分析、</li><li>代表产品：Google Dremel，Hive，Cassandra，Impala</li></ul><p><strong>云计算</strong></p><p>云计算解决两大核心问题：</p><ul><li>分布式存储</li><li>分布式处理</li></ul><p>云计算电信特征：</p><ul><li>虚拟化</li><li>多租户</li></ul><p>云计算的概念：云计算是通过网络以服务的方式为用户提供非常廉价的IT资源</p><p>云计算的优势：企业不需要自建IT基础设施，可以租用云端资源</p><p>云计算的三种模式：</p><p>公有云：百度云</p><p>私有云</p><p>混合云</p><p>自底向上的三种服务：</p><ul><li>IaaS<ul><li>将基础设施（计算资源和存储）作为服务出租，如弹性云计算EC2</li><li>面向网络架构师</li></ul></li><li>PaaS<ul><li>平台即服务，云计算环境的开发平台</li><li>面向应用开发者</li></ul></li><li>SaaS<ul><li>典型案例：云财务软件</li></ul></li></ul><p>云计算的关键技术：</p><ul><li>虚拟化，</li><li>多租户，云计算同时为多个用户服务</li><li>云计算数据中心</li></ul><p><strong>物联网</strong></p><p>物联网层次架构：感知层，网络层，处理层，应用层</p><p>物联网的关键技术：</p><ul><li>识别技术</li><li>感知技术</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;根据IBM前首席执行官郭士纳的观点，IT领域每隔十五年就会迎来一次重大变革。&lt;/p&gt;
&lt;img src=&quot;/2018/09/27/什么是大数
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>【转载】hexo框架基于next主题定制</title>
    <link href="https://tangguangen.com/2018/09/06/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91hexo%E6%A1%86%E6%9E%B6%E5%9F%BA%E4%BA%8Enext%E4%B8%BB%E9%A2%98%E5%AE%9A%E5%88%B6/"/>
    <id>https://tangguangen.com/2018/09/06/【转载】hexo框架基于next主题定制/</id>
    <published>2018-09-06T00:13:18.000Z</published>
    <updated>2018-09-06T00:29:47.293Z</updated>
    
    <content type="html"><![CDATA[<p>hexo框架基于next主题深度定制方案</p><p>主要有：</p><ul><li>在右上角或者左上角实现fork me on github</li><li>添加RSS</li><li>背景配置</li><li>添加动态背景</li><li>实现点击出现桃心效果</li><li>修改文章内链接文本样式</li><li>修改文章底部的那个带#号的标签</li><li>在每篇文章末尾统一添加“本文结束”标记</li><li>修改作者头像并旋转</li><li>博文压缩</li><li>修改“代码块自定义样式</li><li>侧边栏社交小图标设置</li><li>主页文章添加阴影效果</li><li>在网站底部加上访问量</li><li>添加热度</li><li>网站底部字数统计</li><li>添加 README.md 文件</li><li>设置网站的图标Favicon</li><li>实现统计功能</li><li>添加顶部加载条</li><li>在文章底部增加版权信息</li><li>添加Gitment评论系统</li><li>隐藏网页底部powered By Hexo / 强力驱动</li><li>修改网页底部的桃心</li><li>文章加密访问</li><li>添加jiathis分享</li><li>博文置顶</li><li>修改字体大小</li><li>修改打赏字体不闪动</li><li>侧边栏推荐阅读</li><li>自定义鼠标样式</li><li>hexo 添加百度站长推送</li><li>hexo NexT主题首页title链接的优化</li><li>Hexo NexT主题修改文章标题样式</li><li>hexo 添加百度站长推送</li></ul><h2 id="在右上角或者左上角实现fork-me-on-github"><a href="#在右上角或者左上角实现fork-me-on-github" class="headerlink" title="在右上角或者左上角实现fork me on github"></a>在右上角或者左上角实现fork me on github</h2><p>具体实现方法<br>点击<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Jsb2cvMjczLWdpdGh1Yi1yaWJib25z" title="https://github.com/blog/273-github-ribbons">这里<i class="fa fa-external-link"></i></span> 挑选自己喜欢的样式，并复制代码。 例如，我是复制如下代码：</p><p>然后粘贴刚才复制的代码到themes/next/layout/_layout.swig文件中放在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>的下面，并把href改为你的github地址</p><h2 id="添加RSS"><a href="#添加RSS" class="headerlink" title="添加RSS"></a>添加RSS</h2><p>具体实现方法<br>切换到你的blog的路径，例如我是在/Users/Hexo/blog这个路径上，也就是在你的根目录下</p><p>然后安装 Hexo 插件：(这个插件会放在node_modules这个文件夹里)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install --save hexo-generator-feed</span><br></pre></td></tr></table></figure><p>接下来打开配置文件</p><p>在里面的末尾添加：(请注意在冒号后面要加一个空格，不然会发生错误！)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Extensions</span><br><span class="line">## Plugins: http://hexo.io/plugins/</span><br><span class="line">plugins: hexo-generate-feed</span><br></pre></td></tr></table></figure><p>然后打开next主题文件夹里面的_config.yml,在里面配置为如下样子：(就是在rss:的后面加上/atom.xml,注意在冒号后面要加一个空格)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Set rss to false to disable feed link.</span><br><span class="line"># Leave rss as empty to use site&apos;s feed link.</span><br><span class="line"># Set rss to specific value if you have burned your feed already.</span><br><span class="line">rss: /atom.xml</span><br></pre></td></tr></table></figure><p>配置完之后运行：<br>$ hexo g</p><p>重新生成一次，你会在./public 文件夹中看到 atom.xml 文件。然后启动服务器查看是否有效，之后再部署到 Github 中。</p><h2 id="背景配置"><a href="#背景配置" class="headerlink" title="背景配置"></a>背景配置</h2><p>背景透明</p><p>博客根目录 themes\next\source\css_schemes\Pisces_layout.styl这个文件的<br>第65行background:删除掉</p><p>按钮背景</p><p>博客根目录 themes\next\source\css_common\components\post\post-button.styl 第七行修 background: ;</p><p>站点概况背景</p><p>博客根目录 themes\next\source\css_schemes\Pisces_sidebar.styl</p><p>菜单栏背景</p><p>next\source\css_schemes\Pisces_layout.styl 文件里.header-inner 这个选择器下的background 就是背景色</p><h2 id="添加动态背景"><a href="#添加动态背景" class="headerlink" title="添加动态背景"></a>添加动态背景</h2><p>具体实现方法<br>修改代码</p><p>打开next/layout/_layout.swig，在之前添加如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if theme.canvas_nest %&#125;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>修改主题配置文件</p><p>打开/next/_config.yml，添加以下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># --------------------------------------------------------------</span><br><span class="line"># background settings</span><br><span class="line"># --------------------------------------------------------------</span><br><span class="line"># add canvas-nest effect</span><br><span class="line"># see detail from https://github.com/hustcc/canvas-nest.js</span><br><span class="line">canvas_nest: true</span><br></pre></td></tr></table></figure><p>运行hexo clean 和 hexo g hexo s之后就可以看到效果了</p><h2 id="实现点击出现桃心效果"><a href="#实现点击出现桃心效果" class="headerlink" title="实现点击出现桃心效果"></a>实现点击出现桃心效果</h2><p>具体实现方法<br>点击这里<span class="exturl" data-url="aHR0cDovLzd1MnNzMS5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS9sb3ZlLmpz" title="http://7u2ss1.com1.z0.glb.clouddn.com/love.js">love.js<i class="fa fa-external-link"></i></span></p><p>然后将里面的代码copy一下，新建love.js文件并且将代码复制进去，然后保存。将love.js文件放到路径/themes/next/source/js/src里面，然后打开\themes\next\layout_layout.swig文件,在末尾（在前面引用会出现找不到的bug）添加以下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 页面点击小红心 --&gt;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><h2 id="修改文章内链接文本样式"><a href="#修改文章内链接文本样式" class="headerlink" title="修改文章内链接文本样式"></a>修改文章内链接文本样式</h2><p>具体实现方法<br>修改文件 themes\next\source\css_common\components\post\post.styl，在末尾添加如下css样式，：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 文章内链接文本样式</span><br><span class="line">.post-body p a&#123;</span><br><span class="line">  color: #0593d3;</span><br><span class="line">  border-bottom: none;</span><br><span class="line">  border-bottom: 1px solid #0593d3;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    color: #fc6423;</span><br><span class="line">    border-bottom: none;</span><br><span class="line">    border-bottom: 1px solid #fc6423;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中选择.post-body 是为了不影响标题，选择 p 是为了不影响首页“阅读全文”的显示样式,颜色可以自己定义。</p><h2 id="修改文章底部的那个带-号的标签"><a href="#修改文章底部的那个带-号的标签" class="headerlink" title="修改文章底部的那个带#号的标签"></a>修改文章底部的那个带#号的标签</h2><p>具体实现方法<br>修改模板/themes/next/layout/_macro/post.swig，搜索 rel=”tag”&gt;#，将 # 换成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;</span><br></pre></td></tr></table></figure><h2 id="在每篇文章末尾统一添加“本文结束”标记"><a href="#在每篇文章末尾统一添加“本文结束”标记" class="headerlink" title="在每篇文章末尾统一添加“本文结束”标记"></a>在每篇文章末尾统一添加“本文结束”标记</h2><p>具体实现方法<br>在路径 \themes\next\layout_macro 中新建 passage-end-tag.swig 文件,并添加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">    &#123;% if not is_index %&#125;</span><br><span class="line">        &lt;div style=&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束&lt;i class=&quot;fa fa-paw&quot;&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>接着打开\themes\next\layout_macro\post.swig文件，在post-body 之后， post-footer 之前添加如下画红色部分代码（post-footer之前两个DIV）：</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include &apos;passage-end-tag.swig&apos; %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>然后打开主题配置文件（_config.yml),在末尾添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 文章末尾添加“本文结束”标记</span><br><span class="line">passage_end_tag:</span><br><span class="line">  enabled: true</span><br></pre></td></tr></table></figure><p>完成以上设置之后，在每篇文章之后都会添加如上效果图的样子。</p><h2 id="修改作者头像并旋转"><a href="#修改作者头像并旋转" class="headerlink" title="修改作者头像并旋转"></a>修改作者头像并旋转</h2><p>具体实现方法</p><p>打开\themes\next\source\css_common\components\sidebar\sidebar-author.styl，在里面添加如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">.site-author-image &#123;</span><br><span class="line">  display: block;</span><br><span class="line">  margin: 0 auto;</span><br><span class="line">  padding: $site-author-image-padding;</span><br><span class="line">  max-width: $site-author-image-width;</span><br><span class="line">  height: $site-author-image-height;</span><br><span class="line">  border: $site-author-image-border-width solid $site-author-image-border-color;</span><br><span class="line"></span><br><span class="line">  /* 头像圆形 */</span><br><span class="line">  border-radius: 80px;</span><br><span class="line">  -webkit-border-radius: 80px;</span><br><span class="line">  -moz-border-radius: 80px;</span><br><span class="line">  box-shadow: inset 0 -1px 0 #333sf;</span><br><span class="line"></span><br><span class="line">  /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 </span><br><span class="line">    (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  /* 鼠标经过头像旋转360度 */</span><br><span class="line">  -webkit-transition: -webkit-transform 1.0s ease-out;</span><br><span class="line">  -moz-transition: -moz-transform 1.0s ease-out;</span><br><span class="line">  transition: transform 1.0s ease-out;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">img:hover &#123;</span><br><span class="line">  /* 鼠标经过停止头像旋转 </span><br><span class="line">  -webkit-animation-play-state:paused;</span><br><span class="line">  animation-play-state:paused;*/</span><br><span class="line"></span><br><span class="line">  /* 鼠标经过头像旋转360度 */</span><br><span class="line">  -webkit-transform: rotateZ(360deg);</span><br><span class="line">  -moz-transform: rotateZ(360deg);</span><br><span class="line">  transform: rotateZ(360deg);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* Z 轴旋转动画 */</span><br><span class="line">@-webkit-keyframes play &#123;</span><br><span class="line">  0% &#123;</span><br><span class="line">    -webkit-transform: rotateZ(0deg);</span><br><span class="line">  &#125;</span><br><span class="line">  100% &#123;</span><br><span class="line">    -webkit-transform: rotateZ(-360deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">@-moz-keyframes play &#123;</span><br><span class="line">  0% &#123;</span><br><span class="line">    -moz-transform: rotateZ(0deg);</span><br><span class="line">  &#125;</span><br><span class="line">  100% &#123;</span><br><span class="line">    -moz-transform: rotateZ(-360deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">@keyframes play &#123;</span><br><span class="line">  0% &#123;</span><br><span class="line">    transform: rotateZ(0deg);</span><br><span class="line">  &#125;</span><br><span class="line">  100% &#123;</span><br><span class="line">    transform: rotateZ(-360deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="博文压缩"><a href="#博文压缩" class="headerlink" title="博文压缩"></a>博文压缩</h2><p>在站点的根目录下执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install gulp -g</span><br><span class="line">$ npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save</span><br></pre></td></tr></table></figure><p>在如下图所示，新建 gulpfile.js ，并填入以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">var gulp = require(&apos;gulp&apos;);</span><br><span class="line">var minifycss = require(&apos;gulp-minify-css&apos;);</span><br><span class="line">var uglify = require(&apos;gulp-uglify&apos;);</span><br><span class="line">var htmlmin = require(&apos;gulp-htmlmin&apos;);</span><br><span class="line">var htmlclean = require(&apos;gulp-htmlclean&apos;);</span><br><span class="line">// 压缩 public 目录 css</span><br><span class="line">gulp.task(&apos;minify-css&apos;, function() &#123;</span><br><span class="line">    return gulp.src(&apos;./public/**/*.css&apos;)</span><br><span class="line">        .pipe(minifycss())</span><br><span class="line">        .pipe(gulp.dest(&apos;./public&apos;));</span><br><span class="line">&#125;);</span><br><span class="line">// 压缩 public 目录 html</span><br><span class="line">gulp.task(&apos;minify-html&apos;, function() &#123;</span><br><span class="line">  return gulp.src(&apos;./public/**/*.html&apos;)</span><br><span class="line">    .pipe(htmlclean())</span><br><span class="line">    .pipe(htmlmin(&#123;</span><br><span class="line">         removeComments: true,</span><br><span class="line">         minifyJS: true,</span><br><span class="line">         minifyCSS: true,</span><br><span class="line">         minifyURLs: true,</span><br><span class="line">    &#125;))</span><br><span class="line">    .pipe(gulp.dest(&apos;./public&apos;))</span><br><span class="line">&#125;);</span><br><span class="line">// 压缩 public/js 目录 js</span><br><span class="line">gulp.task(&apos;minify-js&apos;, function() &#123;</span><br><span class="line">    return gulp.src(&apos;./public/**/*.js&apos;)</span><br><span class="line">        .pipe(uglify())</span><br><span class="line">        .pipe(gulp.dest(&apos;./public&apos;));</span><br><span class="line">&#125;);</span><br><span class="line">// 执行 gulp 命令时执行的任务</span><br><span class="line">gulp.task(&apos;default&apos;, [</span><br><span class="line">    &apos;minify-html&apos;,&apos;minify-css&apos;,&apos;minify-js&apos;</span><br><span class="line">]);</span><br></pre></td></tr></table></figure><p>生成博文是执行 hexo g &amp;&amp; gulp 就会根据 gulpfile.js 中的配置，对 public 目录中的静态资源文件进行压缩。</p><h2 id="修改“代码块自定义样式"><a href="#修改“代码块自定义样式" class="headerlink" title="修改“代码块自定义样式"></a>修改“代码块自定义样式</h2><p>具体实现方法<br>打开\themes\next\source\css_custom\custom.styl,向里面加入：(颜色可以自己定义)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// Custom styles.</span><br><span class="line">code &#123;</span><br><span class="line">    color: #ff7600;</span><br><span class="line">    background: #fbf7f8;</span><br><span class="line">    margin: 2px;</span><br><span class="line">&#125;</span><br><span class="line">// 大代码块的自定义样式</span><br><span class="line">.highlight, pre &#123;</span><br><span class="line">    margin: 5px 0;</span><br><span class="line">    padding: 5px;</span><br><span class="line">    border-radius: 3px;</span><br><span class="line">&#125;</span><br><span class="line">.highlight, code, pre &#123;</span><br><span class="line">    border: 1px solid #d6d6d6;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="侧边栏社交小图标设置"><a href="#侧边栏社交小图标设置" class="headerlink" title="侧边栏社交小图标设置"></a>侧边栏社交小图标设置</h2><p>具体实现方法<br>打开主题配置文件（_config.yml），搜索social_icons:,在图标库找自己喜欢的小图标，并将名字复制在如下位置，保存即可</p><h2 id="主页文章添加阴影效果"><a href="#主页文章添加阴影效果" class="headerlink" title="主页文章添加阴影效果"></a>主页文章添加阴影效果</h2><p>具体实现方法<br>打开\themes\next\source\css_custom\custom.styl,向里面加入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 主页文章添加阴影效果</span><br><span class="line"> .post &#123;</span><br><span class="line">   margin-top: 60px;</span><br><span class="line">   margin-bottom: 60px;</span><br><span class="line">   padding: 25px;</span><br><span class="line">   -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);</span><br><span class="line">   -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="在网站底部加上访问量"><a href="#在网站底部加上访问量" class="headerlink" title="在网站底部加上访问量"></a>在网站底部加上访问量</h2><p>具体实现方法<br>打开\themes\next\layout_partials\footer.swig文件,在copyright前加上画红线这句话：</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>然后再合适的位置添加显示统计的代码，如图：</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;powered-by&quot;&gt;</span><br><span class="line">&lt;i class=&quot;fa fa-user-md&quot;&gt;&lt;/i&gt;&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt;</span><br><span class="line">  本站访客数:&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;</span><br><span class="line">&lt;/span&gt;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>在这里有两中不同计算方式的统计代码：</p><ol><li><p>pv的方式，单个用户连续点击n篇文章，记录n次访问量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;</span><br><span class="line">    本站总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次</span><br><span class="line">&lt;/span&gt;</span><br></pre></td></tr></table></figure></li></ol><p>uv的方式，单个用户连续点击n篇文章，只记录1次访客数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt;</span><br><span class="line">  本站总访问量&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;次</span><br><span class="line">&lt;/span&gt;</span><br></pre></td></tr></table></figure><p>添加之后再执行hexo d -g，然后再刷新页面就能看到效果</p><h2 id="添加热度"><a href="#添加热度" class="headerlink" title="添加热度"></a>添加热度</h2><p>具体实现方法<br>next主题集成leanCloud，打开/themes/next/layout/_macro/post.swig,在画红线的区域添加℃：</p><p>然后打开，/themes/next/languages/zh-Hans.yml,将画红框的改为热度就可以了</p><h2 id="网站底部字数统计"><a href="#网站底部字数统计" class="headerlink" title="网站底部字数统计"></a>网站底部字数统计</h2><p>具体方法实现<br>切换到根目录下，然后运行如下代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-wordcount --save</span><br></pre></td></tr></table></figure><p>然后在/themes/next/layout/_partials/footer.swig文件尾部加上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;theme-info&quot;&gt;</span><br><span class="line">  &lt;div class=&quot;powered-by&quot;&gt;&lt;/div&gt;</span><br><span class="line">  &lt;span class=&quot;post-count&quot;&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><h2 id="添加-README-md-文件"><a href="#添加-README-md-文件" class="headerlink" title="添加 README.md 文件"></a>添加 README.md 文件</h2><p>每个项目下一般都有一个 README.md 文件，但是使用 hexo 部署到仓库后，项目下是没有 README.md 文件的。<br>在 Hexo 目录下的 source 根目录下添加一个 README.md 文件，修改站点配置文件 _config.yml，将 skip_render 参数的值设置为<br>skip_render: README.md</p><p>保存退出即可。再次使用 hexo d 命令部署博客的时候就不会在渲染 README.md 这个文件了。</p><h2 id="设置网站的图标Favicon"><a href="#设置网站的图标Favicon" class="headerlink" title="设置网站的图标Favicon"></a>设置网站的图标Favicon</h2><p>具体方法实现<br>在EasyIcon中找一张（32*32）的ico图标,或者去别的网站下载或者制作，并将图标名称改为favicon.ico，然后把图标放在/themes/next/source/images里，并且修改主题配置文件：<br># Put your favicon.ico into <code>hexo-site/source/</code> directory.<br>favicon: /favicon.ico</p><h2 id="实现统计功能"><a href="#实现统计功能" class="headerlink" title="实现统计功能"></a>实现统计功能</h2><p>具体实现方法<br>在根目录下安装 hexo-wordcount,运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-wordcount --save</span><br></pre></td></tr></table></figure><p>然后在主题的配置文件中，配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Post wordcount display settings</span><br><span class="line"># Dependencies: https://github.com/willin/hexo-wordcount</span><br><span class="line">post_wordcount:</span><br><span class="line">  item_text: true</span><br><span class="line">  wordcount: true</span><br><span class="line">  min2read: true</span><br></pre></td></tr></table></figure><h2 id="添加顶部加载条"><a href="#添加顶部加载条" class="headerlink" title="添加顶部加载条"></a>添加顶部加载条</h2><p>具体实现方法<br>打开/themes/next/layout/_partials/head.swig文件，添加红框上的代码</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=&quot;//cdn.bootcss.com/pace/1.0.2/pace.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&lt;link href=&quot;//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css&quot; rel=&quot;stylesheet&quot;&gt;</span><br></pre></td></tr></table></figure><p>但是，默认的是粉色的，要改变颜色可以在/themes/next/layout/_partials/head.swig文件中添加如下代码（接在刚才link的后面）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;style&gt;</span><br><span class="line">    .pace .pace-progress &#123;</span><br><span class="line">        background: #1E92FB; /*进度条颜色*/</span><br><span class="line">        height: 3px;</span><br><span class="line">    &#125;</span><br><span class="line">    .pace .pace-progress-inner &#123;</span><br><span class="line">         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/</span><br><span class="line">    &#125;</span><br><span class="line">    .pace .pace-activity &#123;</span><br><span class="line">        border-top-color: #1E92FB;    /*上边框颜色*/</span><br><span class="line">        border-left-color: #1E92FB;    /*左边框颜色*/</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure><p>目前，博主的增加顶部加载条的pull request 已被Merge<br>现在升级最新版的next主题，升级后只需修改主题配置文件(_config.yml)将pace: false改为pace: true就行了，你还可以换不同样式的加载条.</p><h2 id="在文章底部增加版权信息"><a href="#在文章底部增加版权信息" class="headerlink" title="在文章底部增加版权信息"></a>在文章底部增加版权信息</h2><p>在目录 next/layout/_macro/下添加 my-copyright.swig：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if page.copyright %&#125;</span><br><span class="line">&lt;div class=&quot;my_post_copyright&quot;&gt;</span><br><span class="line">  &lt;script src=&quot;//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- JS库 sweetalert 可修改路径 --&gt;</span><br><span class="line">  &lt;script type=&quot;text/javascript&quot; src=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;script src=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css&quot;&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot;&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=&quot;/&quot; title=&quot;访问 &#123;&#123; theme.author &#125;&#125; 的个人博客&quot;&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title=&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt;</span><br><span class="line">    &lt;span class=&quot;copy-path&quot;  title=&quot;点击复制文章链接&quot;&gt;&lt;i class=&quot;fa fa-clipboard&quot; data-clipboard-text=&quot;&#123;&#123; page.permalink &#125;&#125;&quot;  aria-label=&quot;复制成功！&quot;&gt;&lt;/i&gt;&lt;/span&gt;</span><br><span class="line">  &lt;/p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=&quot;fa fa-creative-commons&quot;&gt;&lt;/i&gt; &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; target=&quot;_blank&quot; title=&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt;  </span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&lt;script&gt; </span><br><span class="line">    var clipboard = new Clipboard(&apos;.fa-clipboard&apos;);</span><br><span class="line">    clipboard.on(&apos;success&apos;, $(function()&#123;</span><br><span class="line">      $(&quot;.fa-clipboard&quot;).click(function()&#123;</span><br><span class="line">        swal(&#123;   </span><br><span class="line">          title: &quot;&quot;,   </span><br><span class="line">          text: &apos;复制成功&apos;,   </span><br><span class="line">          html: false,</span><br><span class="line">          timer: 500,   </span><br><span class="line">          showConfirmButton: false</span><br><span class="line">        &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;));  </span><br><span class="line">&lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>在目录next/source/css/_common/components/post/下添加my-post-copyright.styl：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">.my_post_copyright &#123;</span><br><span class="line">  width: 85%;</span><br><span class="line">  max-width: 45em;</span><br><span class="line">  margin: 2.8em auto 0;</span><br><span class="line">  padding: 0.5em 1.0em;</span><br><span class="line">  border: 1px solid #d3d3d3;</span><br><span class="line">  font-size: 0.93rem;</span><br><span class="line">  line-height: 1.6em;</span><br><span class="line">  word-break: break-all;</span><br><span class="line">  background: rgba(255,255,255,0.4);</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright p&#123;margin:0;&#125;</span><br><span class="line">.my_post_copyright span &#123;</span><br><span class="line">  display: inline-block;</span><br><span class="line">  width: 5.2em;</span><br><span class="line">  color: #b5b5b5;</span><br><span class="line">  font-weight: bold;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .raw &#123;</span><br><span class="line">  margin-left: 1em;</span><br><span class="line">  width: 5em;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright a &#123;</span><br><span class="line">  color: #808080;</span><br><span class="line">  border-bottom:0;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright a:hover &#123;</span><br><span class="line">  color: #a3d2a3;</span><br><span class="line">  text-decoration: underline;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright:hover .fa-clipboard &#123;</span><br><span class="line">  color: #000;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .post-url:hover &#123;</span><br><span class="line">  font-weight: normal;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .copy-path &#123;</span><br><span class="line">  margin-left: 1em;</span><br><span class="line">  width: 1em;</span><br><span class="line">  +mobile()&#123;display:none;&#125;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .copy-path:hover &#123;</span><br><span class="line">  color: #808080;</span><br><span class="line">  cursor: pointer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改next/layout/_macro/post.swig，在代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">      &#123;% if not is_index %&#125;</span><br><span class="line">        &#123;% include &apos;wechat-subscriber.swig&apos; %&#125;</span><br><span class="line">      &#123;% endif %&#125;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>之前添加增加如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">      &#123;% if not is_index %&#125;</span><br><span class="line">        &#123;% include &apos;my-copyright.swig&apos; %&#125;</span><br><span class="line">      &#123;% endif %&#125;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>如下：</p><p>修改next/source/css/_common/components/post/post.styl文件，在最后一行增加代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@import &quot;my-post-copyright&quot;</span><br></pre></td></tr></table></figure><p>保存重新生成即可。<br>如果要在该博文下面增加版权信息的显示，需要在 Markdown 中增加 copyright: true 的设置，类似：<br>小技巧：如果你觉得每次都要输入 copyright: true 很麻烦的话,那么在 /scaffolds/post.md 文件中添加：</p><p>这样每次hexo new “你的内容”之后，生成的md文件会自动把copyright:加到里面去<br>(注意：如果解析出来之后，你的原始链接有问题：如：<span class="exturl" data-url="aHR0cDovL3lvdXJzaXRlLmNvbS8lRTUlODklOEQlRTclQUIlQUYlRTUlQjAlOEYlRTklQTElQjklRTclOUIlQUUlRUYlQkMlOUElRTQlQkQlQkYlRTclOTQlQThjYW52YXMlRTclQkIlOTglRTclOTQlQkIlRTUlOTMlODYlRTUlOTUlQTZBJUU2JUEyJUE2Lmh0bWwsJUU5JTgyJUEzJUU0JUI5JTg4JUU1JTlDJUE4JUU2JUEwJUI5JUU3JTlCJUFFJUU1JUJEJTk1JUU0JUI4JThCX2NvbmZpZy55bWwlRTQlQjglQUQlRTUlODYlOTklRTYlODglOTAlRTclQjElQkIlRTQlQkMlQkMlRTglQkYlOTklRTYlQTAlQjclRUYlQkMlOUElRUYlQkMlODk=" title="http://yoursite.com/%E5%89%8D%E7%AB%AF%E5%B0%8F%E9%A1%B9%E7%9B%AE%EF%BC%9A%E4%BD%BF%E7%94%A8canvas%E7%BB%98%E7%94%BB%E5%93%86%E5%95%A6A%E6%A2%A6.html,%E9%82%A3%E4%B9%88%E5%9C%A8%E6%A0%B9%E7%9B%AE%E5%BD%95%E4%B8%8B_config.yml%E4%B8%AD%E5%86%99%E6%88%90%E7%B1%BB%E4%BC%BC%E8%BF%99%E6%A0%B7%EF%BC%9A%EF%BC%89">http://yoursite.com/前端小项目：使用canvas绘画哆啦A梦.html,那么在根目录下_config.yml中写成类似这样：）<i class="fa fa-external-link"></i></span><br>就行了。</p><h2 id="添加Gitment评论系统"><a href="#添加Gitment评论系统" class="headerlink" title="添加Gitment评论系统"></a>添加Gitment评论系统</h2><p>具体实现方法见 <span class="exturl" data-url="aHR0cDovL3d3dy5haXN1bi5vcmcvMjAxNy8wOS9oZXhvK25leHQrZ2l0bWVudC8=" title="http://www.aisun.org/2017/09/hexo+next+gitment/">为 hexo NexT 添加 Gitment 评论插件<i class="fa fa-external-link"></i></span></p><h2 id="隐藏网页底部powered-By-Hexo-强力驱动"><a href="#隐藏网页底部powered-By-Hexo-强力驱动" class="headerlink" title="隐藏网页底部powered By Hexo / 强力驱动"></a>隐藏网页底部powered By Hexo / 强力驱动</h2><p>打开themes/next/layout/_partials/footer.swig,使用””隐藏之间的代码即可，或者直接删除。</p><h2 id="修改网页底部的桃心"><a href="#修改网页底部的桃心" class="headerlink" title="修改网页底部的桃心"></a>修改网页底部的桃心</h2><p>还是打开themes/next/layout/_partials/footer.swig，找到：<br>，然后还是在图标库中找到你自己喜欢的图标，然后修改画红线的部分就可以了。</p><h2 id="文章加密访问"><a href="#文章加密访问" class="headerlink" title="文章加密访问"></a>文章加密访问</h2><p>具体实现方法<br>打开themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig文件,在以下位置插入这样一段代码：</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">    (function()&#123;</span><br><span class="line">        if(&apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123;</span><br><span class="line">            if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123;</span><br><span class="line">                alert(&apos;密码错误！&apos;);</span><br><span class="line">                history.back();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)();</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure><h2 id="添加jiathis分享"><a href="#添加jiathis分享" class="headerlink" title="添加jiathis分享"></a>添加jiathis分享</h2><p>在主题配置文件中，jiathis为true，就行了，如下图</p><p>默认是这样子的：</p><p>如果你想自定义话，打开themes/next/layout/_partials/share/jiathis.swig修改画红线部分就可以了</p><h2 id="博文置顶"><a href="#博文置顶" class="headerlink" title="博文置顶"></a>博文置顶</h2><p>修改 hero-generator-index 插件，把文件：node_modules/hexo-generator-index/lib/generator.js 内的代码替换为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&apos;use strict&apos;;</span><br><span class="line">var pagination = require(&apos;hexo-pagination&apos;);</span><br><span class="line">module.exports = function(locals)&#123;</span><br><span class="line">  var config = this.config;</span><br><span class="line">  var posts = locals.posts;</span><br><span class="line">    posts.data = posts.data.sort(function(a, b) &#123;</span><br><span class="line">        if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义</span><br><span class="line">            if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排</span><br><span class="line">            else return b.top - a.top; // 否则按照top值降序排</span><br><span class="line">        &#125;</span><br><span class="line">        else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span><br><span class="line">            return -1;</span><br><span class="line">        &#125;</span><br><span class="line">        else if(!a.top &amp;&amp; b.top) &#123;</span><br><span class="line">            return 1;</span><br><span class="line">        &#125;</span><br><span class="line">        else return b.date - a.date; // 都没定义按照文章日期降序排</span><br><span class="line">    &#125;);</span><br><span class="line">  var paginationDir = config.pagination_dir || &apos;page&apos;;</span><br><span class="line">  return pagination(&apos;&apos;, posts, &#123;</span><br><span class="line">    perPage: config.index_generator.per_page,</span><br><span class="line">    layout: [&apos;index&apos;, &apos;archive&apos;],</span><br><span class="line">    format: paginationDir + &apos;/%d/&apos;,</span><br><span class="line">    data: &#123;</span><br><span class="line">      __index: true</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>在文章中添加 top 值，数值越大文章越靠前，如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 解决Charles乱码问题</span><br><span class="line">date: 2017-05-22 22:45:48</span><br><span class="line">tags: 技巧</span><br><span class="line">categories: 技巧</span><br><span class="line">copyright: true</span><br><span class="line">top: 100</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h2 id="修改字体大小"><a href="#修改字体大小" class="headerlink" title="修改字体大小"></a>修改字体大小</h2><p>打开\themes\next\source\css_variables\base.styl文件，将 $font-size-base改成16px ，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$font-size-base            = 16px</span><br></pre></td></tr></table></figure><h2 id="修改打赏字体不闪动"><a href="#修改打赏字体不闪动" class="headerlink" title="修改打赏字体不闪动"></a>修改打赏字体不闪动</h2><p>修改文件next/source/css/_common/components/post/post-reward.styl，然后注释其中的函数wechat:hover和alipay:hover，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/* 注释文字闪动函数</span><br><span class="line"> #wechat:hover p&#123;</span><br><span class="line">    animation: roll 0.1s infinite linear;</span><br><span class="line">    -webkit-animation: roll 0.1s infinite linear;</span><br><span class="line">    -moz-animation: roll 0.1s infinite linear;</span><br><span class="line">&#125;</span><br><span class="line"> #alipay:hover p&#123;</span><br><span class="line">   animation: roll 0.1s infinite linear;</span><br><span class="line">    -webkit-animation: roll 0.1s infinite linear;</span><br><span class="line">    -moz-animation: roll 0.1s infinite linear;</span><br><span class="line">&#125;</span><br><span class="line">*/</span><br></pre></td></tr></table></figure><h2 id="侧边栏推荐阅读"><a href="#侧边栏推荐阅读" class="headerlink" title="侧边栏推荐阅读"></a>侧边栏推荐阅读</h2><p>打开主题配置文件修改成这样就行了(links里面写你想要的链接):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Blogrolls</span><br><span class="line">links_title: 推荐阅读</span><br><span class="line">#links_layout: block</span><br><span class="line">links_layout: inline</span><br><span class="line">links:</span><br><span class="line">  优设: http://www.uisdc.com/</span><br><span class="line">  张鑫旭: http://www.zhangxinxu.com/</span><br><span class="line">  Web前端导航: http://www.alloyteam.com/nav/</span><br><span class="line">  前端书籍资料: http://www.36zhen.com/t?id=3448</span><br><span class="line">  百度前端技术学院: http://ife.baidu.com/</span><br><span class="line">  google前端开发基础: http://wf.uisdc.com/cn/</span><br></pre></td></tr></table></figure><h2 id="自定义鼠标样式"><a href="#自定义鼠标样式" class="headerlink" title="自定义鼠标样式"></a>自定义鼠标样式</h2><p>打开themes/next/source/css/_custom/custom.styl,在里面写下如下代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 鼠标样式</span><br><span class="line">  * &#123;</span><br><span class="line">      cursor: url(&quot;http://om8u46rmb.bkt.clouddn.com/sword2.ico&quot;),auto!important</span><br><span class="line">  &#125;</span><br><span class="line">  :active &#123;</span><br><span class="line">      cursor: url(&quot;http://om8u46rmb.bkt.clouddn.com/sword1.ico&quot;),auto!important</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>其中 url 里面必须是 ico 图片，ico 图片可以上传到网上（我是使用七牛云图床），然后获取外链，复制到 url 里就行了</p><h2 id="hexo-添加百度站长推送"><a href="#hexo-添加百度站长推送" class="headerlink" title="hexo 添加百度站长推送"></a>hexo 添加百度站长推送</h2><p>具体实现方法见 <span class="exturl" data-url="aHR0cDovL3d3dy5haXN1bi5vcmcvMjAxNy8wOS9iYWlkdStoZXhvK3poYW56aGFuZy8=" title="http://www.aisun.org/2017/09/baidu+hexo+zhanzhang/">hexo 添加百度站长推送<i class="fa fa-external-link"></i></span></p><h2 id="hexo-NexT主题首页title链接的优化"><a href="#hexo-NexT主题首页title链接的优化" class="headerlink" title="hexo NexT主题首页title链接的优化"></a>hexo NexT主题首页title链接的优化</h2><p>具体实现方法见 <span class="exturl" data-url="aHR0cDovL3d3dy5haXN1bi5vcmcvMjAxNy8wOS9oZXhvK25leHQrdGl0bGUv" title="http://www.aisun.org/2017/09/hexo+next+title/">hexo NexT主题首页title链接的优化<i class="fa fa-external-link"></i></span></p><h2 id="Hexo-NexT主题修改文章标题样式"><a href="#Hexo-NexT主题修改文章标题样式" class="headerlink" title="Hexo NexT主题修改文章标题样式"></a>Hexo NexT主题修改文章标题样式</h2><p>进入主题目录</p><p>hexo\themes\next\source\css_common\components\post</p><p>修改post.styl文件，在配置的后面添加下面的代码。该文件是博文的样式表。</p><p>注意如果想把主页标题样式一同修改，可以用把 .page-post-detail 去掉</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/*添加下面的CSS代码来修改博客标题样式*/</span><br><span class="line">.page-post-detail .post-title &#123;</span><br><span class="line">  font-size: 26px;</span><br><span class="line">  text-align: center;</span><br><span class="line">  word-break: break-word;</span><br><span class="line">  font-weight: $posts-expand-title-font-weight</span><br><span class="line">  background-color: #b9d3ee;</span><br><span class="line">  border-radius:.3em;</span><br><span class="line">  line-height:1em;</span><br><span class="line">  padding-bottom:.12em;</span><br><span class="line">  padding-top:.12em;</span><br><span class="line">  box-shadow:2px 2px 7px #9fb6cd;</span><br><span class="line">  +mobile() &#123;</span><br><span class="line">    font-size: 22px;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">/*添加上面的CSS代码来修改博客标题样式*/</span><br><span class="line">@import &quot;post-expand&quot;;</span><br><span class="line">@import &quot;post-collapse&quot;;</span><br><span class="line">@import &quot;post-type&quot;;</span><br><span class="line">@import &quot;post-title&quot;;</span><br></pre></td></tr></table></figure><h2 id="hexo-添加百度站长推送-1"><a href="#hexo-添加百度站长推送-1" class="headerlink" title="hexo 添加百度站长推送"></a>hexo 添加百度站长推送</h2><p><span class="exturl" data-url="aHR0cHM6Ly9odWktd2FuZy5pbmZvLzIwMTYvMTAvMjMvSGV4byVFNiU4RiU5MiVFNCVCQiVCNiVFNCVCOSU4QiVFNyU5OSVCRSVFNSVCQSVBNiVFNCVCOCVCQiVFNSU4QSVBOCVFNiU4RiU5MCVFNCVCQSVBNCVFOSU5MyVCRSVFNiU4RSVBNS8=" title="https://hui-wang.info/2016/10/23/Hexo%E6%8F%92%E4%BB%B6%E4%B9%8B%E7%99%BE%E5%BA%A6%E4%B8%BB%E5%8A%A8%E6%8F%90%E4%BA%A4%E9%93%BE%E6%8E%A5/">主动推送<i class="fa fa-external-link"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly9wdXJld2hpdGUuaW8vMjAxNy8wNC8yOS9oZXhvLWJhaWR1LXVybC1zdWJtaXQv" title="https://purewhite.io/2017/04/29/hexo-baidu-url-submit/">自动推送，sitemap推送<i class="fa fa-external-link"></i></span></p><p>本文转载自<span class="exturl" data-url="aHR0cHM6Ly93d3cuYWlzdW4ub3JnLzIwMTcvMTAvaGV4by1uZXh0K2Rpbmd6aGkv" title="https://www.aisun.org/2017/10/hexo-next+dingzhi/">Hunter-Zack<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;hexo框架基于next主题深度定制方案&lt;/p&gt;
&lt;p&gt;主要有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在右上角或者左上角实现fork me on github&lt;/li&gt;
&lt;li&gt;添加RSS&lt;/li&gt;
&lt;li&gt;背景配置&lt;/li&gt;
&lt;li&gt;添加动态背景&lt;/li&gt;
&lt;li&gt;实现点击出现桃心
      
    
    </summary>
    
      <category term="技术" scheme="https://tangguangen.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hexo" scheme="https://tangguangen.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>1936年9月竺可桢校长在浙大开学典礼上的讲话</title>
    <link href="https://tangguangen.com/2018/09/05/1936%E5%B9%B49%E6%9C%88%E7%AB%BA%E5%8F%AF%E6%A1%A2%E6%A0%A1%E9%95%BF%E5%9C%A8%E6%B5%99%E5%A4%A7%E5%BC%80%E5%AD%A6%E5%85%B8%E7%A4%BC%E4%B8%8A%E7%9A%84%E8%AE%B2%E8%AF%9D/"/>
    <id>https://tangguangen.com/2018/09/05/1936年9月竺可桢校长在浙大开学典礼上的讲话/</id>
    <published>2018-09-05T12:54:25.000Z</published>
    <updated>2018-11-20T01:24:50.345Z</updated>
    
    <content type="html"><![CDATA[<p>诸位同学，学校开课已一周，今天训育处召集这个会，能如家人似的在一起谈话，觉得非常愉快。</p><p>大学生，是人生最快活的时期，没有直接的经济负担，没有谋生的问题。诸位在中学中，同学大都是同县或同省，可是，来大学后，有从全国各方面来的同学，可以知道全国的情形，时间长了，各人都认识。这样，各人家庭的状况，故乡的风物，都能互相知道，这亦是一种教育。大学比之中学，在经费和设备方面，都来的充实，教师的经验和学识，也远胜于中学，这供给诸位切磋学问的极好机会。同时，国家花在诸位身上的钱，每年有一千五百元，而且，全中国大学生仅四万人，诸位都是这一万分之一的青年，这种机会，万万不能错过。</p><p>诸位到这里来，应该明了这里的校风。一校有一校的精神，英文称为College Spirit。至于浙大的精神，可以把“诚”、“勤”两字来表示。浙大的前身是求是书院和高等学堂，一脉相传，都可以诚勤两字代表它的学风，学生不浮夸，做事很勤恳，在社会上的声誉亦很好。有的学校校舍很好，可是毕业生做事，初出去就希望有物质的享受，待遇低一点便不愿做，房屋陋不愿住。浙大的毕业生便无此习惯，校外的人，碰见了，总是称赞浙大的风气朴实。这种风气，希望诸位把它保持。</p><p><strong>诸位在校，有两个问题应该自己问问，第一，到浙大来做什么?第二，将来毕业后要做什么样的人?</strong>我想诸位中间，一定没有人说为文凭而到浙大来的，或者有的为到这里来是为了求一种技术，以做谋生的工具。但是，谋生之道很多，不一定到大学来，就是讲技术，亦不一定在大学。美国大文豪罗威尔氏说：“大学的目的，不在使学生得到面包，而在使所得到的面包味道更好。”教育不仅使学生谋得求生之道，单学一种技术，尚非教育最重要的目的。</p><img src="/2018/09/05/1936年9月竺可桢校长在浙大开学典礼上的讲话/62031557_1.jpg"><p>这里我可以讲一个故事。中国古时有一个人求神仙心切，遍走名山大川。吕纯阳发慈悲，知道他诚心，想送给他一点金钱宝贝，向他说道，我的指头能指石为金，或任何物件，你要什么我便给你什么。可是那个人并不要金钱宝贝，而要他那只指头。这故事西洋也有的，英文所谓Wishing Ring，便是这个意思，要想什么就可得什么。世界上万事万物统有他存在的理由，朱子所谓格物致知就是即事而穷其理。<strong>要能即事而穷其理，最要紧的是一个清醒的头脑。</strong></p><p>清醒的头脑，是事业成功的基础。两三年以后诸位出去，在社会上做一番事业，无论工农商学，都须有清醒的头脑。专精一门技术的人，头脑未必清楚。反之，头脑清楚，做学问办事业统行，我们国家到这步田地，完全靠头脑清醒的人才有救。凡是办一桩事或是研究一个问题，大致可分为以下三个步骤：</p><p>第一，以科学的方法来分析，使复杂的变成简单;</p><p>第二，以公正的态度来计划;</p><p>第三，以果断的决心来执行。</p><p>这三点，科学的方法，公正的态度，果断的决心，统应该在小学时代养成和学习的。中国历年来工商业的不振，科学的不进步，都是由于主持者没有清醒的头脑。瘟疫流行，水旱灾荒，连年叠见，仍旧还要靠拜忏求神扶乩种种迷信方法。兴办事业，毫无计划，都是吃了头脑不清楚的亏。风水扶乩算命求神等之为迷信，不但为近世科学家所诟病，即我国古代明理之君子亦早深悉而痛绝之。但到如今，大学毕业生和东西洋留学生中，受了环境的同化，而同流合污的很不少。大的企业如久大公司、永利公司和商务印书馆的成功，要算例外了。</p><p>近年来政府对社会所办的棉纱厂、面粉厂、硫酸厂、酒精厂和糖厂等，大多数是失败的。失败的原因或是由于调查的时候不用科学方法。譬如办糖厂，应在事先调查在该厂附近地域产多少甘蔗，出产的糖销至何处，成本的多少，赢利的厚薄，与夫国外倾销竞争的状况。若事先不调查清楚，后来必至蚀本倒闭。这类事在中国司空见惯，如汉口的造纸厂，梧州的硫酸厂，真不胜枚举。还有失败的原因是用人行政重情而不重理，这就是没有公正的态度。用人不完全以人才为标准，而喜欢滥用亲戚。每个机关、公司应该多聘专家，计划决定以后，外界无论如何攻击，都得照着计划做去，这样才能成功。</p><p><strong>盲从的习惯，我们应该竭力避免，我们不能因为口号叫得响一点，或是主义新一点，就一唱百和的盲从起来。我们大家要静心平气的来观察口号的目的，主义的背景，凭我们的裁判，扪良心来决定我们的主张。</strong>若是对的，我们应竭力奉行。若是不对的我们应尽力排除。依违两可，明哲保身的态度，和盲从是一样要避免的。我们要做有主张有作为的人，这样就非有清醒之头脑不可。</p><p>现在，要问第二个问题，便是，离开大学以后，将来做什么样的人?我们的人生观应如何?有人认为中国的人生观很受孔孟的影响，实际影响最大的还是老子。孔孟主张见义勇为，老子主张明哲保身;孔孟主张正是非，老子主张明祸福。孟子说：“天之将降大任于斯人也，必先苦其心志，劳其筋骨”，诸葛亮“鞠躬尽瘁，死而后已”，这才不是享福哲学。老子说：“祸莫大于不知足”，又曰“祸兮福所倚，福兮祸所伏”。</p><p>中国一般人的最后目的还是享福。我们羡慕人家说某人福气好，娶媳妇进门，即祝之曰“多福多寿多男子”。就是生子的最大目的，也就是想年老的时候可以享福。中国普通人意想中的天堂，是可以不劳而获的一个世界，茶来开口，饭来伸手，这样享福哲学影响于民生问题很大。</p><p>人以享福为人生最大目的，中国民族必遭灭亡，历史上罗马之亡可为殷鉴。现在的世界是竞争的世界，如果一个民族还是一味以享受为目的，不肯以服务为目的，必归失败。我们应该以享福为可耻，只有老弱残废才能享福，而以自食其力为光荣。英国国王在幼年时，必在军舰充当小兵，惟其如此方能知兵士的疾苦。全世界最富的人是煤油大王洛克菲勒(Rockefeller)，他的儿子做事从小伙计做起，所以他们的事业能子孙相传不替。</p><p>多年前，中日同时派学生留学欧美，中国的学生，一看见各类机械，便问从何处购买?何处最便宜?而日本的学生，只问如何制造?中国人只知道买，以享受为目的，而日本人则重做，以服务为目的;中国从前学工学农的人，统是只叫工人农夫去推动机器，耕耘田亩，而自己却在一边袖手旁观，这样讲究农工业是不会进步的。中国古代轻视劳力，现在已经完全改变，样样应该自己动手，这种人生观的改造，是极重要的。</p><p>以上所说的两点：第一，诸位求学，应不仅在科目本身，而且要训练如何能正确地训练自己的思想;第二，我们人生的目的是在能服务，而不在享受。</p><p>——刊于《国立浙江大学日刊》第20号（1936年9月23日）“讲演”栏</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;诸位同学，学校开课已一周，今天训育处召集这个会，能如家人似的在一起谈话，觉得非常愉快。&lt;/p&gt;
&lt;p&gt;大学生，是人生最快活的时期，没有直接的经济负担，没有谋生的问题。诸位在中学中，同学大都是同县或同省，可是，来大学后，有从全国各方面来的同学，可以知道全国的情形，时间长了，各
      
    
    </summary>
    
      <category term="生活随笔" scheme="https://tangguangen.com/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="感想" scheme="https://tangguangen.com/tags/%E6%84%9F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>华为云图引擎服务实时推荐（Real-time Recommendation）算法</title>
    <link href="https://tangguangen.com/2018/09/05/huawei-real-time-recommendation/"/>
    <id>https://tangguangen.com/2018/09/05/huawei-real-time-recommendation/</id>
    <published>2018-09-05T00:33:33.163Z</published>
    <updated>2018-09-05T03:50:46.861Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>图引擎服务提供的实时推荐算法是一种基于随机游走模型的实时推荐算法，能够推荐与输入节点相近程度高、关系或喜好相近的节点。</p><p>实时推荐过程主要是基于对图上多起点“<strong>source</strong>”的随机游走：其根据各节点的重要性给各请求节点分配不同的游走步长， 同时对各请求节点进行随机游走，综合各起点随机游走下候选节点的得分结果，进行推荐。</p><p>其中，随机游走过程：从请求节点”<strong>source</strong>“沿着边，根据一定的倾向性不断地“游走“到相邻的节点，达到一定步数（步长）时返回起点重新游走。</p><p>游走步长：CurrSteps = SampleWalkLength(<strong>alpha</strong>)[1]</p><p>多次这样的游走后， 对于一个节点，如果其在随机游走过程被访问到，且被访问到的次数达到“<strong>nv</strong>”，则该节点将记入候选推荐的节点。若某个source节点的候选推荐节点达到“<strong>np</strong>”，对于该source节点的随机游走将提前结束，或者达到总的游走步数上限”<strong>N</strong>“时结束。</p><p>随机游走过程中，随着被经过的次数的增加，候选节点被推荐的得分也相应增加；当所有的随机游走结束时，系统将给出各候选节点的综合得分，得分越高被推荐的程度越大。</p><h2 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h2><p><strong>sources</strong>： 给定节点ID，点击”+”可添加多个节点，最多不超过30个。该参数必填。</p><p><strong>alpha</strong>：权重系数, 其值越大步长越长, 取值范围（0~1）, 实数，默认值为0.85。</p><p><strong>N</strong>：总的游走步数, [1,200000], 正整数，默认值10000。</p><p><strong>nv</strong>：候选推荐节点所需访问次数的最小值, 取值范围(0,10], 正整数，默认值5。</p><p><strong>np</strong>：游走过程提前结束参数：候选推荐节点个数。取值范围 [1,2000], 正整数，默认值1000。</p><p><strong>label</strong>：希望输出的点的类型。其值为空时，将不考虑点的类型，输出算法原始计算结果。对其赋值时，将从计算结果中过滤出具有该“label”的点的返回。</p><p><strong>directed</strong>：是否考虑边的方向。true 或false，布尔型。默认值true。</p><h2 id="参数分析"><a href="#参数分析" class="headerlink" title="参数分析"></a>参数分析</h2><ul><li><p>sources</p><ul><li><p>单个source</p><p>例如：“当Mike打开某电影平台，向其推荐电影“时，可以输入sources为Mike, label为movie，其他参数如下图（左），点击运行。页面画布上将会出现含有推荐节点的子图，大小反映了推荐程度。</p><img src="/2018/09/05/huawei-real-time-recommendation/1536045185212.png"></li><li><p>多个sources</p><p>例如：“Leo的某个朋友，浏览了Leo主页，添加Steven Spielberg导演动态,为其进行电影推荐”，这时，我们可以输入参数如下图左，综合考虑这两种兴趣（“Leo”、“Steven Spielberg”）进行实时推荐：</p><img src="/2018/09/05/huawei-real-time-recommendation/1536045628125.png"></li></ul></li><li><p>alpha</p><p>alpha和N的值决定了总的游走步长。通过实验得知<strong>alpha的取值不应过小</strong>。当alpha取一个比较小的值（这里取的是0.3）时，推荐的结果中，得分从201过渡到34，如下图所示。而当alpha取一个较大值，例如0.85的时候就不会出现这种情况。</p><img src="/2018/09/05/huawei-real-time-recommendation/1536046674109.png"></li><li><p>N</p><p>总的游走步数，当达到总的游走步数上限”<strong>N</strong>“时结束。控制程序何时结束。</p></li><li><p>nv</p><p>候选推荐节点所需访问次数的最小值。 对于一个节点，如果其在随机游走过程被访问到，且被访问到的次数达到“<strong>nv</strong>”，则该节点将记入候选推荐的节点。</p></li><li><p>np</p><p>若某个source节点的候选推荐节点达到“<strong>np</strong>”，对于该source节点的随机游走将提前结束。这是一个早停参数。如下图所示，当np=1000时，算法总耗时:0.0616 s，当np=10时，算法总耗时:0.0019 s。可知选择一个合适的np值可以减少耗时，提高效率。</p><img src="/2018/09/05/huawei-real-time-recommendation/1536048298993.png"></li><li><p>label</p><p>控制输出的点的类型。</p><ul><li>其值为空时，将不考虑点的类型，输出算法原始计算结果。</li><li>对其赋值时，将从计算结果中过滤出具有该“label”的点的返回。</li></ul></li><li><p>directed</p><p>是否考虑边的方向。当directed=true时，可以看到结果画布中Leo节点的入度为0。</p><img src="/2018/09/05/huawei-real-time-recommendation/1536049018099.png"></li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MTEuMDc2MDEucGRm" title="https://arxiv.org/pdf/1711.07601.pdf">[1] Eksombatchai C, Jindal P, Liu J Z, et al. Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time[J]. 2017.<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;图引擎服务提供的实时推荐算法是一种基于随机游走模型的实时推荐算法，能够推荐与输入节点相近程度高、关系或喜好相近的节点。&lt;/p&gt;
&lt;p&gt;实时推
      
    
    </summary>
    
      <category term="技术" scheme="https://tangguangen.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Artificial Intelligence" scheme="https://tangguangen.com/tags/Artificial-Intelligence/"/>
    
      <category term="华为云" scheme="https://tangguangen.com/tags/%E5%8D%8E%E4%B8%BA%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>Natural-Language-Processing-Tasks-and-References</title>
    <link href="https://tangguangen.com/2018/09/05/Natural-Language-Processing-Tasks-and-References/"/>
    <id>https://tangguangen.com/2018/09/05/Natural-Language-Processing-Tasks-and-References/</id>
    <published>2018-09-05T00:27:02.000Z</published>
    <updated>2018-11-28T09:09:45.258Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Text-Similarity"><a href="#Text-Similarity" class="headerlink" title="Text Similarity"></a>Text Similarity</h2><ul><li><strong>WIKI</strong> <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvU2VtYW50aWNfc2ltaWxhcml0eQ==" title="https://en.wikipedia.org/wiki/Semantic_similarity">Semantic similarity<i class="fa fa-external-link"></i></span></li><li><strong>PAPER</strong> <span class="exturl" data-url="aHR0cHM6Ly9wZGZzLnNlbWFudGljc2Nob2xhci5vcmcvNWI1Yy9hODc4YzUzNGFlZTM4ODJhMDM4ZWY5ZTgyZjQ2ZTEwMjEzMWIucGRm" title="https://pdfs.semanticscholar.org/5b5c/a878c534aee3882a038ef9e82f46e102131b.pdf">A Survey of Text Similarity Approaches<i class="fa fa-external-link"></i></span></li><li><strong>PAPER</strong> <span class="exturl" data-url="aHR0cDovL2Nhc2EuZGlzaS51bml0bi5pdC9+bW9zY2hpdHQvc2luY2UyMDEzLzIwMTVfU0lHSVJfU2V2ZXJ5bl9MZWFybmluZ1JhbmtTaG9ydC5wZGY=" title="http://casa.disi.unitn.it/~moschitt/since2013/2015_SIGIR_Severyn_LearningRankShort.pdf">Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks<i class="fa fa-external-link"></i></span></li><li><strong>PAPER</strong> <span class="exturl" data-url="aHR0cHM6Ly9ubHAuc3RhbmZvcmQuZWR1L3B1YnMvdGFpLXNvY2hlci1tYW5uaW5nLWFjbDIwMTUucGRm" title="https://nlp.stanford.edu/pubs/tai-socher-manning-acl2015.pdf">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks<i class="fa fa-external-link"></i></span></li><li><strong>CHALLENGE</strong> <span class="exturl" data-url="aHR0cDovL2FsdC5xY3JpLm9yZy9zZW1ldmFsMjAxNC90YXNrMy8=" title="http://alt.qcri.org/semeval2014/task3/">SemEval-2014 Task 3: Cross-Level Semantic Similarity<i class="fa fa-external-link"></i></span></li><li><strong>CHALLENGE</strong> <span class="exturl" data-url="aHR0cDovL2FsdC5xY3JpLm9yZy9zZW1ldmFsMjAxNC90YXNrMTAv" title="http://alt.qcri.org/semeval2014/task10/">SemEval-2014 Task 10: Multilingual Semantic Textual Similarity<i class="fa fa-external-link"></i></span></li><li><strong>CHALLENGE</strong> <span class="exturl" data-url="aHR0cDovL2FsdC5xY3JpLm9yZy9zZW1ldmFsMjAxNy90YXNrMS8=" title="http://alt.qcri.org/semeval2017/task1/">SemEval-2017 Task 1: Semantic Textual Similarity<i class="fa fa-external-link"></i></span></li><li><strong>WIKI</strong> <span class="exturl" data-url="aHR0cDovL2l4YTIuc2kuZWh1LmVzL3N0c3dpa2kvaW5kZXgucGhwL01haW5fUGFnZQ==" title="http://ixa2.si.ehu.es/stswiki/index.php/Main_Page">Semantic Textual Similarity Wiki<i class="fa fa-external-link"></i></span></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0t5dWJ5b25nL25scF90YXNrcw==" title="https://github.com/Kyubyong/nlp_tasks">https://github.com/Kyubyong/nlp_tasks<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Text-Similarity&quot;&gt;&lt;a href=&quot;#Text-Similarity&quot; class=&quot;headerlink&quot; title=&quot;Text Similarity&quot;&gt;&lt;/a&gt;Text Similarity&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;WIKI
      
    
    </summary>
    
      <category term="技术" scheme="https://tangguangen.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="NLP" scheme="https://tangguangen.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>HEXO备忘</title>
    <link href="https://tangguangen.com/2018/09/04/hexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>https://tangguangen.com/2018/09/04/hexo常用命令/</id>
    <published>2018-09-04T11:46:51.021Z</published>
    <updated>2018-09-05T12:55:58.046Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hexo常用命令"><a href="#hexo常用命令" class="headerlink" title="hexo常用命令"></a>hexo常用命令</h2><p>预览访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure><p>部署步骤</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><h2 id="为文章添加附件"><a href="#为文章添加附件" class="headerlink" title="为文章添加附件"></a>为文章添加附件</h2><h3 id="Hexo配置文件的设置"><a href="#Hexo配置文件的设置" class="headerlink" title="Hexo配置文件的设置"></a>Hexo配置文件的设置</h3><p>确保你的Hexo的配置文件_config.yml里面有个这个选项配置，并将其置为true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure><h3 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h3><p>在 \source\_posts 建立一个md文件来写博客，同时建立一个同名的文件夹去存放资源。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![图片描述](/文件夹/图片名.png)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;hexo常用命令&quot;&gt;&lt;a href=&quot;#hexo常用命令&quot; class=&quot;headerlink&quot; title=&quot;hexo常用命令&quot;&gt;&lt;/a&gt;hexo常用命令&lt;/h2&gt;&lt;p&gt;预览访问&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;tabl
      
    
    </summary>
    
      <category term="技术" scheme="https://tangguangen.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hexo" scheme="https://tangguangen.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>The State of the Art in Semantic Representation</title>
    <link href="https://tangguangen.com/2018/08/02/the-state-of-the-art-in-semantic-repesentation/"/>
    <id>https://tangguangen.com/2018/08/02/the-state-of-the-art-in-semantic-repesentation/</id>
    <published>2018-08-02T07:38:31.845Z</published>
    <updated>2018-11-28T09:07:35.465Z</updated>
    
    <content type="html"><![CDATA[<p>作者：Omri Abend   and   Ari Rappoport</p><p>这是一篇ACL2017的文章，是一篇语义表示方面的综述性论文。作者对当前一些最先进的语义表示方案以及资源（比如 AMR, UCCA, GMB, UDS)进行了一个survey。 </p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>语义表示在过去的几年中越来越受到人们的关注，并且发表了很多语义表示方面的文章（例如：<em>AMR, UCCA, GMB, UDS</em>）。但是这些文章中很少和句法方案进行比较，评估他们之间的优缺点，也没有清楚的阐明语义表示研究的总体目标。这篇文章通过语义表示领域的最先进的论文和技术进行了survey，填补了这项空白。</p><p>这篇文章可以分为四部分：</p><ul><li><p>第一部分详细阐述了文本语义表示（SRT）的目的，作者将语义表示定义为<strong>将文本的意思</strong>映射为<strong>语言发言者所理解的语义</strong>表示。</p></li><li><p>第二部分对<strong>文本语义表示</strong>包含的主要内容进行了总结，语义表示的内容主要包括<strong>谓词论元关系</strong>（predicate-argument）、<strong>话语关系</strong>（discourse relations）和<strong>逻辑结构</strong>（logical structure）等。</p></li><li>第三部分详细介绍了当前最先进的SRT方案和注释资源，并且分别介绍了它们的评估标准以及与句法方案的比较。</li><li>第四部分进行了总结。</li></ul><h2 id="语义表示的定义"><a href="#语义表示的定义" class="headerlink" title="语义表示的定义"></a>语义表示的定义</h2><p>（术语 <em>semantics</em> 在不同的上下文中会有不同意思。）在这篇论文中，作者将语义表示定义为<strong>将文本的意思</strong>映射为<strong>语言发言者(language speaker)所理解的语义</strong>表示。因此，语义表示可以看成是<strong>提取信息的方法</strong>，所提取出来的信息可以由语言发言者直接评估。 提取过程应该可靠且计算效率高。</p><p>一种方法是通过外部（文本外）标准或应用程序提取信息。比如支持推理的文本语义表示，推理出文本蕴涵或自然逻辑。 其他例子包括根据支持知识库查询定义语义表示等。</p><p>另一个SRT的方法是通过向量空间模型（VSM），从单词到短语和句子级别的所有语言元素建模为向量，它避免使用符号结构。 使用这种方法的通常会调用神经网络，在许多任务上获得优秀的结果，包括词汇任务，如跨语言单词相似度，机器翻译等。</p><h2 id="语义内容"><a href="#语义内容" class="headerlink" title="语义内容"></a>语义内容</h2><p>SRTs的内容的基本组成成分是论元（argument）结构（谁对谁做了什么，何地，何时以及为什么）。换句话说就是事件（events），表示参与者和他们之间的关系。</p><p>我们将使用以下作为运行示例：<br>(1) Although Ann was leaving, she gave the present to John.</p><p><strong>Events.</strong>  </p><p>一个事件包括谓词，它是事件的主要决定因素。  事件还包括论元（参与者，核心元素）和次级关系（修饰符，非核心元素）。示例1通常被视为具有两个事件，由谓词“leaving”和“gave”引起。</p><p><strong>Predicates and Arguments.</strong>  </p><p>谓词论元关系被普遍认为是语义表示的基础。大多数语义角色标注（SRL）方案涵盖了各种各样的动词谓词，但不同之处在于他们是否涵盖了名词性谓词和形容词谓词。例如，PropBank（Palmer et al。，2005）是SRL的主要资源之一，涵盖动词，并且在其最新版本中也包括偶数名词和多论元形容词。 FrameNet（Ruppenhofer等，2016）涵盖了所有这些，但也包括不引起事件的关系名词，如“president”。 其他工作涉及出现在句子边界之外的语义参数，或者没有明确出现在文本中的任何地方（Gerber和Chai，2010; Roth和Frank，2015）。</p><p><strong>Core and Non-core Arguments.</strong></p><p>论元类型之间最常见的区别在于核心和非核心论元。虽然可以在必需论元和可选论元之间进行区别，但在这里我们关注语义维度，它区分了其含义是特定谓词的论元，并且是所描述事件（核心）的必要组成，以及一般谓词（非核心）。 </p><p><strong>Semantic Roles.</strong></p><p>语义角色是论元的类别。 多年来已经在NLP中提出并使用了许多不同的语义角色库，最突出的是FrameNet，以及PropBank。 PropBank的角色集由AMR等后续项目扩展。 另一个突出的语义角色库是VerbNet和之后的项目，它们定义了一组封闭的抽象语义角色集合，适用于所有谓词论元。</p><p><strong>Co-reference and Anaphora.</strong></p><p>共指关系允许从引用相同实体的不同方式中抽象出来，并且通常包括在语义资源中。</p><p><strong>Temporal Relations.</strong></p><p>NLP中的大多数时间语义工作都集中在事件之间的时间关系上，或者通过根据文本中找到的时间表达式对它们进行时间戳，或者通过预测它们在时间上的相对顺序。 重要资源包括TimeML，它是时间关系的规范语言，以及TempEval系列共享任务和注释语料库。与时间关系相关的是事件之间的因果关系，它们在语言中无处不在，并且是各种应用的核心。</p><p><strong>Spatial Relations.</strong></p><p> 空间关系的表示在语义的认知理论中是关键的，并且在诸如地理信息系统或机器人导航的应用领域中是关键的。 该领域的重要任务包括空间角色标记，该任务包括空间元素和空间关系的识别和分类，例如地点，路径，方向和运动，以及它们的相对结构。</p><p><strong>Discourse Relations</strong></p><p>话语关系包含事件或更大的语义单元之间的任何语义关系。 例如，在（1）中，leaving和giving事件有时通过CONCESSION类型的话语关系相关，由“although”引起。 此类信息非常有用，通常对于各种NLP任务例如摘要，机器翻译和信息提取至关重要，但在开发此类系统时通常会被忽略。</p><p><strong>Logical Structure.</strong></p><p>逻辑结构是许多理论语言学中语义分析的基石，也在NLP领域引起了广泛关注。常见的表示通常基于谓词演算的变体，对于需要将文本映射到外部（通常是可执行的）形式语言的应用程序非常有用，例如查询语言或机器人指令。逻辑结构对于识别句子之间的蕴涵关系也很有用，因为一些蕴涵可以通过正式证明者从文本的逻辑结构中计算出来。</p><p><strong>Inference and Entailment.</strong></p><p> 许多语义方案的主要动机是它们支持推理和蕴涵的能力。 </p><h2 id="语义方案和资源"><a href="#语义方案和资源" class="headerlink" title="语义方案和资源"></a>语义方案和资源</h2><p>本节简要介绍了SRT的不同方案和资源。 </p><p><strong>Semantic Role Labeling.</strong>  SRL方案通常被称为“浅层语义分析”，因为它们专注于论元结构，忽略了其他关系，如话语事件，或者谓词和论元是如何在内部构建的。</p><p>SRL方案在它们的事件类型，涵盖的谓词类型，粒度，跨语言适用性，组织原则以及它们与句法的关系方面有所不同。大多数SRL方案相对于某些语法结构定义它们的注释，例如在PropBank的情况下PTB的解析树，或者在FrameNet的情况下为SRL目的定义的专用语法类别。除了上面讨论的PropBank，FrameNet和VerbNet之外，其他值得关注的资源包括Semlink（Loper等，2007），它链接不同资源中的相应条目，如Prop-Bank，FrameNet，VerbNet和WordNet，以及Preposition Supersenses项目（Schneider et al。，2015），侧重于介词引发的角色。</p><p><strong>AMR</strong> （抽象语义表示）涵盖谓词 - 论元关系，包括语义角色，它适用于各种谓词（包括言语，名词和形容词谓词），修饰词，共指关系，命名实体和一些时间表达。AMR目前不支持高于句子级别的关系到一个语义类别。并且是以英语中心，因此，在跨语言时，AMR面临着困难。</p><p><strong>UCCA</strong> （通用概念认知注释）是一种跨语言适用的语义注释方案，建立在类型学理论的基础上，主要基于基础语言学理论。UCCA的基础层次侧重于各种类型的论元结构和它们之间的关系。其当前状态中，UCCA比上述方案更粗糙（例如，它不包括语义角色信息）。 然而，它的跨语言中得到很好的推广。UCCA另一个优点是支持非专家的注释。</p><p><strong>UDS.</strong>  Universal Decompositional Semantics 通用分解语义是一种多层方案，目前包括语义角色注释，词义和体类。 然而，UDS表示的骨架结构源自句法依赖性，并且仅包括动词论元结构。</p><p><strong>The Prague Dependency Treebank (PDT) Tectogrammatical Layer (PDT-TL)</strong>涵盖了丰富的功能和语义差异，例如论元结构（包括语义角色）， 时态，省略号，主题/焦点，共指关系，词义消歧和方言信息。 PDT-TL源于对PDT语法层的抽象，其与语法的密切关系是显而易见的。</p><p><strong>CCG-based Schemes.</strong>  CCG是一种词汇化语法（即，几乎所有语义内容都在词典中编码），它定义了词汇信息如何组成以构成短语和句子含义的理论，并且在各种语义任务中被证明是有效的。</p><p><strong>HPSG-based Schemes.</strong> 与基于CCG的方案相关的是基于Head-driven Phrase Structure Grammar的SRT，其中句法和语义特征被表示为特征束，其通过统一规则迭代地组成以形成复合单元。 基于HPSG的SRT方案通常使用Minimal Recursion Semantics形式化。 注释语料库和手工制作的语法存在多种语言，并且通常关注论元结构和逻辑语义现象。</p><p><strong>OntoNotes</strong> 是一个有用的资源，具有多个相互关联的注释层，借用于不同的方案。 这些层包括句法，SRL，共指关系和词义消歧内容。 谓词的某些属性，例如哪些名词是偶数，也被编码。</p><p>总而言之，虽然SRT方案在它们支持的内容类型上有所不同，但方案不断发展以不断添加新的内容类型，从而使这些差异不那么重要。 这些方案之间的根本区别在于它们从语法中抽象出来的程度。 例如，AMR和UCCA从语法中抽象出来作为其设计的一部分，而在大多数其他方案中，句法法和语义更加紧密耦合。</p><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>人类评估是验证SRT方案的最终标准，因为我们将语义定义为语言发言者理解的意义。 确定SRT方案的理想程度是通过人类根据预先指定的要求对文本进行一些语义预测或注释，并将其与从SRT提取的信息进行比较。</p><p>另一种评估方法是基于任务的评估。NLP中的许多语义表示都是在考虑应用程序的情况下定义的，这使得这种评估方式显得更加自然。例如，AMR的一个主要动机是它对机器翻译的适用性，使MT成为AMR评估的一个自然（虽然迄今未开发）测试平台。 另一个例子是使用问答来评估基于知识查询中的语义解析。</p><p>评估语义方案的另一个常见标准是不变性（invariance），其中语义分析应该在释义或翻译对之间相似。</p><p>重要的是，这些评估标准也适用于自动引发表示而非手动定义的情况。 例如，通常通过基于任务的评估或者根据从它们计算的语义特征来评估向量空间表示，其有效性由人类注释者建立（例如，Agirre等人，2013,2014）。</p><p>最后，在通过手动注释（而不是通过自动化程序）引入语义方案的情况下，确定指南是否足够清晰以及类别是否定义明确的共同标准是通过为注释者指定一致来衡量注释者之间的一致性。 相同的文本和测量结果的相似性。 措施包括针对AMR的SMATCH测量（Cai和Knight，2013），以及适用于UCCA的DAG的PARSEVAL F-评分（Black等，1991）。</p><p>SRT方案在他们的注释者所需的背景和训练方面存在分歧。 一些方案需要广泛的训练（例如，AMR），而其他方案可以（至少部分地）通过众包（例如，UDS）来收集。 其他例子包括FrameNet，它需要专家注释者来创建新的事件，但是使用训练有素的内部注释器将现有的帧应用于文本; QASRL，远程使用非专家注释器; 和UCCA，它使用内部非专家，在初始培训期后证明对非专家注释器的专家没有优势。 GMB的另一种方法是使用在线协作，其中专家协作者参与手动纠正自动创建的表示。 他们进一步采用游戏化策略来收集注释的某些方面。</p><p><strong>Universality.</strong>  语义分析（通过更多表面形式的分析）的巨大承诺之一是其跨语言潜力。 然而，尽管普遍性在语义学中的理论和应用重要性早已得到认可（Goddard，2011），但普遍语义学的本质仍然未知。 最近，诸如BabelNet（Ehrmann等人，2014），UBY（Gurevych等人，2012）和Open Multilingual Wordnet等项目构建了庞大的多语言语义网络，通过链接Wikipedia和WordNet等资源并使用现代NLP技术处理它们。 然而，此类项目目前侧重于词汇语义和百科全书信息，而不是文本语义。</p><p>诸如SRL方案和AMR之类的符号SRT方案也因其跨语言适用性而被研究（Pad’o和Lapata，2009; Sun等，2010; Xue等，2014），表明跨语言的部分可移植性。 已经为多种语言构建了PropBank和FrameNet的翻译版本（例如，Akbik等，2016; Hartmann和Gurevych，2013）。 然而，由于PropBank和FrameNet都是词汇化方案，并且由于词汇在跨语言中大相径庭，因此这些方案在跨语言移植时需要相当大的适应性（Kozhevnikov和Titov，2013）。 正在进行的研究解决了将VerbNet的非语言化角色概括为普遍适用的集合（例如，Schneider等，2015）。 很少有SRT方案将跨语言适用性作为其主要标准之一，例如UCCA和LinGO语法矩阵（Bender和Flickinger，2005），两者都借鉴了类型学理论。</p><p>在向量空间中嵌入单词和句子的向量空间模型也被用于引发共享的跨语言空间（Klementiev等，2012; Rajendran等，2015; Wu等，2016）。 然而，需要进一步评估以确定这些表示的可能含义的哪些方面可靠地反映。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>NLP中的语义表示正在经历快速变化。 传统的语义工作要么使用侧重于特定语义现象的浅层方法，要么采用形式语义理论，这些理论通过语法 - 语义推理理论与句法方案相结合。 近年来，人们越来越关注一种独立于任何句法或分布标准定义语义结构的替代方法，这很大程度上归功于实现这种方法的语义树库的可用性。</p><p>语义方案在它们是否固定在文本的单词和短语(例如，所有类型的语义依赖关系和UCCA)中存在分歧(例如，AMR和基于逻辑的表示)。我们不认为这是一个主要的区别，因为大多数非雇定的表示(包括AMR)与句子中的单词保持着密切的亲和力，这可能是因为没有一个可行的词汇分解方案，而依赖结构可以转换成基于逻辑的表示(Reddy et al.， 2016)。在实践中，固定可以促进解析，而非锚定表示可以更灵活地使用不存在单词和语义组件的一一对应。</p><p>这篇论文总结了方案之间的主要区别因素是它们与句法方案之间的关系、它们的普遍性程度以及它们对注释者的专业知识的要求，这是解决注释瓶颈的一个重要因素。我们希望通过对语义表示的最先进的研究来促进研究者的讨论，使更多的研究人员接触到语义表示中最紧迫的问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作者：Omri Abend   and   Ari Rappoport&lt;/p&gt;
&lt;p&gt;这是一篇ACL2017的文章，是一篇语义表示方面的综述性论文。作者对当前一些最先进的语义表示方案以及资源（比如 AMR, UCCA, GMB, UDS)进行了一个survey。 &lt;/p&gt;

      
    
    </summary>
    
      <category term="阅读笔记" scheme="https://tangguangen.com/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://tangguangen.com/tags/NLP/"/>
    
      <category term="Paper" scheme="https://tangguangen.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://tangguangen.com/2018/07/31/hello-world/"/>
    <id>https://tangguangen.com/2018/07/31/hello-world/</id>
    <published>2018-07-31T11:43:20.284Z</published>
    <updated>2018-08-02T06:51:12.495Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvLw==" title="https://hexo.io/">Hexo<i class="fa fa-external-link"></i></span>! This is your very first post. Check <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3Mv" title="https://hexo.io/docs/">documentation<i class="fa fa-external-link"></i></span> for more info. If you get any problems when using Hexo, you can find the answer in <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3MvdHJvdWJsZXNob290aW5nLmh0bWw=" title="https://hexo.io/docs/troubleshooting.html">troubleshooting<i class="fa fa-external-link"></i></span> or you can ask me on <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hleG9qcy9oZXhvL2lzc3Vlcw==" title="https://github.com/hexojs/hexo/issues">GitHub<i class="fa fa-external-link"></i></span>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3Mvd3JpdGluZy5odG1s" title="https://hexo.io/docs/writing.html">Writing<i class="fa fa-external-link"></i></span></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3Mvc2VydmVyLmh0bWw=" title="https://hexo.io/docs/server.html">Server<i class="fa fa-external-link"></i></span></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3MvZ2VuZXJhdGluZy5odG1s" title="https://hexo.io/docs/generating.html">Generating<i class="fa fa-external-link"></i></span></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3MvZGVwbG95bWVudC5odG1s" title="https://hexo.io/docs/deployment.html">Deployment<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly9oZXhvLmlvLw==&quot; title=&quot;https://hexo.io/&quot;&gt;Hexo&lt;i class=&quot;fa fa-external-link&quot;&gt;&lt;/i&gt;&lt;/spa
      
    
    </summary>
    
      <category term="生活随笔" scheme="https://tangguangen.com/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
</feed>
