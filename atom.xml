<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>逍遥&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tangguangen.com/"/>
  <updated>2018-12-01T10:24:28.709Z</updated>
  <id>https://tangguangen.com/</id>
  
  <author>
    <name>逍遥</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hadoop教程：命令手册</title>
    <link href="https://tangguangen.com/hadoop-command-reference/"/>
    <id>https://tangguangen.com/hadoop-command-reference/</id>
    <published>2018-12-01T10:11:02.000Z</published>
    <updated>2018-12-01T10:24:28.709Z</updated>
    
    <content type="html"><![CDATA[<p>与这里演示的相比，<strong>“$HADOOP_HOME/bin/hadoop fs”</strong>中有更多的命令，尽管这些基本操作可以帮助您入门。不带附加参数运行./bin/hadoop dfs将列出所有可以与FsShell系统一起运行的命令。此外，如果遇到问题，<strong>$HADOOP_HOME/bin/hadoop fs -help</strong>命令名将显示有关操作的简短使用摘要。</p><p>所有操作的表如下所示。参数使用以下约定:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"&lt;path&gt;"</span> means any file or directory name. </span><br><span class="line"><span class="string">"&lt;path&gt;..."</span> means one or more file or directory names. </span><br><span class="line"><span class="string">"&lt;file&gt;"</span> means any filename. </span><br><span class="line"><span class="string">"&lt;src&gt;"</span> and <span class="string">"&lt;dest&gt;"</span> are path names <span class="keyword">in</span> a directed operation. </span><br><span class="line"><span class="string">"&lt;localSrc&gt;"</span> and <span class="string">"&lt;localDest&gt;"</span> are paths as above, but on the <span class="built_in">local</span> file system.</span><br></pre></td></tr></table></figure><p>所有其他文件和路径名都引用HDFS中的对象。</p><table><thead><tr><th>序号</th><th>命令</th></tr></thead><tbody><tr><td>1.</td><td><strong>ls <path></path></strong> <br>列出路径指定的目录的内容，显示每个条目的名称、权限、所有者、大小和修改日期。</td></tr><tr><td>2.</td><td><strong>lsr <path></path></strong><br>行为类似于-ls，但是递归地显示path的所有子目录中的条目。</td></tr><tr><td>3.</td><td><strong>du <path></path></strong><br>显示与路径匹配的所有文件的磁盘使用情况(以字节为单位);使用完整的HDFS协议前缀报告文件名。</td></tr><tr><td>4.</td><td><strong>dus <path></path></strong><br>与-du类似，但打印路径中所有文件/目录的磁盘使用情况摘要。</td></tr><tr><td>5.</td><td><strong>mv <src><dest></dest></src></strong><br>将src指示的文件或目录移动到HDFS内的dest。</td></tr><tr><td>6.</td><td><strong>cp <src> <dest></dest></src></strong><br>在HDFS中将src标识的文件或目录复制到dest。</td></tr><tr><td>7.</td><td><strong>rm <path></path></strong><br>删除路径标识的文件或空目录。</td></tr><tr><td>8.</td><td><strong>rmr <path></path></strong><br>删除路径标识的文件或目录。递归地删除任何子条目 (i.e., files or subdirectories of path).</td></tr><tr><td>9.</td><td><strong>put <localsrc> <dest></dest></localsrc></strong><br>将由localSrc标识的本地文件系统中的文件或目录复制到DFS中的dest。</td></tr><tr><td>10.</td><td><strong>copyFromLocal <localsrc> <dest><br></dest></localsrc></strong>-put相同</td></tr><tr><td>11.</td><td><strong>moveFromLocal <localsrc> <dest></dest></localsrc></strong><br>将由localSrc标识的本地文件系统中的文件或目录复制到HDFS中的dest，然后成功删除本地副本。</td></tr><tr><td>12.</td><td><strong>get [-crc] <src> <localdest></localdest></src></strong><br>将src标识的HDFS中的文件或目录复制到localDest标识的本地文件系统路径。</td></tr><tr><td>13.</td><td><strong>getmerge <src> <localdest></localdest></src></strong><br>检索与HDFS中的路径src匹配的所有文件，并将它们复制到localDest标识的本地文件系统中合并的单个文件。</td></tr><tr><td>14.</td><td><strong>cat <filen-ame></filen-ame></strong><br>在标准输出上显示文件名的内容。</td></tr><tr><td>15.</td><td><strong>copyToLocal <src> <localdest></localdest></src></strong><br>与 -get相同</td></tr><tr><td>16.</td><td><strong>moveToLocal <src> <localdest></localdest></src></strong><br>类似于-get，但成功时删除HDFS副本。</td></tr><tr><td>17.</td><td><strong>mkdir <path></path></strong><br>在HDFS中创建一个名为path的目录。<br>在路径中创建缺少的任何父目录(e.g., mkdir -p in Linux).</td></tr><tr><td>18.</td><td><strong>setrep [-R][-w] rep <path></path></strong><br>为通过路径到rep标识的文件设置目标复制因子(随着时间的推移，实际复制因子将向目标移动)</td></tr><tr><td>19.</td><td><strong>touchz <path></path></strong><br>在包含当前时间作为时间戳的路径上创建一个文件。如果文件在路径上已经存在，则失败，除非文件的大小已经为0。</td></tr><tr><td>20.</td><td><strong>test -[ezd] <path></path></strong><br>如果路径存在，返回1;长度为零;或者是目录，或者是0。</td></tr><tr><td>21.</td><td><strong>stat [format] <path></path></strong><br>打印关于路径的信息。Format是一个字符串，它接受块大小(%b)、文件名(%n)、块大小(%o)、复制(%r)和修改日期(%y， %y)。</td></tr><tr><td>22.</td><td><strong>tail [-f] <file2name></file2name></strong><br>显示stdout上文件的最后1KB。</td></tr><tr><td>23.</td><td><strong>chmod [-R] mode,mode,… <path></path>…</strong><br>与一个或多个相关联的文件权限对象的更改了路径….使用r模式递归执行更改是3位八进制模式，或{augo}+/-{rwxX}。假设没有指定范围且不应用umask。</td></tr><tr><td>24.</td><td><strong>chown [-R][owner][:[group]] <path></path>…</strong><br>集拥有用户和/或组的文件或目录被路径….如果指定-R，则递归设置所有者。</td></tr><tr><td>25.</td><td>chgrp [-R] group <path></path>…<br>设置拥有小组确认的文件或目录路径….如果指定-R，则递归地设置组。</td></tr><tr><td>26.</td><td><strong>help <cmd-name></cmd-name></strong><br>返回上述命令之一的使用信息。你必须省略cmd中的“-”字符。</td></tr></tbody></table><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfY29tbWFuZF9yZWZlcmVuY2UuaHRt" title="https://www.tutorialspoint.com/hadoop/hadoop_command_reference.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;与这里演示的相比，&lt;strong&gt;“$HADOOP_HOME/bin/hadoop fs”&lt;/strong&gt;中有更多的命令，尽管这些基本操作可以帮助您入门。不带附加参数运行./bin/hadoop dfs将列出所有可以与FsShell系统一起运行的命令。此外，如果遇到问题，
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/Hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：HDFS操作</title>
    <link href="https://tangguangen.com/hadoop-hdfs-operations/"/>
    <id>https://tangguangen.com/hadoop-hdfs-operations/</id>
    <published>2018-12-01T09:58:19.000Z</published>
    <updated>2018-12-01T10:11:24.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="启动HDFS"><a href="#启动HDFS" class="headerlink" title="启动HDFS"></a>启动HDFS</h2><p>首先，您必须格式化配置的HDFS文件系统，打开namenode (HDFS服务器)，并执行以下命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop namenode -format</span><br></pre></td></tr></table></figure><p>格式化HDFS之后，启动分布式文件系统。下面的命令将启动namenode以及数据节点作为集群。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-dfs.sh</span><br></pre></td></tr></table></figure><h2 id="列出HDFS中的文件"><a href="#列出HDFS中的文件" class="headerlink" title="列出HDFS中的文件"></a>列出HDFS中的文件</h2><p>在服务器中加载信息后，我们可以使用“ls”查找目录中的文件列表、文件状态。下面给出了可以作为参数传递到目录或文件名的ls语法。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -ls &lt;args&gt;</span><br></pre></td></tr></table></figure><h2 id="将数据插入HDFS"><a href="#将数据插入HDFS" class="headerlink" title="将数据插入HDFS"></a>将数据插入HDFS</h2><p>假设我们在本地系统中一个名为file.txt的文件，应该保存在hdfs文件系统中。按照下面给出的步骤在Hadoop文件系统中插入所需的文件。</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>您必须创建一个输入目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir /user/input</span><br></pre></td></tr></table></figure><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p>使用put命令将数据文件从本地系统传输和存储到Hadoop文件系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -put /home/file.txt /user/input</span><br></pre></td></tr></table></figure><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>您可以使用ls命令验证该文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -ls /user/input</span><br></pre></td></tr></table></figure><h2 id="从HDFS检索数据"><a href="#从HDFS检索数据" class="headerlink" title="从HDFS检索数据"></a>从HDFS检索数据</h2><p>假设HDFS中有一个名为outfile的文件。下面是一个从Hadoop文件系统检索所需文件的简单演示。</p><h3 id="Step-1-1"><a href="#Step-1-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>首先，使用cat命令查看来自HDFS的数据。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -cat /user/output/outfile</span><br></pre></td></tr></table></figure><h3 id="Step-2-1"><a href="#Step-2-1" class="headerlink" title="Step 2"></a>Step 2</h3><p>使用get命令将文件从HDFS获取到本地文件系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -get /user/output/ /home/hadoop_tp/</span><br></pre></td></tr></table></figure><h2 id="关闭HDFS"><a href="#关闭HDFS" class="headerlink" title="关闭HDFS"></a>关闭HDFS</h2><p>可以使用以下命令关闭HDFS</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stop-dfs.sh</span><br></pre></td></tr></table></figure><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfaGRmc19vcGVyYXRpb25zLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_hdfs_operations.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;启动HDFS&quot;&gt;&lt;a href=&quot;#启动HDFS&quot; class=&quot;headerlink&quot; title=&quot;启动HDFS&quot;&gt;&lt;/a&gt;启动HDFS&lt;/h2&gt;&lt;p&gt;首先，您必须格式化配置的HDFS文件系统，打开namenode (HDFS服务器)，并执行以下命令。&lt;/p&gt;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/Hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：HDFS概述</title>
    <link href="https://tangguangen.com/hadoop-hdfs-overview/"/>
    <id>https://tangguangen.com/hadoop-hdfs-overview/</id>
    <published>2018-12-01T09:35:37.000Z</published>
    <updated>2018-12-01T09:51:28.208Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop文件系统采用分布式文件系统设计开发。它在普通硬件上运行。与其他分布式系统不同，HDFS具有很高的容错性，并且使用低成本的硬件进行设计。</p><p>HDFS存储大量数据并提供更容易的访问。为了存储如此巨大的数据，文件被存储在多台机器上。这些文件以冗余的方式存储，以便在发生故障时将系统从可能的数据损失中拯救出来。HDFS还使应用程序可用于并行处理。</p><h2 id="HDFS的特点"><a href="#HDFS的特点" class="headerlink" title="HDFS的特点"></a>HDFS的特点</h2><ul><li>适用于分布式存储和处理。</li><li>Hadoop提供了一个与HDFS交互的命令接口。</li><li>namenode和datanode的内置服务器可以方便地检查集群的状态。</li><li>对文件系统数据的流访问。</li><li>HDFS提供文件权限和身份验证。</li></ul><h2 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h2><p>下面给出Hadoop文件系统的架构。</p><img title="hdfs_architecture" alt="hdfs_architecture" src="http://cdn.tangguangen.com/images/hdfs_architecture.jpg"><p>HDFS遵循主从体系结构，它具有以下元素。</p><h3 id="Namenode"><a href="#Namenode" class="headerlink" title="Namenode"></a>Namenode</h3><p>namenode是包含GNU/Linux操作系统和namenode软件的商品硬件。它是一种可以在普通硬件上运行的软件。具有namenode的系统充当主服务器，它执行以下任务:</p><ul><li>管理文件系统名称空间。</li><li>管理客户对文件的访问。</li><li>它还执行文件系统操作，如重命名、关闭和打开文件和目录。</li></ul><h3 id="Datanode"><a href="#Datanode" class="headerlink" title="Datanode"></a>Datanode</h3><p>datanode是一种具有GNU/Linux操作系统和datanode软件的普通硬件。对于集群中的每个节点(商品硬件/系统)，都将有一个datanode。这些节点管理其系统的数据存储。</p><ul><li>数据节点根据客户端请求在文件系统上执行读写操作。</li><li>它们还根据namenode的指令执行块创建、删除和复制等操作。</li></ul><h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><p>用户数据一般存储在HDFS文件中。文件系统中的文件将被分成一个或多个段和/或存储在单个数据节点中。这些文件段称为块。换句话说，HDFS可以读写的最小数据量称为块。默认块大小为64MB，但是可以根据需要在HDFS配置中进行更改而增加。</p><h2 id="HDFS的目标"><a href="#HDFS的目标" class="headerlink" title="HDFS的目标"></a>HDFS的目标</h2><ul><li><strong>故障检测与恢复:</strong> 由于HDFS包含大量的商用硬件，部件故障频繁。因此，HDFS应该具有快速、自动的故障检测和恢复机制。</li><li><strong>海量的数据集:</strong> HDFS每个集群应该有数百个节点，以管理拥有庞大数据集的应用程序。</li><li><strong>硬件在数据上</strong> 当计算发生在数据附近时，可以有效地完成请求的任务。特别是在涉及到大量数据集的情况下，它会减少网络流量并增加吞吐量。</li></ul><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfaGRmc19vdmVydmlldy5odG0=" title="https://www.tutorialspoint.com/hadoop/hadoop_hdfs_overview.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hadoop文件系统采用分布式文件系统设计开发。它在普通硬件上运行。与其他分布式系统不同，HDFS具有很高的容错性，并且使用低成本的硬件进行设计。&lt;/p&gt;
&lt;p&gt;HDFS存储大量数据并提供更容易的访问。为了存储如此巨大的数据，文件被存储在多台机器上。这些文件以冗余的方式存储
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/Hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：安装与环境设置</title>
    <link href="https://tangguangen.com/hadoop-enviornment-setup/"/>
    <id>https://tangguangen.com/hadoop-enviornment-setup/</id>
    <published>2018-12-01T08:23:55.000Z</published>
    <updated>2018-12-01T09:29:10.735Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop支持Windows, Mac, Linux, 但推荐是用Linux环境学习Hadoop。因此，我们必须安装一个Linux操作系统来设置Hadoop环境。如果您的操作系统不是Linux，那么您可以在其中安装一个Virtualbox软件，并在Virtualbox中包含Linux。</p><h2 id="安装前配置"><a href="#安装前配置" class="headerlink" title="安装前配置"></a>安装前配置</h2><p>在将Hadoop安装到Linux环境之前，我们需要使用ssh(Secure Shell)来设置Linux。按照下面给出的步骤设置Linux环境。</p><h3 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h3><p>首先，建议为Hadoop创建一个单独的用户，以便将Hadoop文件系统与Unix文件系统隔离开来。按照以下步骤创建用户:</p><ul><li>使用“su”命令打开根目录。</li><li>使用“useradd username”命令从根帐户创建一个用户。</li><li>现在您可以使用“su用户名”命令打开一个现有的用户帐户。</li></ul><p>打开Linux终端，输入以下命令来创建用户。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ su </span><br><span class="line">   password: </span><br><span class="line"><span class="comment"># useradd hadoop </span></span><br><span class="line"><span class="comment"># passwd hadoop </span></span><br><span class="line">   New passwd: </span><br><span class="line">   Retype new passwd</span><br></pre></td></tr></table></figure><h2 id="SSH设置和密钥生成"><a href="#SSH设置和密钥生成" class="headerlink" title="SSH设置和密钥生成"></a>SSH设置和密钥生成</h2><p>在集群上执行操作需要设置SSH，例如启动、停止、分布式守护进程shell操作。为了验证Hadoop的不同用户，需要为Hadoop用户提供公钥/私钥对，并与不同的用户共享。</p><p>下面的命令用于使用SSH生成键值对。从id_rsa.pub复制公钥到authorized_keys，并分别向所有者提供authorized_keys文件的读写权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa </span><br><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys </span><br><span class="line">$ chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h2 id="安装Java"><a href="#安装Java" class="headerlink" title="安装Java"></a>安装Java</h2><p>Hadoop必须安装Java。首先，您应该使用“java -version”命令验证系统中是否存在java。查看java版本命令的语法如下所示。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ java -version</span><br></pre></td></tr></table></figure><p>如果一切正常，它将给出以下输出。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java version <span class="string">"1.7.0_71"</span> </span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_71-b13) </span><br><span class="line">Java HotSpot(TM) Client VM (build 25.0-b02, mixed mode)</span><br></pre></td></tr></table></figure><p>如果您的系统中没有安装java，那么按照下面给出的步骤安装java。</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>下载java (JDK &lt;最新版本&gt; - X64.tar.gz)，请访问以下链接<span class="exturl" data-url="aHR0cDovL3d3dy5vcmFjbGUuY29tL3RlY2huZXR3b3JrL2phdmEvamF2YXNlL2Rvd25sb2Fkcy9qZGs3LWRvd25sb2FkczE4ODAyNjAuaHRtbOOAgg==" title="http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads1880260.html。">http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads1880260.html。<i class="fa fa-external-link"></i></span></p><p>然后jdk-7u71-linux-x64.tar.gz将被下载到您的系统中。</p><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p>通常您会在下载文件夹中找到下载的java文件。验证它并提取jdk-7u71-linux-x64.gz文件使用以下命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> Downloads/ </span><br><span class="line">$ ls </span><br><span class="line">jdk-7u71-linux-x64.gz </span><br><span class="line">$ tar zxf jdk-7u71-linux-x64.gz </span><br><span class="line">$ ls </span><br><span class="line">jdk1.7.0_71   jdk-7u71-linux-x64.gz</span><br></pre></td></tr></table></figure><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>要让所有用户都可以使用java，必须将其移动到“/usr/local/”位置。打开root，并键入以下命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ su </span><br><span class="line">password: </span><br><span class="line"><span class="comment"># mv jdk1.7.0_71 /usr/local/ </span></span><br><span class="line"><span class="comment"># exit</span></span><br></pre></td></tr></table></figure><h3 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h3><p>要设置PATH和JAVA_HOME变量，请向~/.bashrc添加以下命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.7.0_71 </span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure><p>现在令所有更改生效。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="Step-5"><a href="#Step-5" class="headerlink" title="Step 5"></a>Step 5</h3><p>使用以下命令配置java替代方案:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># alternatives --install /usr/bin/java java usr/local/java/bin/java 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --install /usr/bin/javac javac usr/local/java/bin/javac 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --install /usr/bin/jar jar usr/local/java/bin/jar 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --set java usr/local/java/bin/java</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --set javac usr/local/java/bin/javac</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --set jar usr/local/java/bin/jar</span></span><br></pre></td></tr></table></figure><p>现在验证来自终端的java版本命令，使用之前提到的查看Java版本命令。</p><h2 id="下载Hadoop"><a href="#下载Hadoop" class="headerlink" title="下载Hadoop"></a>下载Hadoop</h2><p>使用以下命令从Apache software foundation下载并提取Hadoop 2.4.1（根据需要选择版本）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ su </span><br><span class="line">password: </span><br><span class="line"><span class="comment"># cd /usr/local </span></span><br><span class="line"><span class="comment"># wget http://apache.claz.org/hadoop/common/hadoop-2.4.1/ </span></span><br><span class="line">hadoop-2.4.1.tar.gz </span><br><span class="line"><span class="comment"># tar xzf hadoop-2.4.1.tar.gz </span></span><br><span class="line"><span class="comment"># mv hadoop-2.4.1/* to hadoop/ </span></span><br><span class="line"><span class="comment"># exit</span></span><br></pre></td></tr></table></figure><h2 id="Hadoop的操作模式"><a href="#Hadoop的操作模式" class="headerlink" title="Hadoop的操作模式"></a>Hadoop的操作模式</h2><p>下载Hadoop后，可以使用以下三种支持模式之一来操作Hadoop集群:</p><ul><li><strong>本地/独立模式:</strong> 在您的系统中下载Hadoop之后，默认情况下，它是在独立模式下配置的，可以作为单个java进程运行。</li><li><strong>伪分布模式:</strong> 这是一个模拟在单机上的分布式。每个Hadoop守护进程(如hdfs、yarn、MapReduce等)将作为一个单独的java进程运行。这种模式对开发很有用。</li><li><strong>全分布模式:</strong> 这种模式是完全分布式的，集群中至少有两台或多台机器。我们将在接下来的章节中详细介绍这种模式。</li></ul><h2 id="以独立模式安装Hadoop"><a href="#以独立模式安装Hadoop" class="headerlink" title="以独立模式安装Hadoop"></a>以独立模式安装Hadoop</h2><p>这里我们将讨论Hadoop 2.4.1在独立模式下的安装。</p><p>没有运行守护进程，所有东西都在单个JVM中运行。独立模式适合在开发过程中运行MapReduce程序，因为它易于测试和调试。</p><h3 id="Hadoop配置"><a href="#Hadoop配置" class="headerlink" title="Hadoop配置"></a>Hadoop配置</h3><p>您可以通过向~/.bashrc文件添加以下命令来设置Hadoop环境变量。</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure><p>在继续之前，您需要确保Hadoop工作正常。使用以下命令查看hadoop是否安装成功:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop version</span><br></pre></td></tr></table></figure><p>如果您的设置一切正常，那么您应该会看到以下结果:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 2.4.1 </span><br><span class="line">Subversion https://svn.apache.org/repos/asf/hadoop/common -r 1529768 </span><br><span class="line">Compiled by hortonmu on 2013-10-07T06:28Z </span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From <span class="built_in">source</span> with checksum 79e53ce7994d1628b240f09af91e1af4</span><br></pre></td></tr></table></figure><p>这意味着Hadoop的独立模式设置工作正常。默认情况下，Hadoop被配置为在一台机器上以非分布式模式运行。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>让我们查看Hadoop的一个简单示例。Hadoop安装提供了以下示例MapReduce jar文件，它提供了MapReduce的基本功能，可以用于计算，如Pi值、给定文件列表中的字数等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar</span><br></pre></td></tr></table></figure><p>让我们有一个输入目录，在这里我们将推动一些文件，我们的要求是计数总字数在这些文件。要计算单词总数，我们不需要编写MapReduce，只要.jar文件包含单词计数的实现即可。您可以使用相同的.jar文件尝试其他示例;只要发出以下命令，检查hadoop- MapReduce -example -2.2.0.jar文件支持的MapReduce功能程序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduceexamples-2.2.0.jar</span><br></pre></td></tr></table></figure><h3 id="Step-1-1"><a href="#Step-1-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>在输入目录中创建临时内容文件。您可以在希望工作的任何地方创建此输入目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir input </span><br><span class="line">$ cp <span class="variable">$HADOOP_HOME</span>/*.txt input </span><br><span class="line">$ ls -l input</span><br></pre></td></tr></table></figure><p>它将在您的输入目录中提供以下文件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total 24 </span><br><span class="line">-rw-r--r-- 1 root root 15164 Feb 21 10:14 LICENSE.txt </span><br><span class="line">-rw-r--r-- 1 root root   101 Feb 21 10:14 NOTICE.txt</span><br><span class="line">-rw-r--r-- 1 root root  1366 Feb 21 10:14 README.txt</span><br></pre></td></tr></table></figure><p>这些文件是从Hadoop安装主目录复制的。对于您的实验，您可以拥有不同的大文件集。</p><h3 id="Step-2-1"><a href="#Step-2-1" class="headerlink" title="Step 2"></a>Step 2</h3><p>步骤2将进行所需的处理，并将输出保存在output/part-r00000文件中，您可以使用以下命令进行检查:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$cat</span> output/*</span><br></pre></td></tr></table></figure><p>它将列出输入目录中所有文件中可用的所有单词及其总数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"AS      4 </span></span><br><span class="line"><span class="string">"</span>Contribution<span class="string">" 1 </span></span><br><span class="line"><span class="string">"</span>Contributor<span class="string">" 1 </span></span><br><span class="line"><span class="string">"</span>Derivative 1</span><br><span class="line"><span class="string">"Legal 1</span></span><br><span class="line"><span class="string">"</span>License<span class="string">"      1</span></span><br><span class="line"><span class="string">"</span>License<span class="string">");     1 </span></span><br><span class="line"><span class="string">"</span>Licensor<span class="string">"      1</span></span><br><span class="line"><span class="string">"</span>NOTICE”        1 </span><br><span class="line"><span class="string">"Not      1 </span></span><br><span class="line"><span class="string">"</span>Object<span class="string">"        1 </span></span><br><span class="line"><span class="string">"</span>Source”        1 </span><br><span class="line"><span class="string">"Work”    1 </span></span><br><span class="line"><span class="string">"</span>You<span class="string">"     1 </span></span><br><span class="line"><span class="string">"</span>Your<span class="string">")   1 </span></span><br><span class="line"><span class="string">"</span>[]<span class="string">"      1 </span></span><br><span class="line"><span class="string">"</span>control<span class="string">"       1 </span></span><br><span class="line"><span class="string">"</span>printed        1 </span><br><span class="line"><span class="string">"submitted"</span>     1 </span><br><span class="line">(50%)     1 </span><br><span class="line">(BIS),    1 </span><br><span class="line">(C)       1 </span><br><span class="line">(Don<span class="string">'t)   1 </span></span><br><span class="line"><span class="string">(ECCN)    1 </span></span><br><span class="line"><span class="string">(INCLUDING      2 </span></span><br><span class="line"><span class="string">(INCLUDING,     2 </span></span><br><span class="line"><span class="string">.............</span></span><br></pre></td></tr></table></figure><h2 id="以伪分布式模式安装Hadoop"><a href="#以伪分布式模式安装Hadoop" class="headerlink" title="以伪分布式模式安装Hadoop"></a>以伪分布式模式安装Hadoop</h2><p>按照下面给出的步骤以伪分布式模式安装Hadoop 2.4.1。</p><h3 id="Step-1-环境配置"><a href="#Step-1-环境配置" class="headerlink" title="Step 1: 环境配置"></a>Step 1: 环境配置</h3><p>您可以通过向~/.bashrc文件添加以下命令来设置Hadoop环境变量。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop </span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> YARN_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$HADOOP_HOME</span>/lib/native </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$HADOOP_HOME</span>/bin </span><br><span class="line"><span class="built_in">export</span> HADOOP_INSTALL=<span class="variable">$HADOOP_HOME</span></span><br></pre></td></tr></table></figure><p>现在将所有更改应用到当前运行的系统中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="Step-2-Hadoop配置"><a href="#Step-2-Hadoop配置" class="headerlink" title="Step 2: Hadoop配置"></a>Step 2: Hadoop配置</h3><p>您可以在“$HADOOP_HOME/etc/hadoop”位置找到所有Hadoop配置文件。需要根据Hadoop基础设施对这些配置文件进行更改。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br></pre></td></tr></table></figure><p>为了用java开发Hadoop程序，必须在<strong>Hadoop -env.sh</strong>文件中替换<strong>JAVA_HOME</strong>值</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.7.0_71</span><br></pre></td></tr></table></figure><p>以下是配置Hadoop需要编辑的文件列表。</p><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a><strong>core-site.xml</strong></h4><p><strong>core-site.xml</strong>文件包含一些信息，例如Hadoop实例使用的端口号、为文件系统分配的内存、存储数据的内存限制以及读/写缓冲区的大小。</p><p>打开<strong>core-site.xml</strong>并在<configuration>, </configuration>之间添加以下属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a><strong>hdfs-site.xml</strong></h4><p><strong>hdfs-site.xml</strong>文件包含数据备份数量值、namenode路径和本地文件系统的datanode路径等信息。它意味着您希望存储Hadoop基础结构的地方。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dfs.replication (data replication value) = 1 </span><br><span class="line">(In the below given path /hadoop/ is the user name. </span><br><span class="line">hadoopinfra/hdfs/namenode is the directory created by hdfs file system.) </span><br><span class="line">namenode path = //home/hadoop/hadoopinfra/hdfs/namenode </span><br><span class="line">(hadoopinfra/hdfs/datanode is the directory created by hdfs file system.) </span><br><span class="line">datanode path = //home/hadoop/hadoopinfra/hdfs/datanode</span><br></pre></td></tr></table></figure><p>打开该文件，并在该文件中<configuration> </configuration>标签之间添加以下属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;file:///home/hadoop/hadoopinfra/hdfs/namenode &lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.data.dir&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;file:///home/hadoop/hadoopinfra/hdfs/datanode &lt;/value&gt; </span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">       </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 在上面的文件中，所有属性值都是用户定义的，您可以根据Hadoop基础结构进行更改。</p><h4 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a><strong>yarn-site.xml</strong></h4><p>这个文件用于将yarn配置到Hadoop中。打开yarn-site.xml文件并在该文件中的<configuration>, </configuration>标签之间添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;mapreduce_shuffle&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a><strong>mapred-site.xml</strong></h4><p>这个文件用于指定我们正在使用的MapReduce框架。默认情况下，Hadoop包含一个yarn-site.xml模板。首先，需要从<strong>mapred-site.xml.template</strong>复制文件到<strong>mapred-site.xml</strong>文件。使用以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p>打开mapred-site.xml文件。并在该文件中的<configuration>, </configuration>标签之间添加以下属性。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> </span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">&lt;/configuration</span><br></pre></td></tr></table></figure><h2 id="验证Hadoop安装"><a href="#验证Hadoop安装" class="headerlink" title="验证Hadoop安装"></a>验证Hadoop安装</h2><p>以下步骤用于验证Hadoop的安装。</p><h3 id="Step-1-Name-Node"><a href="#Step-1-Name-Node" class="headerlink" title="Step 1: Name Node"></a>Step 1: Name Node</h3><p>使用“hdfs namenode -format”命令设置namenode，如下所示。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~ </span><br><span class="line">$ hdfs namenode -format</span><br></pre></td></tr></table></figure><p>预期结果如下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">10/24/14 21:30:55 INFO namenode.NameNode: STARTUP_MSG: </span><br><span class="line">/************************************************************ </span><br><span class="line">STARTUP_MSG: Starting NameNode </span><br><span class="line">STARTUP_MSG:   host = localhost/192.168.1.11 </span><br><span class="line">STARTUP_MSG:   args = [-format] </span><br><span class="line">STARTUP_MSG:   version = 2.4.1 </span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">10/24/14 21:30:56 INFO common.Storage: Storage directory </span><br><span class="line">/home/hadoop/hadoopinfra/hdfs/namenode has been successfully formatted. </span><br><span class="line">10/24/14 21:30:56 INFO namenode.NNStorageRetentionManager: Going to </span><br><span class="line">retain 1 images with txid &gt;= 0 </span><br><span class="line">10/24/14 21:30:56 INFO util.ExitUtil: Exiting with status 0 </span><br><span class="line">10/24/14 21:30:56 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************ </span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at localhost/192.168.1.11 </span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><h3 id="Step-2-验证Hadoop-dfs"><a href="#Step-2-验证Hadoop-dfs" class="headerlink" title="Step 2: 验证Hadoop dfs"></a>Step 2: 验证Hadoop dfs</h3><p>下面的命令用于启动dfs。执行此命令将启动Hadoop文件系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-dfs.sh</span><br></pre></td></tr></table></figure><p>预期输出如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">10/24/14 21:37:56 </span><br><span class="line">Starting namenodes on [localhost] </span><br><span class="line">localhost: starting namenode, logging to /home/hadoop/hadoop</span><br><span class="line">2.4.1/logs/hadoop-hadoop-namenode-localhost.out </span><br><span class="line">localhost: starting datanode, logging to /home/hadoop/hadoop</span><br><span class="line">2.4.1/logs/hadoop-hadoop-datanode-localhost.out </span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br></pre></td></tr></table></figure><h3 id="Step-3-验证Yarn脚本"><a href="#Step-3-验证Yarn脚本" class="headerlink" title="Step 3: 验证Yarn脚本"></a>Step 3: 验证Yarn脚本</h3><p>下面的命令用于启动纱线脚本。执行此命令将启动纱线守护进程。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-yarn.sh</span><br></pre></td></tr></table></figure><p>预期输出如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">starting yarn daemons </span><br><span class="line">starting resourcemanager, logging to /home/hadoop/hadoop</span><br><span class="line">2.4.1/logs/yarn-hadoop-resourcemanager-localhost.out </span><br><span class="line">localhost: starting nodemanager, logging to /home/hadoop/hadoop</span><br><span class="line">2.4.1/logs/yarn-hadoop-nodemanager-localhost.out</span><br></pre></td></tr></table></figure><h3 id="Step-4-在浏览器上访问Hadoop"><a href="#Step-4-在浏览器上访问Hadoop" class="headerlink" title="Step 4: 在浏览器上访问Hadoop"></a>Step 4: 在浏览器上访问Hadoop</h3><p>访问Hadoop的默认端口号是50070。使用以下url在浏览器上获取Hadoop服务。</p><p>｛% qnimg hadoop_on_browser.jpg title:hadoop_on_browser alt: hadoop_on_browser%｝</p><h3 id="Step-5-验证集群中的所有应用程序"><a href="#Step-5-验证集群中的所有应用程序" class="headerlink" title="Step 5: 验证集群中的所有应用程序"></a>Step 5: 验证集群中的所有应用程序</h3><p>访问集群所有应用程序的默认端口号是8088。使用以下url访问此服务。</p><img alt="hadoop_application_cluster" src="http://cdn.tangguangen.com/images/hadoop_application_cluster.jpg"><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfZW52aW9ybm1lbnRfc2V0dXAuaHRt" title="https://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hadoop支持Windows, Mac, Linux, 但推荐是用Linux环境学习Hadoop。因此，我们必须安装一个Linux操作系统来设置Hadoop环境。如果您的操作系统不是Linux，那么您可以在其中安装一个Virtualbox软件，并在Virtualbox中包
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/Hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：Hadoop介绍</title>
    <link href="https://tangguangen.com/hadoop-introduction/"/>
    <id>https://tangguangen.com/hadoop-introduction/</id>
    <published>2018-11-30T13:05:54.000Z</published>
    <updated>2018-11-30T14:19:52.898Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop是一个用java编写的Apache开源框架，它允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序工作在一个跨计算机集群提供分布式存储和计算的环境中。Hadoop被设计成从单个服务器扩展到数千台机器，每台机器都提供本地计算和存储。</p><h2 id="Hadoop架构"><a href="#Hadoop架构" class="headerlink" title="Hadoop架构"></a>Hadoop架构</h2><p>Hadoop框架包括以下四个模块:</p><ul><li><strong>Hadoop Common:</strong> 这是其他Hadoop模块依赖的Java库和工具。这些库提供文件系统和操作系统级别的抽象，并包含启动Hadoop所需的Java文件和脚本。</li><li><strong>Hadoop YARN: </strong>这是一个用于作业调度和集群资源管理的框架。</li><li><strong>Hadoop分布式文件系统(HDFS™):</strong>  一种分布式文件系统，提供对应用程序数据的高吞吐量访问。</li><li><strong>MapReduce:</strong> 这是一个基于YARN的大型数据集并行处理系统。</li></ul><p>我们可以使用下面的图来描述Hadoop框架中可用的这四个组件。</p><img title="hadoop-architecture" alt="hadoop-architecture" src="http://cdn.tangguangen.com/images/hadoop_architecture.jpg"><p>自2012年以来，“Hadoop”一词通常不仅指上述基本模块，还指可以安装在Hadoop之上或与Hadoop并行的附加软件包集合，如<strong>Apache Pig、Apache Hive、Apache HBase、Apache Spark</strong>等。</p><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>Hadoop <strong>MapReduce</strong>是一个易于编写应用程序的软件框架，这些应用程序以可靠、容错的方式并行处理大集群(数千个节点)上的海量数据。</p><p>MapReduce这个词实际上是指Hadoop程序执行的两个不同的任务:</p><ul><li><strong>Map任务:</strong> 这是第一个任务，它接受输入数据并将其转换为一组数据集合，其中单个元素被分解为元组(键/值对)。</li><li><strong>Reduce任务:</strong> 该任务将map任务的输出作为输入，并将这些数据元组组合成较小的元组集合。reduce任务总是在map任务之后执行。</li></ul><p>通常输入和输出都存储在文件系统中。该框架负责调度任务、监视任务并重新执行失败的任务。</p><p>MapReduce框架由一个master <strong>JobTracker</strong>和每个集群节点的一个slave <strong>TaskTracker</strong>组成。master 负责资源管理，跟踪资源消耗/可用性，调度从服务器上的作业组件任务，监视它们并重新执行失败的任务。slave 按照主任务的指示执行任务，并定期向主任务提供任务状态信息。</p><p>JobTracker是Hadoop MapReduce服务的一个单点故障，这意味着如果JobTracker宕机，所有正在运行的作业都会停止。</p><h2 id="Hadoop-Distributed-File-System"><a href="#Hadoop-Distributed-File-System" class="headerlink" title="Hadoop Distributed File System"></a>Hadoop Distributed File System</h2><p>Hadoop可以直接与任何可挂载的分布式文件系统(如本地FS、HFTP FS、S3 FS等)一起工作，但是Hadoop最常用的文件系统是Hadoop分布式文件系统(HDFS)。</p><p>Hadoop分布式文件系统(HDFS)是基于谷歌文件系统(GFS)的，它提供了一个分布式文件系统，可以在小型计算机的大型集群(数千台计算机)上以可靠、容错的方式运行。</p><p>HDFS使用主/从（master/slave）架构，其中主架构由管理文件系统元数据的单个名称节点<strong>NameNode</strong>和存储实际数据的一个或多个从数据节点<strong>DataNodes</strong>组成。</p><p>HDFS名称空间中的文件被分成几个块，这些块存储在一组数据节点DataNodes中。NameNode确定块到DataNodes的映射。DataNodes 负责文件系统的读写操作。它们还根据NameNode给出的指令负责块的创建、删除和复制。</p><p>HDFS提供了与任何其他文件系统一样的shell，可以使用命令列表与文件系统交互。这些shell命令将在单独的一章中介绍，并提供适当的示例。</p><h2 id="Hadoop是如何工作的"><a href="#Hadoop是如何工作的" class="headerlink" title="Hadoop是如何工作的"></a>Hadoop是如何工作的</h2><h3 id="Stage-1"><a href="#Stage-1" class="headerlink" title="Stage 1"></a>Stage 1</h3><p>用户/应用程序可以通过以下项目将作业提交给Hadoop (Hadoop作业客户端)进行所需的处理:</p><ol><li>在分布式文件系统中输入和输出文件的位置。</li><li>java类以jar文件的形式包含map和reduce函数的实现。</li><li>通过设置特定于作业的不同参数进行作业配置。</li></ol><h3 id="Stage-2"><a href="#Stage-2" class="headerlink" title="Stage 2"></a>Stage 2</h3><p>然后Hadoop客户端将作业(jar/可执行文件等)和配置提交给JobTracker, JobTracker负责将软件/配置分发给slaves服务器，调度和监视任务，向客户端提供状态和诊断信息。</p><h3 id="Stage-3"><a href="#Stage-3" class="headerlink" title="Stage 3"></a>Stage 3</h3><p>不同节点上的TaskTrackers 按照MapReduce实现执行任务，reduce函数的输出存储在文件系统上的输出文件中。</p><h2 id="Hadoop的优点"><a href="#Hadoop的优点" class="headerlink" title="Hadoop的优点"></a>Hadoop的优点</h2><ul><li>Hadoop框架允许用户快速编写和测试分布式系统。它是高效的，它自动分配数据并跨机器工作，反过来利用CPU核心的底层并行性。</li><li>Hadoop不依赖硬件来提供容错和高可用性(FTHA)，Hadoop库本身被设计来检测和处理应用层的故障。</li><li>可以动态地从集群中添加或删除服务器，Hadoop可以不间断地继续运行。</li><li>Hadoop的另一个巨大优势是，它不仅是开源的，而且可以在所有平台上兼容，因为它是基于Java的。</li></ul><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfaW50cm9kdWN0aW9uLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_introduction.htm">https://www.tutorialspoint.com/hadoop/hadoop_introduction.htm<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hadoop是一个用java编写的Apache开源框架，它允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序工作在一个跨计算机集群提供分布式存储和计算的环境中。Hadoop被设计成从单个服务器扩展到数千台机器，每台机器都提供本地计算和存储
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/Hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：大数据解决方案</title>
    <link href="https://tangguangen.com/hadoop-big-data-solutions/"/>
    <id>https://tangguangen.com/hadoop-big-data-solutions/</id>
    <published>2018-11-30T12:50:44.000Z</published>
    <updated>2018-11-30T14:19:32.337Z</updated>
    
    <content type="html"><![CDATA[<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><p>在这种方式下，企业将使用一台计算机来存储和处理数据，处理所需的数据，并将其呈现给用户以供分析之用。在这里，数据将存储在RDBMS，如：Oracle数据库、MS SQL Server或DB2以及可以与数据库交互的复杂软件。</p><p>｛% qnimg traditional_approach.jpg title:traditional_approach alt:traditional_approach.jpg %｝</p><h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>这种方法在标准数据库服务器可以容纳的数据量更少或处理数据的处理器的极限的情况下工作得很好。但是当涉及到处理大量数据时，通过传统的数据库服务器来处理这些数据确实是一项非常繁琐的任务。</p><h2 id="Google的解决方案"><a href="#Google的解决方案" class="headerlink" title="Google的解决方案"></a>Google的解决方案</h2><p>谷歌使用<strong>MapReduce</strong>算法解决了这个问题。该算法将任务划分为多个小部分，并将这些小部分分配给通过网络连接的多台计算机，最后收集结果形成最终的结果数据集。</p><p>｛% qnimg mapreduce.jpg title:mapreduce alt:mapreduce %｝</p><p>上图显示了各种各样的商品硬件，这些硬件可以是单CPU机器，也可以是容量更大的服务器。</p><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><p>Doug Cutting、Mike Cafarella和团队采用了谷歌提供的解决方案，并在2005年启动了一个名为HADOOP 的开源项目，Doug以他儿子的玩具大象命名了这个项目。现在Apache Hadoop是Apache软件基金会的注册商标。</p><p>Hadoop使用MapReduce算法运行应用程序，数据在不同的CPU节点上并行处理。简而言之，Hadoop框架有足够的能力开发能够在计算机集群上运行的应用程序，并且能够对大量数据执行完整的统计分析。</p><img title="hadoop_framework" alt="hadoop_framework" src="http://cdn.tangguangen.com/images/hadoop_framework.jpg"><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfYmlnX2RhdGFfc29sdXRpb25zLmh0bWw=" title="https://www.tutorialspoint.com/hadoop/hadoop_big_data_solutions.html">https://www.tutorialspoint.com/hadoop/hadoop_big_data_solutions.html<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;传统方法&quot;&gt;&lt;a href=&quot;#传统方法&quot; class=&quot;headerlink&quot; title=&quot;传统方法&quot;&gt;&lt;/a&gt;传统方法&lt;/h2&gt;&lt;p&gt;在这种方式下，企业将使用一台计算机来存储和处理数据，处理所需的数据，并将其呈现给用户以供分析之用。在这里，数据将存储在RDB
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/Hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：大数据概述</title>
    <link href="https://tangguangen.com/hadoop-big-data-overview/"/>
    <id>https://tangguangen.com/hadoop-big-data-overview/</id>
    <published>2018-11-30T12:12:28.000Z</published>
    <updated>2018-11-30T14:19:27.389Z</updated>
    
    <content type="html"><![CDATA[<p>由于新技术、新设备和社交网站等通信手段的出现，人类产生的数据量每年都在迅速增长。2003年之前的所有数据量总和是50亿G。如果你把数据以磁盘的形式堆起来，它可能会填满整个足球场。到了2011年，每两天就能创造同样的数量，2013年每十分钟创造同样的数量。这一比例仍在大幅增长。虽然所有这些信息都是有意义的，并且在处理时很有用，但它却被忽略了。</p><blockquote><p>世界上90%的数据是在过去几年生成的。</p></blockquote><h2 id="什么是大数据"><a href="#什么是大数据" class="headerlink" title="什么是大数据"></a>什么是大数据</h2><p>大数据其实就是海量的数据，它是不能用传统计算技术处理的海量数据集的集合。<strong>大数据不仅仅是一种数据，它已经成为一门完整的学科，涉及到各种工具、技术和框架。</strong></p><h2 id="大数据的来源"><a href="#大数据的来源" class="headerlink" title="大数据的来源"></a>大数据的来源</h2><p>大数据涉及不同设备和应用产生的数据。以下是大数据保护下的一些领域。</p><ul><li><strong>黑匣子数据</strong>：是直升机、飞机、喷气机等的组成部分，它可以捕捉机组人员的声音、麦克风和耳机的录音，以及飞机的性能信息。</li><li><strong>社交媒体数据：</strong>Facebook和Twitter等社交媒体包含全球数百万人发布的信息和观点。</li><li><strong>股票交易数据：</strong>股票交易数据包含客户对不同公司股票的“买入”和“卖出”决策的信息。</li><li><strong>电网数据：</strong>电网数据包含特定节点相对于基站所消耗的信息。</li><li><strong>搜索引擎数据：</strong>搜索引擎从不同的数据库检索大量数据。</li></ul><p>｛% qnimg big_data.jpg title: big data alt: Big Data %｝</p><p>因此，大数据包括大容量、高速度和可扩展的各种数据。其中的数据有三种类型。</p><ul><li><strong>结构化数据:</strong> 关系型数据库。</li><li><strong>半结构化数据: </strong>XML数据。</li><li><strong>非结构化数据:</strong> Word、PDF、文本、媒体日志。</li></ul><h2 id="大数据带来的好处"><a href="#大数据带来的好处" class="headerlink" title="大数据带来的好处"></a>大数据带来的好处</h2><p>大数据对我们的生活至关重要，它正在成为现代世界最重要的技术之一。下面是我们大家都知道的几个好处:</p><ul><li>利用Facebook等社交网络中保存的信息，营销机构正在了解他们的活动、促销和其他广告媒介的效果。</li><li>利用社交媒体上的信息，如消费者的喜好和对产品满意度，产品公司和零售组织正在优化他们的生产。</li><li>利用患者既往病史资料，医院提供更好、更快的服务。</li></ul><h2 id="大数据技术"><a href="#大数据技术" class="headerlink" title="大数据技术"></a>大数据技术</h2><p>大数据技术在提供更准确的分析方面有很重要的作用，这可以提供更具体的决策，从而提高运营效率，降低成本，降低业务风险。</p><p>想要利用大数据的力量，你需要一个能够实时管理和处理海量结构化和非结构化数据、能够保护数据隐私和安全的基础设施。</p><p>市场上有来自亚马逊、IBM、微软等不同厂商的各种处理大数据的技术。在研究处理大数据的技术时，我们考察了以下两类技术:</p><h3 id="大数据操作"><a href="#大数据操作" class="headerlink" title="大数据操作"></a>大数据操作</h3><p>这包括像MongoDB这样的系统，它提供了实时、交互式工作负载的操作能力，数据主要是在这些工作负载中捕获和存储的。</p><p>NoSQL大数据系统旨在利用过去十年出现的新的云计算架构，以低成本和高效率运行大量计算。这使得操作大数据工作负载更容易管理、更便宜、实现更快。</p><p>一些NoSQL系统可以提供基于实时数据的模式和趋势的洞察，而只需最少的编码，并且不需要数据科学家和额外的基础设施。</p><h3 id="大数据分析"><a href="#大数据分析" class="headerlink" title="大数据分析"></a>大数据分析</h3><p>这包括大规模并行处理(Massively Parallel Processing)数据库系统和MapReduce系统，它们提供可追溯和复杂的分析能力，可能涉及大部分或所有数据的分析。</p><p>MapReduce提供了一种新的数据分析方法，它是SQL提供的功能的补充，并且基于MapReduce的系统可以从单个服务器扩展到数千台高端和低端机器。</p><p>这两类技术是互补的，经常一起部署。</p><h3 id="操作VS分析"><a href="#操作VS分析" class="headerlink" title="操作VS分析"></a>操作VS分析</h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">操作</th><th style="text-align:center">分析</th></tr></thead><tbody><tr><td style="text-align:center">延迟</td><td style="text-align:center">1 ms - 100 ms</td><td style="text-align:center">1 min - 100 min</td></tr><tr><td style="text-align:center">并发</td><td style="text-align:center">1000 - 100,000</td><td style="text-align:center">1 - 10</td></tr><tr><td style="text-align:center">访问模式</td><td style="text-align:center">Writes and Reads</td><td style="text-align:center">Reads</td></tr><tr><td style="text-align:center">查询</td><td style="text-align:center">Selective</td><td style="text-align:center">Unselective</td></tr><tr><td style="text-align:center">数据使用范围</td><td style="text-align:center">Operational</td><td style="text-align:center">Retrospective</td></tr><tr><td style="text-align:center">End User</td><td style="text-align:center">Customer</td><td style="text-align:center">Data Scientist</td></tr><tr><td style="text-align:center">技术</td><td style="text-align:center">NoSQL</td><td style="text-align:center">MapReduce, MPP Database</td></tr></tbody></table><h2 id="大数据的挑战"><a href="#大数据的挑战" class="headerlink" title="大数据的挑战"></a>大数据的挑战</h2><p>与大数据相关的主要挑战如下:</p><ul><li>数据采集</li><li>管理</li><li>存储</li><li>搜索</li><li>共享</li><li>传输</li><li>分析</li><li>展示</li></ul><p>为了完成上述挑战，通常需要企业服务器的帮助。</p><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfYmlnX2RhdGFfb3ZlcnZpZXcuaHRt" title="https://www.tutorialspoint.com/hadoop/hadoop_big_data_overview.htm">https://www.tutorialspoint.com/hadoop/hadoop_big_data_overview.htm<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于新技术、新设备和社交网站等通信手段的出现，人类产生的数据量每年都在迅速增长。2003年之前的所有数据量总和是50亿G。如果你把数据以磁盘的形式堆起来，它可能会填满整个足球场。到了2011年，每两天就能创造同样的数量，2013年每十分钟创造同样的数量。这一比例仍在大幅增长
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/Hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程</title>
    <link href="https://tangguangen.com/hadoop-tutorial-home/"/>
    <id>https://tangguangen.com/hadoop-tutorial-home/</id>
    <published>2018-11-30T11:53:46.000Z</published>
    <updated>2018-12-01T09:37:12.026Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Hadoop教程目录：</strong></p><ol><li><a href="https://tangguangen.com/hadoop-big-data-overview/">大数据概述</a></li><li><a href="https://tangguangen.com/hadoop-big-data-solutions/">大数据解决方案</a></li><li><a href="https://tangguangen.com/hadoop-introduction/">Hadoop介绍</a></li><li><a href="https://tangguangen.com/hadoop-enviornment-setup">Hadoop安装与环境设置</a></li><li><a href="https://tangguangen.com/hadoop-hdfs-overview">HDFS概述</a></li></ol><p>Hadoop以Apache 2.0许可协议发布的<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUJDJTgwJUU2JUJBJTkw" title="https://zh.wikipedia.org/wiki/%E5%BC%80%E6%BA%90">开源<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU4JUJCJTlGJUU5JUFCJTk0JUU2JUExJTg2JUU2JTlFJUI2" title="https://zh.wikipedia.org/wiki/%E8%BB%9F%E9%AB%94%E6%A1%86%E6%9E%B6">软件框架<i class="fa fa-external-link"></i></span>。用户可以通过简单的程序模型在分布式环境集群中存储和处理大数据。 它旨在从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和存储。</p><img title="Hadoop生态系统" alt="Hadoop生态系统" src="http://cdn.tangguangen.com/images/apache-hadoop-ecosystem.png"><p>本教程简要介绍了大数据，MapReduce算法和Hadoop分布式文件系统（HDFS）。</p><p><strong>Audience</strong></p><p>本教程是为希望学习使用Hadoop框架进行大数据分析的基础知识并成为Hadoop开发人员的专业人士准备的。软件专业人员、分析专业人员和ETL开发人员是本课程的主要受益者。</p><p><strong>Prerequisites</strong></p><p>在开始学习本教程之前，我们假设您已经接触过核心Java、数据库概念和任何Linux操作系统风格。</p><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9pbmRleC5odG0=" title="https://www.tutorialspoint.com/hadoop/index.htm">https://www.tutorialspoint.com/hadoop/index.htm<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Hadoop教程目录：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://tangguangen.com/hadoop-big-data-overview/&quot;&gt;大数据概述&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https:/
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/Hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Java中的String、StringBuffer、StringBuilder有什么区别？</title>
    <link href="https://tangguangen.com/string-stringbuffer-and-stringbuilder-in-java/"/>
    <id>https://tangguangen.com/string-stringbuffer-and-stringbuilder-in-java/</id>
    <published>2018-11-30T06:16:15.000Z</published>
    <updated>2018-11-30T07:46:59.542Z</updated>
    
    <content type="html"><![CDATA[<p>String、StringBuffer、StringBuilder有什么区别？这个问题在面试中经常碰到，今天主要讲解一下如何<strong>理解Java中的String、StringBuffer、StringBuilder</strong>。</p><h2 id="典型回答"><a href="#典型回答" class="headerlink" title="典型回答"></a>典型回答</h2><p><strong>String</strong> 是 Java 语言非常基础和重要的类，提供了构造和管理字符串的各种基本逻辑。它是典型的<strong>不可变类</strong>（ Immutable ），被声明成为 <strong>final class</strong>，所有属性也都是 final 的。也由于它的不可变性，类似拼接、裁剪字符串等动作，都会产生新的 String 对象。由于字符串操作的普遍性，所以相关操作的效率往往对应用性能有明显影响。</p><p><strong>StringBuffer</strong> 是为解决上面提到拼接产生太多中间对象的问题而提供的一个类，我们可以用 append 或者 add 方法，把字符串添加到已有序列的末尾或者指定位置。StringBuffer 本质是一个<strong>线程安全</strong>的可修改字符序列，它保证了线程安全，也随之带来了额外的性能开销，所以除非有线程安全的需要，不然还是推荐使用它的后继者，也就是 StringBuilder。</p><p><strong>StringBuilder</strong> 是 Java 1.5 中新增的，在能力上和 StringBuffer 没有本质区别，但是它<strong>去掉了线程安全</strong>的部分，有效减小了开销，<strong>是绝大部分情况下进行字符串拼接的首选</strong>。</p><h2 id="考点分析"><a href="#考点分析" class="headerlink" title="考点分析"></a>考点分析</h2><p>几乎所有的应用开发都离不开操作字符串，理解字符串的设计和实现以及相关工具如拼接类的使用，对写出高质量代码是非常有帮助的。关于这个问题，前面的回答是一个通常的概要性回答，至少你要知道 <strong>String 是 Immutable</strong> 的，<strong>字符串操作不当可能会产生大量临时字符串，以及线程安全方面的区别</strong>。</p><p>如果继续深入，面试官可以从各种不同的角度考察，比如可以：</p><ul><li>通过 String 和相关类，考察基本的线程安全设计与实现，各种基础编程实践。</li><li>考察 JVM 对象缓存机制的理解以及如何良好地使用。</li><li>考察 JVM 优化 Java 代码的一些技巧。</li><li>String 相关类的演进，比如 Java 9 中实现的巨大变化。</li><li>…</li></ul><h2 id="知识扩展"><a href="#知识扩展" class="headerlink" title="知识扩展"></a>知识扩展</h2><h3 id="字符串设计和实现考量"><a href="#字符串设计和实现考量" class="headerlink" title="字符串设计和实现考量"></a>字符串设计和实现考量</h3><p>String 是 Immutable 类的典型实现，原生的保证了基础线程安全，因为你无法对它内部数据进行任何修改，这种便利甚至体现在拷贝构造函数中，由于不可变，Immutable 对象在拷贝时不需要额外复制数据。</p><p>我们再来看看 StringBuffer 实现的一些细节，它的线程安全是通过把各种修改数据的方法都加上 synchronized 关键字实现的，非常直白。其实，这种简单粗暴的实现方式，非常适合我们常见的线程安全类实现，不必纠结于 synchronized 性能之类的，有人说“过早优化是万恶之源”，考虑可靠性、正确性和代码可读性才是大多数应用开发最重要的因素。</p><p>为了实现修改字符序列的目的，<strong>StringBuffer 和 StringBuilder</strong> 底层都是利用可修改的（char，JDK 9 以后是 byte）数组，二者都继承了 AbstractStringBuilder，里面包含了基本操作，<strong>区别仅在于最终的方法是否加了 synchronized</strong>。</p><p>在具体的代码书写中，应该如何选择呢？</p><p>在没有线程安全问题的情况下，全部拼接操作是应该都用 StringBuilder 实现吗？毕竟这样书写的代码，还是要多敲很多字的，可读性也不理想，下面的对比非常明显。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">String strByBuilder  = <span class="keyword">new</span></span><br><span class="line">StringBuilder().append(<span class="string">"aa"</span>).append(<span class="string">"bb"</span>).append(<span class="string">"cc"</span>).append</span><br><span class="line">            (<span class="string">"dd"</span>).toString();</span><br><span class="line">             </span><br><span class="line">String strByConcat = <span class="string">"aa"</span> + <span class="string">"bb"</span> + <span class="string">"cc"</span> + <span class="string">"dd"</span>;</span><br></pre></td></tr></table></figure><p>其实，在通常情况下，没有必要过于担心，要相信 Java 还是非常智能的。</p><p>我们来做个实验，把下面一段代码，利用不同版本的 JDK 编译，然后再反编译，例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringConcat</span> </span>&#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">          String myStr = <span class="string">"aa"</span> + <span class="string">"bb"</span> + <span class="string">"cc"</span> + <span class="string">"dd"</span>;   </span><br><span class="line">           System.out.println(<span class="string">"My String:"</span> + myStr);   </span><br><span class="line">      &#125; </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>先编译再反编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/javac StringConcat.java</span><br><span class="line"><span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/javap -v StringConcat.class</span><br></pre></td></tr></table></figure><p>JDK 8 的输出片段是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> 0: ldc           <span class="comment">#2                  // String hellojava!</span></span><br><span class="line"> 2: astore_1</span><br><span class="line"> 3: getstatic     <span class="comment">#3                  // Field java/lang/System.out:Ljava/io/PrintStream;</span></span><br><span class="line"> 6: new           <span class="comment">#4                  // class java/lang/StringBuilder</span></span><br><span class="line"> 9: dup</span><br><span class="line">10: invokespecial <span class="comment">#5                  // Method java/lang/StringBuilder."&lt;init&gt;":()V</span></span><br><span class="line">13: ldc           <span class="comment">#6                  // String Concat String:</span></span><br><span class="line">15: invokevirtual <span class="comment">#7                  // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;</span></span><br><span class="line">18: aload_1</span><br><span class="line">19: invokevirtual <span class="comment">#7                  // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;</span></span><br><span class="line">22: invokevirtual <span class="comment">#8                  // Method java/lang/StringBuilder.toString:()Ljava/lang/String;</span></span><br><span class="line">25: invokevirtual <span class="comment">#9                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V</span></span><br><span class="line">28: <span class="built_in">return</span></span><br></pre></td></tr></table></figure><p>你可以看到，在 JDK 8 中，字符串拼接操作会自动被 javac 转换为 StringBuilder 操作，而在 JDK 9 里面则是因为 Java 9 为了更加统一字符串操作优化，提供了 StringConcatFactory，作为一个统一的入口。javac 自动生成的代码，虽然未必是最优化的，但普通场景也足够了，你可以酌情选择。</p><p><strong>参考自极客时间：Java核心技术36讲</strong></p><p><strong>感谢原作者：杨晓峰老师</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;String、StringBuffer、StringBuilder有什么区别？这个问题在面试中经常碰到，今天主要讲解一下如何&lt;strong&gt;理解Java中的String、StringBuffer、StringBuilder&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&quot;典型回
      
    
    </summary>
    
      <category term="Java" scheme="https://tangguangen.com/categories/Java/"/>
    
    
      <category term="java" scheme="https://tangguangen.com/tags/java/"/>
    
      <category term="string" scheme="https://tangguangen.com/tags/string/"/>
    
      <category term="stringbuffer" scheme="https://tangguangen.com/tags/stringbuffer/"/>
    
      <category term="stringbuilder" scheme="https://tangguangen.com/tags/stringbuilder/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 344.反转字符串</title>
    <link href="https://tangguangen.com/leetcode-344-reverse-string/"/>
    <id>https://tangguangen.com/leetcode-344-reverse-string/</id>
    <published>2018-11-30T04:49:20.000Z</published>
    <updated>2018-11-30T07:33:23.791Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><blockquote><p>编写一个函数，其作用是将输入的字符串反转过来。</p><p><strong>示例 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: &quot;hello&quot;</span><br><span class="line">&gt; 输出: &quot;olleh&quot;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>示例 2:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: &quot;A man, a plan, a canal: Panama&quot;</span><br><span class="line">&gt; 输出: &quot;amanaP :lanac a ,nalp a ,nam A&quot;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ol><li><p>使用StringBuider或StringBuffer修改字符序列，关于StringBuilder和Stringbuffer的区别可以参考我的零一篇文章<a href="https://tangguangen.com/string-stringbuffer-and-stringbuilder-in-java/">Java中的String、StringBuffer、StringBuilder有什么区别？</a></p></li><li><p>将字符串转化为字符串数组，对换首尾字符位置即可。</p><h2 id="Java-AC"><a href="#Java-AC" class="headerlink" title="Java AC"></a>Java AC</h2><ol><li>使用StringBuilder</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">reverseString</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        String res = <span class="keyword">new</span> StringBuilder(s).reverse().toString();</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>转换为数组</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">reverseString</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="keyword">char</span>[] arr = s.toCharArray();</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>, j = arr.length - <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt; j) &#123;</span><br><span class="line"><span class="keyword">char</span> tmp = arr[i];</span><br><span class="line">arr[i] = arr[j];</span><br><span class="line">arr[j] = tmp;</span><br><span class="line">i++;</span><br><span class="line">j--;</span><br><span class="line">&#125;</span><br><span class="line">String str = String.valueOf(arr);</span><br><span class="line"><span class="keyword">return</span> str;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;编写一个函数，其作用是将输入的字符串反转过来。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例 1:&lt;/str
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle比赛top2%特征探索之featexp</title>
    <link href="https://tangguangen.com/secret-to-be-in-top-2-of-kaggle/"/>
    <id>https://tangguangen.com/secret-to-be-in-top-2-of-kaggle/</id>
    <published>2018-11-30T02:03:21.000Z</published>
    <updated>2018-11-30T02:39:12.071Z</updated>
    
    <content type="html"><![CDATA[<p>在数值数据上构建任意监督学习模型的一个重要方面是理解特征。查看模型的部分依赖图可帮助理解任意特征对模型输出的影响。</p><img title="模型依赖图" alt="模型依赖图" src="http://cdn.tangguangen.com/images/模型依赖图.png"><p>但是，部分依赖图存在一个问题，即它们是使用训练好的模型创建的。如果我们可以从训练数据中直接创建部分依赖图，那么它将帮助我们更好地理解底层数据。事实上，它能够帮助你做好以下事情：</p><ol><li>特征理解</li><li>识别带噪声的特征 (<strong>the most interesting part!</strong>)</li><li>特征工程</li><li>特征重要性</li><li>特征 debug</li><li>泄露检测和理解</li><li>模型监控</li></ol><p>为了使其更加易于使用，作者将这些技术封装进一个 Python 包 featexp 中，本文将介绍如何使用它进行特征探索。本文使用的是 Kaggle Home Credit Default Risk 竞赛的应用数据集。该竞赛的任务是使用给定数据预测违约者。</p><p>featexp：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FiaGF5c3Bhd2FyL2ZlYXRleHA=" title="https://github.com/abhayspawar/featexp">https://github.com/abhayspawar/…<i class="fa fa-external-link"></i></span></p><p><strong>1. 特征理解</strong></p><img title="Scatter" alt="Scatter" src="http://cdn.tangguangen.com/images/feature.png"><p>如果依赖变量（目标）是二元的，则散点图无效，因为所有点要么是 0 要么是 1。对于连续目标来说，数据点太多会造成难以理解目标 vs 特征趋势。featexp 创建了更好的图，可帮助解决该问题。我们来试一下！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> featexp <span class="keyword">import</span> get_univariate_plots</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plots drawn for all features if nothing is passed in feature_list parameter.</span></span><br><span class="line">get_univariate_plots(data=data_train, target_col=<span class="string">'target'</span>, </span><br><span class="line">                     features_list=[<span class="string">'DAYS_BIRTH'</span>], bins=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><img title="target" alt="Feature" src="http://cdn.tangguangen.com/images/target.png"><p>featexp 为数值特征创建了同等人口数量的 bin（x 轴），然后计算每个 bin 的目标平均值，再绘制出来（如上图左）。在我们的案例中，目标平均值是违约率。该图告诉我们年龄越大的客户违约率越低。这些图帮助我们理解特征表达的意义，及其对模型的影响。右图显示了每个 bin 中客户的数量。</p><p><strong>2. 识别带噪声的特征</strong></p><p>带噪声的特征导致过拟合，识别它们并非易事。在 featexp 中，你可以输出一个测试集（或者验证集），对比训练／测试集中的特征趋势来确定带噪声的特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_univariate_plots(data=data_train, target_col=<span class="string">'target'</span>, data_test=data_test, features_list=[<span class="string">'DAYS_EMPLOYED'</span>])</span><br></pre></td></tr></table></figure><img title="Comparison" alt="Comparison" src="http://cdn.tangguangen.com/images/1_tpjxrjbxhH-lJo0hbRerfg.png"><p>featexp 计算两个指标（如上图所示），来帮助测量噪声：</p><ol><li>趋势相关度（见测试图）：如果某个特征未体现目标在训练集和测试集中的同样趋势，它会导致过拟合，因为模型会学习一些在测试数据中并不使用的东西。趋势相关度有助于理解训练／测试趋势的相似度，如何利用训练和测试集的 bin 的平均目标值来计算趋势相关度。上图中的特征相关度为 99%，几乎没有噪声。</li><li>趋势变化：趋势方向中突然和重复的变化可能表明有噪声。但是，此类趋势变化也会在 bin 的人口数量与其它特征不同时，导致其违约率无法与其它 bin 进行对比。</li></ol><p>下图中的特征没有展现同样的趋势，因为趋势相关度为 85%。这两个指标可用于删除带噪声的特征。</p><img title="Example" alt="Example" src="http://cdn.tangguangen.com/images/1_6lSWurF_qOzm1cMEJFuRmA.png"><p>当特征很多且相互关联时，删除低趋势相关度特征的效果很好。它会带来更少的过拟合，其它相关特征可以避免信息损失。同时需要注意不要删除太多重要特征，因为这可能导致性能下降。此外，你无法利用特征重要性来判断特征是否带噪声，因为重要的特征也会带噪声！</p><p>使用不同时间段的测试数据效果更好，因为你可以借此确定特征趋势是否一直如此。</p><p>featexp 中的 get_trend_stats() 函数返回展示趋势相关度的数据帧，并随着特征而改变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> featexp <span class="keyword">import</span> get_trend_stats</span><br><span class="line">stats = get_trend_stats(data=data_train, target_col=<span class="string">'target'</span>, data_test=data_test)</span><br></pre></td></tr></table></figure><img src="http://cdn.tangguangen.com/images/1_RuxmJA0iWrMRCVxNGlBidw.png"><p>下面我们就试着删除数据中低趋势相关度的特征，然后看结果是否有所改进。</p><img title="AUC" alt="AUC" src="http://cdn.tangguangen.com/images/1_UR-SlR1rZOjp0sjTIaPZ_A.png"><p>我们可以看到，趋势相关度阈值越高，排行榜（LB）AUC 越高。不删除重要的特征进一步将 LB AUC 提高到 0.74。测试 AUC 的变化与 LB AUC 不同，这一点也很有趣。完整代码详见 featexp_demo notebook：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FiaGF5c3Bhd2FyL2ZlYXRleHAvYmxvYi9tYXN0ZXIvZmVhdGV4cF9kZW1vLmlweW5i" title="https://github.com/abhayspawar/featexp/blob/master/featexp_demo.ipynb">https://github.com/abhayspawar/…<i class="fa fa-external-link"></i></span>。</p><p><strong>3. 特征工程</strong></p><p>通过查看这些图所获取的见解可以帮助你创建更好的特征。更好地理解数据将带来更好的特征工程。此外，它还可以帮助你改善现有特征。下面我们来看另一个特征 EXT_SOURCE_1：</p><img src="http://cdn.tangguangen.com/images/1_MQQrhVy5NjtD-7mrKT8jOA.png"><p>具备高 EXT_SOURCE_1 的客户具备较低的违约率。但是，第一个 bin（违约率约 8%）没有遵循该特征趋势（向上升后下降）。它的负值是-99.985，而且人口数量较多。这可能表明这些是特殊的值，因此不遵循特征趋势。幸运的是，非线性模型在学习该关系方面不会有问题。而对于线性模型（如 logistic 回归），此类特殊值和空缺值应该采用类似样本的默认值进行估计，而不是特征平均值。</p><p><strong>4. 特征重要性</strong></p><p>featexp 还可以帮助衡量特征重要性。DAYS_BIRTH 和 EXT_SOURCE_1 都具备很好的趋势。但是 EXT_SOURCE_1 的人口数量集中于特殊值 bin，这表明其重要性可能不如 DAYS_BIRTH。基于 XGBoost 模型的特征重要性，DAYS_BIRTH 的重要性高于 EXT_SOURCE_1。</p><p><strong>5. 特征 debug</strong></p><p>查看 featexp 图可以帮助你捕捉复杂特征工程中的 bug：</p><img src="http://cdn.tangguangen.com/images/1_-NA-fc1LR1yo0JoHp8IFOw.png"><p>检查一下特征的人数分布看起来是否正确。由于存在一些小 bug，我个人经常遭遇上述极端情况。</p><p>在看这些图之前，一定要假设特征趋势会是什么样子。如果特征趋势看起来不像你期望的那样，可能表示其中存在一些问题。坦率地说，这种假设趋势的过程使得构建 ML 模型更加有趣！</p><p><strong>6 泄露检测</strong></p><p>从目标到特征的数据泄露会导致过拟合。泄露特征具有很高的特征重要性，但很难理解为什么特征会发生泄露。查看下列 featexp 图可以帮助你理解。</p><p>下面的特征在「Null」bin 中违约率为 0%，在其它 bin 中为 100%。很明显，这是极端的泄露案例。该特征只有在客户违约时才有价值。根据特征是什么，这可能是因为 bug 或者该特征只为违约者填充（在这种情况下它会下降）。弄清楚特征泄露的原因可以加速 debug。</p><img src="http://cdn.tangguangen.com/images/1_muwhOmAYJTjSZetBv1UOaA.png"><p><strong>7 模型监控</strong></p><p>由于 featexp 计算两个数据集之间的趋势相关性，它可以很轻易地用于模型监控。每次模型被重新训练之后，就可以把新的训练数据与测试好的训练数据（通常从第一次构建模型开始训练数据）进行对比。趋势相关性能够帮助你监控特征信息与目标的关系是否发生任何变化。</p><hr><p><strong>感谢原作者：Abhay Pawar</strong></p><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly90b3dhcmRzZGF0YXNjaWVuY2UuY29tL215LXNlY3JldC1zYXVjZS10by1iZS1pbi10b3AtMi1vZi1hLWthZ2dsZS1jb21wZXRpdGlvbi01N2NmZjA2NzdkM2M=" title="https://towardsdatascience.com/my-secret-sauce-to-be-in-top-2-of-a-kaggle-competition-57cff0677d3c">https://towardsdatascience.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在数值数据上构建任意监督学习模型的一个重要方面是理解特征。查看模型的部分依赖图可帮助理解任意特征对模型输出的影响。&lt;/p&gt;
&lt;img title=&quot;模型依赖图&quot; alt=&quot;模型依赖图&quot; src=&quot;http://cdn.tangguangen.com/images/模型依赖图
      
    
    </summary>
    
      <category term="Kaggle" scheme="https://tangguangen.com/categories/Kaggle/"/>
    
    
      <category term="kaggle" scheme="https://tangguangen.com/tags/kaggle/"/>
    
      <category term="featexp" scheme="https://tangguangen.com/tags/featexp/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 网站优化之SEO</title>
    <link href="https://tangguangen.com/Hexo-website-seo/"/>
    <id>https://tangguangen.com/Hexo-website-seo/</id>
    <published>2018-11-29T03:05:10.000Z</published>
    <updated>2018-11-29T11:36:00.206Z</updated>
    
    <content type="html"><![CDATA[<p>搭建好了个人博客，如果想让Google和百度等搜索引擎搜索到自己的博客，增加博客的访问量，那么对网站做一些SEO(Search Engine Optimization)，即搜索引擎优化，是非常有必要的，下面介绍一些简单的SEO优化方法。</p><h2 id="添加sitemap"><a href="#添加sitemap" class="headerlink" title="添加sitemap"></a>添加sitemap</h2><p>Sitemap即网站地图，它的作用在于便于搜索引擎更加智能地抓取网站。最简单和常见的sitemap形式，是XML文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新时间、更新的频率及相对其他网址重要程度等）。</p><p><strong>Setp 1:</strong> 安装sitemap站点地图自动生成插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-sitemap --save</span><br><span class="line">npm install hexo-generator-baidu-sitemap --save</span><br></pre></td></tr></table></figure><p><strong>Setp 2:</strong> 配置站点跟目录下的_config.yml，添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hexo sitemap网站地图</span></span><br><span class="line">sitemap:</span><br><span class="line">path: sitemap.xml</span><br><span class="line">baidusitemap:</span><br><span class="line">path: baidusitemap.xml</span><br></pre></td></tr></table></figure><h2 id="添加robots-txt"><a href="#添加robots-txt" class="headerlink" title="添加robots.txt"></a>添加robots.txt</h2><p>obots.txt是一种存放于网站根目录下的ASCII编码的文本文件，它的作用是告诉搜索引擎此网站中哪些内容是可以被爬取的，哪些是禁止爬取的。robots.txt应该放在站点目录下的source文件中，网站生成后在网站的根目录(<code>站点目录/public/</code>)下。</p><p>我的robots.txt内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># hexo robots.txt</span><br><span class="line">User-agent: * </span><br><span class="line">Allow: /</span><br><span class="line">Allow: /archives/</span><br><span class="line">Allow: /categories/</span><br><span class="line">Allow: /about/</span><br><span class="line"></span><br><span class="line">Disallow: /vendors/</span><br><span class="line">Disallow: /js/</span><br><span class="line">Disallow: /css/</span><br><span class="line">Disallow: /fonts/</span><br><span class="line">Disallow: /vendors/</span><br><span class="line">Disallow: /fancybox/</span><br><span class="line"></span><br><span class="line">Sitemap: https://tangguangen.com/sitemap.xml</span><br><span class="line">Sitemap: https://tangguangen.com/baidusitemap.xml</span><br></pre></td></tr></table></figure><h2 id="提交百度站长"><a href="#提交百度站长" class="headerlink" title="提交百度站长"></a>提交百度站长</h2><p><strong>Setp 1:</strong> 添加网站，百度提交网址入口<span class="exturl" data-url="aHR0cHM6Ly96aXl1YW4uYmFpZHUuY29tL3NpdGUvaW5kZXg=" title="https://ziyuan.baidu.com/site/index">点这里<i class="fa fa-external-link"></i></span></p><img title="百度站长添加网站" alt="百度站长添加网站" src="http://cdn.tangguangen.com/images/Hexo-website-seo/1.PNG"><p>这里按提示操作即可，网站验证方式建议使用CNAME验证。</p><p><strong>Setp2:</strong> robots验证</p><img title="百度站长robots验证" alt="百度站长robots验证" src="http://cdn.tangguangen.com/images/Hexo-website-seo/2.png"><p><strong>Setp3:</strong> 抓取诊断</p><img title="百度站长抓取诊断" alt="百度站长抓取诊断" src="http://cdn.tangguangen.com/images/Hexo-website-seo/3.png"><p>之所以要进行抓取诊断，是因为githubpage是不允许百度爬虫抓取的，如果你的网站是部署在GitHub上面的话就要选择<strong>主动推送</strong>的方式，如果抓取诊断成功的话就可以直接提交站点地图baidusitemap</p><h3 id="方式一：主动推送"><a href="#方式一：主动推送" class="headerlink" title="方式一：主动推送"></a>方式一：主动推送</h3><p><strong>Setp 1:</strong> 安装主动推送插件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-baidu-url-submit --save</span><br></pre></td></tr></table></figure><p><strong>Setp 2:</strong> 配置站点跟目录下的_config.yml，添加一下内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">baidu_url_submit:</span><br><span class="line">  count: 1 ## 提交最新的 1 个链接</span><br><span class="line">  host: tangguangen.com ## 在百度站长平台中注册的域名</span><br><span class="line">  token: your token ## 请注意这是您的秘钥，所以请不要把博客源代码发布在公众仓库里!</span><br><span class="line">  path: baidu_urls.txt ## 文本文档的地址， 新链接会保存在此文本文档里</span><br></pre></td></tr></table></figure><img title="获取主动推送token" alt="获取主动推送token" src="http://cdn.tangguangen.com/images/Hexo-website-seo/4.png"><p><strong>Setp 3:</strong>  配置站点跟目录下的_config.yml，添加deploy类型</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">deploy: </span><br><span class="line">  - type: git</span><br><span class="line">    repo: git@github.com:xguojing/xguojing.github.io.git</span><br><span class="line">    branch: master</span><br><span class="line">  - type: baidu_url_submitter # 这里是新添加的</span><br><span class="line">    repo:</span><br></pre></td></tr></table></figure><p>最后记得把<strong>主题配置文件</strong>中的<code>baidu_push</code>设置为<code>true</code>，重新渲染部署即可。</p><h3 id="方式二：提交sitemap"><a href="#方式二：提交sitemap" class="headerlink" title="方式二：提交sitemap"></a>方式二：提交sitemap</h3><img title="提交百度站点地图" alt="提交百度站点地图" src="http://cdn.tangguangen.com/images/Hexo-website-seo/5.png"><h2 id="提交Google站长"><a href="#提交Google站长" class="headerlink" title="提交Google站长"></a>提交Google站长</h2><p><strong>Setp 1:</strong> 添加网站</p><p>进入<span class="exturl" data-url="aHR0cHM6Ly9saW5rLmppYW5zaHUuY29tLz90PWh0dHBzJTNBJTJGJTJGd3d3Lmdvb2dsZS5jb20lMkZ3ZWJtYXN0ZXJzJTJG" title="https://link.jianshu.com/?t=https%3A%2F%2Fwww.google.com%2Fwebmasters%2F">Google Search Console<i class="fa fa-external-link"></i></span>，相信大家都有Google账号吧。没有的话注册个账号吧，然后登录进去即可。</p><p>按提示操作添加网址，然后Google会让你验证你对网站的所有权，我选择的是HTML标记的验证方式</p><img src="http://cdn.tangguangen.com/images/Hexo-website-seo/6.png"><p>配置<strong>主题配置文件</strong>_config.yml，填写google_site_verification，之后重新渲染部署点击验证即可</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">google_site_verification:</span> <span class="string">your</span> <span class="string">google_site_verification</span></span><br></pre></td></tr></table></figure><p><strong>Setp 2:</strong> 添加站点地图sitemap</p><img title="添加sitemap" alt="添加sitemap" src="http://cdn.tangguangen.com/images/Hexo-website-seo/7.png"><h2 id="优化title"><a href="#优化title" class="headerlink" title="优化title"></a>优化title</h2><p>编辑站点目录下的<code>themes/layout/index.swig</code>文件，</p><p>将下面的代码</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;%</span> <span class="string">block</span> <span class="string">title</span> <span class="string">%&#125;</span> <span class="string">&#123;&#123;</span> <span class="string">config.title</span> <span class="string">&#125;&#125;</span> <span class="string">&#123;%</span> <span class="string">endlock</span> <span class="string">%&#125;</span></span><br></pre></td></tr></table></figure><p>改成</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;%</span> <span class="string">block</span> <span class="string">title</span> <span class="string">%&#125;</span> <span class="string">&#123;&#123;</span> <span class="string">config.title</span> <span class="string">&#125;&#125;</span> <span class="bullet">-</span> <span class="string">&#123;&#123;</span> <span class="string">theme.description</span> <span class="string">&#125;&#125;</span> <span class="string">&#123;%</span> <span class="string">endlock</span> <span class="string">%&#125;</span></span><br></pre></td></tr></table></figure><p>这时将网站的描述及关键词加入了网站的<code>title</code>中，更有利于详细地描述网站。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>开博客主要是为了记录和分享学习经历、技术笔记等，通过记录总结的方式提高学习效率，在互联网中和更多的人交流学习。学无止境，不忘初心。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;搭建好了个人博客，如果想让Google和百度等搜索引擎搜索到自己的博客，增加博客的访问量，那么对网站做一些SEO(Search Engine Optimization)，即搜索引擎优化，是非常有必要的，下面介绍一些简单的SEO优化方法。&lt;/p&gt;
&lt;h2 id=&quot;添加site
      
    
    </summary>
    
      <category term="Hexo" scheme="https://tangguangen.com/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="https://tangguangen.com/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 292. Nim游戏</title>
    <link href="https://tangguangen.com/LeetCode-292-Nim%E6%B8%B8%E6%88%8F/"/>
    <id>https://tangguangen.com/LeetCode-292-Nim游戏/</id>
    <published>2018-11-28T07:19:19.000Z</published>
    <updated>2018-11-29T05:53:20.459Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h2><blockquote><p>你和你的朋友，两个人一起玩 <span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS9OaW0lRTYlQjglQjglRTYlODglOEYvNjczNzEwNQ==" title="https://baike.baidu.com/item/Nim%E6%B8%B8%E6%88%8F/6737105">Nim游戏<i class="fa fa-external-link"></i></span>：桌子上有一堆石头，每次你们轮流拿掉 1 - 3 块石头。 拿掉最后一块石头的人就是获胜者。你作为先手。</p><p>你们是聪明人，每一步都是最优解。 编写一个函数，来判断你是否可以在给定石头数量的情况下赢得游戏。</p><p><strong>示例:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: 4</span><br><span class="line">&gt; 输出: false </span><br><span class="line">&gt; 解释: 如果堆中有 4 块石头，那么你永远不会赢得比赛；</span><br><span class="line">&gt;      因为无论你拿走 1 块、2 块 还是 3 块石头，最后一块石头总是会被你的朋友拿走。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>简单题，<span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlQjclQjQlRTQlQkIlODAlRTUlOEQlOUElRTUlQTUlOTUvNzEzOTc4Mg==" title="https://baike.baidu.com/item/%E5%B7%B4%E4%BB%80%E5%8D%9A%E5%A5%95/7139782">巴什博弈<i class="fa fa-external-link"></i></span>。</p><p>显然，如果n=m+1，那么由于一次最多只能取m个，所以，无论先取者拿走多少个，后取者都能够一次拿走剩余的物品，后者取胜。因此我们发现了如何取胜的法则：如果n=（m+1）r+s，（r为任意自然数，s≤m),那么先取者要拿走s个物品，如果后取者拿走k（≤m)个，那么先取者再拿走m+1-k个，结果剩下（m+1）（r-1）个，以后保持这样的取法，那么先取者肯定获胜。总之，要保持给对手留下（m+1）的倍数，就能最后获胜。</p><p>对于巴什博弈，那么我们规定，如果最后取光者输，那么又会如何呢？</p><p>（n-1）%（m+1）==0则后手胜利</p><p>先手会重新决定策略，所以不是简单的相反行的</p><h2 id="JAVA-SOLUTION"><a href="#JAVA-SOLUTION" class="headerlink" title="JAVA SOLUTION"></a>JAVA SOLUTION</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canWinNim</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> n % <span class="number">4</span> == <span class="number">0</span> ? <span class="keyword">false</span> : <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><h3 id="威佐夫博奕"><a href="#威佐夫博奕" class="headerlink" title="威佐夫博奕"></a><span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlQTglODElRTQlQkQlOTAlRTUlQTQlQUIlRTUlOEQlOUElRTUlQkMlODgvMTk4NTgyNTY=" title="https://baike.baidu.com/item/%E5%A8%81%E4%BD%90%E5%A4%AB%E5%8D%9A%E5%BC%88/19858256">威佐夫博奕<i class="fa fa-external-link"></i></span></h3><p>威佐夫博弈（Wythoff’s game）：有两堆各若干个物品，两个人轮流从任一堆取至少一个或同时从两堆中取同样多的物品，规定每次至少取一个，多者不限，最后取光者得胜。</p><p><strong>两个人如果都采用正确操作，那么面对非奇异局势，先拿者必胜；反之，则后拿者取胜。</strong></p><p>那么任给一个局势， (a，b)，怎样判断它是不是奇异局势呢？我们有如下公式：<br>$$<br>ak =\lfloor \frac{k}{2}(1+\sqrt{5}) \rfloor ，<br>$$</p><p>$$<br>bk= ak + k \space \space\space\space（k=0，1，2，…n)<br>$$</p><h3 id="尼姆博奕"><a href="#尼姆博奕" class="headerlink" title="尼姆博奕"></a><span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlQjAlQkMlRTUlQTclODYlRTUlOEQlOUElRTUlQTUlOTU=" title="https://baike.baidu.com/item/%E5%B0%BC%E5%A7%86%E5%8D%9A%E5%A5%95">尼姆博奕<i class="fa fa-external-link"></i></span></h3><p>指的是这样的一个博弈游戏，目前有任意堆石子，每堆石子个数也是任意的，双方轮流从中取出石子，规则如下：<br>1)每一步应取走至少一枚石子；每一步只能从某一堆中取走部分或全部石子；<br>2)如果谁取到最后一枚石子就胜。</p><p>判断当前局势是否为必胜（必败）局势：<br>把所有堆的石子数目用二进制数表示出来，当<strong>全部这些数按位异或</strong>结果为0时当前局面为必败局面，否则为必胜局面；</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;  </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="keyword">int</span> temp[ <span class="number">20</span> ]; <span class="comment">//火柴的堆数  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> i, n, min;  </span><br><span class="line">    <span class="keyword">while</span>( <span class="built_in">cin</span> &gt;&gt; n )  </span><br><span class="line">    &#123;  </span><br><span class="line">        <span class="keyword">for</span>( i = <span class="number">0</span>; i &lt; n; i++ )  </span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; temp[ i ]; <span class="comment">//第i个火柴堆的数量  </span></span><br><span class="line">        min = temp[ <span class="number">0</span> ];  </span><br><span class="line">        <span class="keyword">for</span>( i = <span class="number">1</span>; i &lt; n ; i++ )  </span><br><span class="line">            min = min^temp[ i ]; <span class="comment">//按位异或  </span></span><br><span class="line">        <span class="keyword">if</span>( min == <span class="number">0</span> )  </span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"Lose"</span> &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//输  </span></span><br><span class="line">        <span class="keyword">else</span>  </span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"Win"</span> &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//赢  </span></span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="斐波那契博弈"><a href="#斐波那契博弈" class="headerlink" title="斐波那契博弈"></a><strong>斐波那契博弈</strong></h3><p>有一堆个数为n的石子，游戏双方轮流取石子，满足：<br>1)先手不能在第一次把所有的石子取完；<br>2)之后每次可以取的石子数介于1到对手刚取的石子数的2倍之间（包含1和对手刚取的石子数的2倍）。<br>约定取走最后一个石子的人为赢家，求必败态。</p><p>这个游戏叫做斐波那契博弈，肯定和<span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTYlOTYlOTAlRTYlQjMlQTIlRTklODIlQTMlRTUlQTUlOTElRTYlOTUlQjAlRTUlODglOTcvOTkxNDU=" title="https://baike.baidu.com/item/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/99145">斐波那契数列<i class="fa fa-external-link"></i></span>：$f[n]：1,2,3,5,8,13,21,34,55,89,… $有密切的关系。如果试验一番之后，可以猜测：<strong>先手胜当且仅当n不是斐波那契数。换句话说，必败态构成斐波那契数列。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目描述：&quot;&gt;&lt;a href=&quot;#题目描述：&quot; class=&quot;headerlink&quot; title=&quot;题目描述：&quot;&gt;&lt;/a&gt;题目描述：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;你和你的朋友，两个人一起玩 &lt;span class=&quot;exturl&quot; data-url=&quot;
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>[转载]原码, 反码, 补码 详解</title>
    <link href="https://tangguangen.com/%E8%BD%AC%E8%BD%BD-%E5%8E%9F%E7%A0%81-%E5%8F%8D%E7%A0%81-%E8%A1%A5%E7%A0%81-%E8%AF%A6%E8%A7%A3/"/>
    <id>https://tangguangen.com/转载-原码-反码-补码-详解/</id>
    <published>2018-11-26T13:42:58.000Z</published>
    <updated>2018-11-26T13:45:31.379Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作者：<span class="exturl" data-url="aHR0cDovL3d3dy5jbmJsb2dzLmNvbS96aGFuZ3ppcWl1Lw==" title="http://www.cnblogs.com/zhangziqiu/">张子秋<i class="fa fa-external-link"></i></span><br>出处：<span class="exturl" data-url="aHR0cDovL3d3dy5jbmJsb2dzLmNvbS96aGFuZ3ppcWl1Lw==" title="http://www.cnblogs.com/zhangziqiu/">http://www.cnblogs.com/zhangziqiu/<i class="fa fa-external-link"></i></span> </p></blockquote><p>本篇文章讲解了计算机的原码, 反码和补码. 并且进行了深入探求了为何要使用反码和补码, 以及更进一步的论证了为何可以用反码, 补码的加法计算原码的减法. 论证部分如有不对的地方请各位牛人帮忙指正! 希望本文对大家学习计算机基础有所帮助!</p><h2 id="一-机器数和真值"><a href="#一-机器数和真值" class="headerlink" title="一. 机器数和真值"></a>一. 机器数和真值</h2><p>在学习原码, 反码和补码之前, 需要先了解机器数和真值的概念.</p><h3 id="1、机器数"><a href="#1、机器数" class="headerlink" title="1、机器数"></a>1、机器数</h3><p>一个数在计算机中的二进制表示形式,  叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号, 正数为0, 负数为1.</p><p>比如，十进制中的数 +3 ，计算机字长为8位，转换成二进制就是00000011。如果是 -3 ，就是 10000011 。</p><p>那么，这里的 00000011 和 10000011 就是机器数。</p><h3 id="2、真值"><a href="#2、真值" class="headerlink" title="2、真值"></a>2、真值</h3><blockquote><p>因为第一位是符号位，所以机器数的形式值就不等于真正的数值。例如上面的有符号数 10000011，其最高位1代表负，其真正数值是 -3 而不是形式值131（10000011转换成十进制等于131）。所以，为区别起见，将带符号位的机器数对应的真正数值称为机器数的真值。</p></blockquote><p>例：0000 0001的真值 = +000 0001 = +1，1000 0001的真值 = –000 0001 = –1</p><h2 id="二-原码-反码-补码的基础概念和计算方法"><a href="#二-原码-反码-补码的基础概念和计算方法" class="headerlink" title="二. 原码, 反码, 补码的基础概念和计算方法."></a>二. 原码, 反码, 补码的基础概念和计算方法.</h2><p>在探求为何机器要使用补码之前, 让我们先了解原码, 反码和补码的概念.对于一个数, 计算机要使用一定的编码方式进行存储. 原码, 反码, 补码是机器存储一个具体数字的编码方式.</p><h3 id="1-原码"><a href="#1-原码" class="headerlink" title="1. 原码"></a>1. 原码</h3><p>原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是8位二进制:</p><blockquote><p>[+1]原 = 0000 0001</p><p>[-1]原 = 1000 0001</p></blockquote><p>第一位是符号位. 因为第一位是符号位, 所以8位二进制数的取值范围就是:</p><blockquote><p>[1111 1111 , 0111 1111]</p></blockquote><p>即</p><blockquote><p>[-127 , 127]</p></blockquote><p>原码是人脑最容易理解和计算的表示方式.</p><h3 id="2-反码"><a href="#2-反码" class="headerlink" title="2. 反码"></a>2. 反码</h3><p>反码的表示方法是:</p><p>正数的反码是其本身</p><p>负数的反码是在其原码的基础上, 符号位不变，其余各个位取反.</p><blockquote><p>[+1] = [00000001]原 = [00000001]反</p><p>[-1] = [10000001]原 = [11111110]反</p></blockquote><p>可见如果一个反码表示的是负数, 人脑无法直观的看出来它的数值. 通常要将其转换成原码再计算.</p><h3 id="3-补码"><a href="#3-补码" class="headerlink" title="3. 补码"></a>3. 补码</h3><p>补码的表示方法是:</p><p>正数的补码就是其本身</p><p>负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)</p><blockquote><p>[+1] = [00000001]原 = [00000001]反 = [00000001]补</p><p>[-1] = [10000001]原 = [11111110]反 = [11111111]补</p></blockquote><p>对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值.</p><h2 id="三-为何要使用原码-反码和补码"><a href="#三-为何要使用原码-反码和补码" class="headerlink" title="三. 为何要使用原码, 反码和补码"></a>三. 为何要使用原码, 反码和补码</h2><p>在开始深入学习前, 我的学习建议是先”死记硬背”上面的原码, 反码和补码的表示方式以及计算方法.</p><p>现在我们知道了计算机可以有三种编码方式表示一个数. 对于正数因为三种编码方式的结果都相同:</p><blockquote><p>[+1] = [00000001]原 = [00000001]反 = [00000001]补</p></blockquote><p>所以不需要过多解释. 但是对于负数:</p><blockquote><p>[-1] = [10000001]原 = [11111110]反 = [11111111]补</p></blockquote><p>可见原码, 反码和补码是完全不同的. 既然原码才是被人脑直接识别并用于计算表示方式, 为何还会有反码和补码呢?</p><p>首先, 因为人脑可以知道第一位是符号位, 在计算的时候我们会根据符号位, 选择对真值区域的加减. (真值的概念在本文最开头). 但是对于计算机, 加减乘数已经是最基础的运算, 要设计的尽量简单. 计算机辨别”符号位”显然会让计算机的基础电路设计变得十分复杂! 于是人们想出了将符号位也参与运算的方法. 我们知道, 根据运算法则减去一个正数等于加上一个负数, 即: 1-1 = 1 + (-1) = 0 , 所以机器可以只有加法而没有减法, 这样计算机运算的设计就更简单了.</p><p>于是人们开始探索 将符号位参与运算, 并且只保留加法的方法. 首先来看原码:</p><p>计算十进制的表达式: 1-1=0</p><blockquote><p>1 - 1 = 1 + (-1) = [00000001]原 + [10000001]原 = [10000010]原 = -2</p></blockquote><p>如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的.这也就是为何计算机内部不使用原码表示一个数.</p><p>为了解决原码做减法的问题, 出现了反码:</p><p>计算十进制的表达式: 1-1=0</p><blockquote><p>1 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原= [0000 0001]反 + [1111 1110]反 = [1111 1111]反 = [1000 0000]原 = -0</p></blockquote><p>发现用反码计算减法, 结果的真值部分是正确的. 而唯一的问题其实就出现在”0”这个特殊的数值上. 虽然人们理解上+0和-0是一样的, 但是0带符号是没有任何意义的. 而且会有[0000 0000]原和[1000 0000]原两个编码表示0.</p><p>于是补码的出现, 解决了0的符号以及两个编码的问题:</p><blockquote><p>1-1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]补 + [1111 1111]补 = [0000 0000]补=[0000 0000]原</p></blockquote><p>这样0用[0000 0000]表示, 而以前出现问题的-0则不存在了.而且可以用[1000 0000]表示-128:</p><blockquote><p>(-1) + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补</p></blockquote><p>-1-127的结果应该是-128, 在用补码运算的结果中, [1000 0000]补 就是-128. 但是注意因为实际上是使用以前的-0的补码来表示-128, 所以-128并没有原码和反码表示.(对-128的补码表示[1000 0000]补算出来的原码是[0000 0000]原, 这是不正确的)</p><p>使用补码, 不仅仅修复了0的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么8位二进制, 使用原码或反码表示的范围为[-127, +127], 而使用补码表示的范围为[-128, 127].</p><p>因为机器使用补码, 所以对于编程中常用到的32位int类型, 可以表示范围是: [-231, 231-1] 因为第一位表示的是符号位.而使用补码表示时又可以多保存一个最小值.</p><h2 id="四-原码-反码-补码-再深入"><a href="#四-原码-反码-补码-再深入" class="headerlink" title="四 原码, 反码, 补码 再深入"></a>四 原码, 反码, 补码 再深入</h2><p>计算机巧妙地把符号位参与运算, 并且将减法变成了加法, 背后蕴含了怎样的数学原理呢?</p><p>将钟表想象成是一个1位的12进制数. 如果当前时间是6点, 我希望将时间设置成4点, 需要怎么做呢?我们可以:</p><blockquote><p>\1. 往回拨2个小时: 6 - 2 = 4</p><p>\2. 往前拨10个小时: (6 + 10) mod 12 = 4</p><p>\3. 往前拨10+12=22个小时: (6+22) mod 12 =4</p></blockquote><p>2,3方法中的mod是指取模操作, 16 mod 12 =4 即用16除以12后的余数是4.</p><p>所以钟表往回拨(减法)的结果可以用往前拨(加法)替代!</p><p>现在的焦点就落在了如何用一个正数, 来替代一个负数. 上面的例子我们能感觉出来一些端倪, 发现一些规律. 但是数学是严谨的. 不能靠感觉.</p><p>首先介绍一个数学中相关的概念: 同余</p><h3 id="同余的概念"><a href="#同余的概念" class="headerlink" title="同余的概念"></a>同余的概念</h3><p>两个整数a，b，若它们除以整数m所得的余数相等，则称a，b对于模m同余</p><p>记作 a ≡ b (mod m)</p><p>读作 a 与 b 关于模 m 同余。</p><p>举例说明:</p><blockquote><p>4 mod 12 = 4</p><p>16 mod 12 = 4</p><p>28 mod 12 = 4</p></blockquote><p>所以4, 16, 28关于模 12 同余.</p><h3 id="负数取模"><a href="#负数取模" class="headerlink" title="负数取模"></a>负数取模</h3><p>正数进行mod运算是很简单的. 但是负数呢?</p><p>下面是关于mod运算的数学定义:</p><p><span class="exturl" data-url="aHR0cDovL2ltYWdlcy5jbmJsb2dzLmNvbS9jbmJsb2dzX2NvbS96aGFuZ3ppcWl1LzIwMTEwMy8yMDExMDMzMDIxNTU1MDc4OTQuanBn" title="http://images.cnblogs.com/cnblogs_com/zhangziqiu/201103/201103302155507894.jpg"><img src="https://images.cnblogs.com/cnblogs_com/zhangziqiu/201103/201103302155504514.jpg" alt="clip_image001"><i class="fa fa-external-link"></i></span></p><p>上面是截图, “取下界”符号找不到如何输入(word中粘贴过来后乱码). 下面是使用”L”和”J”替换上图的”取下界”符号:</p><blockquote><p>x mod y = x - y L x / y J</p></blockquote><p>上面公式的意思是:</p><p>x mod y等于 x 减去 y 乘上 x与y的商的下界.</p><p>以 -3 mod 2 举例:</p><blockquote><p>-3 mod 2</p><p>= -3 - 2xL -3/2 J</p><p>= -3 - 2xL-1.5J</p><p>= -3 - 2x(-2)</p><p>= -3 + 4 = 1</p></blockquote><p>所以:</p><blockquote><p>(-2) mod 12 = 12-2=10</p><p>(-4) mod 12 = 12-4 = 8</p><p>(-5) mod 12 = 12 - 5 = 7</p></blockquote><h3 id="开始证明"><a href="#开始证明" class="headerlink" title="开始证明"></a>开始证明</h3><p>再回到时钟的问题上:</p><blockquote><p>回拨2小时 = 前拨10小时</p><p>回拨4小时 = 前拨8小时</p><p>回拨5小时= 前拨7小时</p></blockquote><p>注意, 这里发现的规律!</p><p>结合上面学到的同余的概念.实际上:</p><blockquote><p>(-2) mod 12 = 10</p><p>10 mod 12 = 10</p></blockquote><p>-2与10是同余的.</p><blockquote><p>(-4) mod 12 = 8</p><p>8 mod 12 = 8</p></blockquote><p>-4与8是同余的.</p><p>距离成功越来越近了. 要实现用正数替代负数, 只需要运用同余数的两个定理:</p><p>反身性:</p><blockquote><p>a ≡ a (mod m)</p></blockquote><p>这个定理是很显而易见的.</p><p>线性运算定理:</p><blockquote><p>如果a ≡ b (mod m)，c ≡ d (mod m) 那么:</p><p>(1)a ± c ≡ b ± d (mod m)</p><p>(2)a <em> c ≡ b </em> d (mod m)</p></blockquote><p>如果想看这个定理的证明, 请看:<span class="exturl" data-url="aHR0cDovL2JhaWtlLmJhaWR1LmNvbS92aWV3Lzc5MjgyLmh0bQ==" title="http://baike.baidu.com/view/79282.htm">http://baike.baidu.com/view/79282.htm<i class="fa fa-external-link"></i></span></p><p>所以:</p><blockquote><p>7 ≡ 7 (mod 12)</p><p>(-2) ≡ 10 (mod 12)</p><p>7 -2 ≡ 7 + 10 (mod 12)</p></blockquote><p>现在我们为一个负数, 找到了它的正数同余数. 但是并不是7-2 = 7+10, 而是 7 -2 ≡ 7 + 10 (mod 12) , 即计算结果的余数相等.</p><p>接下来回到二进制的问题上, 看一下: 2-1=1的问题.</p><blockquote><p>2-1=2+(-1) = [0000 0010]原 + [1000 0001]原= [0000 0010]反 + [1111 1110]反</p></blockquote><p>先到这一步, -1的反码表示是1111 1110. 如果这里将[1111 1110]认为是原码, 则[1111 1110]原 = -126, 这里将符号位除去, 即认为是126.</p><p>发现有如下规律:</p><blockquote><p>(-1) mod 127 = 126</p><p>126 mod 127 = 126</p></blockquote><p>即:</p><blockquote><p>(-1) ≡ 126 (mod 127)</p><p>2-1 ≡ 2+126 (mod 127)</p></blockquote><p>2-1 与 2+126的余数结果是相同的! 而这个余数, 正式我们的期望的计算结果: 2-1=1</p><p>所以说一个数的反码, 实际上是这个数对于一个膜的同余数. 而这个膜并不是我们的二进制, 而是所能表示的最大值! 这就和钟表一样, 转了一圈后总能找到在可表示范围内的一个正确的数值!</p><p>而2+126很显然相当于钟表转过了一轮, 而因为符号位是参与计算的, 正好和溢出的最高位形成正确的运算结果.</p><p>既然反码可以将减法变成加法, 那么现在计算机使用的补码呢? 为什么在反码的基础上加1, 还能得到正确的结果?</p><blockquote><p>2-1=2+(-1) = [0000 0010]原 + [1000 0001]原 = [0000 0010]补 + [1111 1111]补</p></blockquote><p>如果把[1111 1111]当成原码, 去除符号位, 则:</p><blockquote><p>[0111 1111]原 = 127</p></blockquote><p>其实, 在反码的基础上+1, 只是相当于增加了膜的值:</p><blockquote><p>(-1) mod 128 = 127</p><p>127 mod 128 = 127</p><p>2-1 ≡ 2+127 (mod 128)</p></blockquote><p>此时, 表盘相当于每128个刻度转一轮. 所以用补码表示的运算结果最小值和最大值应该是[-128, 128].</p><p>但是由于0的特殊情况, 没有办法表示128, 所以补码的取值范围是[-128, 127]</p><p>本人一直不善于数学, 所以如果文中有不对的地方请大家多多包含, 多多指点!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;作者：&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cDovL3d3dy5jbmJsb2dzLmNvbS96aGFuZ3ppcWl1Lw==&quot; title=&quot;http://www.cnblogs.com/zhangziqiu
      
    
    </summary>
    
      <category term="计算机基础" scheme="https://tangguangen.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="计算机基础" scheme="https://tangguangen.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop兼容性</title>
    <link href="https://tangguangen.com/Hadoop%E5%85%BC%E5%AE%B9%E6%80%A7/"/>
    <id>https://tangguangen.com/Hadoop兼容性/</id>
    <published>2018-11-26T11:39:53.000Z</published>
    <updated>2018-11-26T12:08:52.367Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>本文档介绍了Apache Hadoop项目的兼容性目标。枚举了影响Hadoop开发人员，下游项目和最终用户的Hadoop版本之间的不同类型的兼容性。对于每种类型的兼容性，我们：</p><ul><li>描述对下游项目或最终用户的影响</li><li>在适用的情况下，当允许不兼容的更改时，请调用Hadoop开发人员采用的策略。</li></ul><h2 id="兼容性类型"><a href="#兼容性类型" class="headerlink" title="兼容性类型"></a>兼容性类型</h2><h3 id="Java-API"><a href="#Java-API" class="headerlink" title="Java API"></a>Java API</h3><p>Hadoop接口和类被注释为描述目标受众和稳定性，以保持与先前版本的兼容性。有关详细信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9JbnRlcmZhY2VDbGFzc2lmaWNhdGlvbi5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/InterfaceClassification.html">Hadoop接口分类<i class="fa fa-external-link"></i></span>。</p><ul><li>InterfaceAudience：捕获目标受众，可能的值是Public（对于最终用户和外部项目），LimitedPrivate（对于其他Hadoop组件，以及密切相关的项目，如YARN，MapReduce，HBase等）和Private（用于组件内部）。</li><li>InterfaceStability：描述允许哪些类型的接口更改。可能的值为Stable，Evolving，Unstable和Deprecated。</li></ul><h4 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h4><ul><li>需要公共稳定的API兼容性，以确保最终用户程序和下游项目继续工作而无需修改。</li><li>需要LimitedPrivate-Stable API兼容性，以允许跨次要版本升级单个组件。</li><li>滚动升级需要Private-Stable API兼容性。</li></ul><h4 id="政策"><a href="#政策" class="headerlink" title="政策"></a>政策</h4><ul><li>在主要版本中删除之前，必须至少弃用一个主要版本的Public-Stable API。</li><li>LimitedPrivate-Stable API可以在主要版本中更改，但不能在主要版本中更改。</li><li>Private-Stable API可以在主要版本中更改，但不能在主要版本中更改。</li><li>未注释的类是隐式“私有”。未注释的类成员继承封闭类的注释。</li><li>注意：从proto文件生成的API需要与滚动升级兼容。有关详细信息，请参阅有关电线兼容性的部分。API和有线通信的兼容性策略需要齐头并进，以解决这个问题。</li></ul><h3 id="语义兼容性"><a href="#语义兼容性" class="headerlink" title="语义兼容性"></a>语义兼容性</h3><p>Apache Hadoop努力确保API的行为在版本上保持一致，但正确性的更改可能会导致行为更改。测试和javadoc指定API的行为。社区正在更严格地指定一些API，并增强测试套件以验证是否符合规范，有效地为易于测试的行为子集创建了正式的规范。</p><h4 id="政策-1"><a href="#政策-1" class="headerlink" title="政策"></a>政策</h4><p>可以更改API的行为以修复不正确的行为，这种更改伴随着更新现有错误测试或在更改之前没有测试的情况下添加测试。</p><h3 id="电线兼容性"><a href="#电线兼容性" class="headerlink" title="电线兼容性"></a>电线兼容性</h3><p>线路兼容性涉及在Hadoop进程之间通过线路传输的数据。Hadoop使用Protocol Buffers进行大多数RPC通信。保持兼容性需要禁止如下所述的修改。还应考虑非RPC通信，例如使用HTTP传输HDFS映像作为快照或传输MapTask输出的一部分。潜在的沟通可以分类如下：</p><ul><li>客户端 - 服务器：Hadoop客户端和服务器之间的通信（例如，HDFS客户端到NameNode协议，或YARN客户端到ResourceManager协议）。</li><li>客户端 - 服务器（管理员）：值得区分仅由管理命令（例如，HAAdmin协议）使用的客户端 - 服务器协议的子集，因为这些协议仅影响能够容忍最终用户（使用通用客户端）的更改的管理员服务器协议）不能。</li><li>服务器 - 服务器：服务器之间的通信（例如，DataNode和NameNode之间的协议，或NodeManager和ResourceManager）</li></ul><h4 id="用例-1"><a href="#用例-1" class="headerlink" title="用例"></a>用例</h4><ul><li>即使在将服务器（群集）升级到更高版本（或反之亦然）之后，也需要客户端 - 服务器兼容性以允许用户继续使用旧客户端。例如，Hadoop 2.1.0客户端与Hadoop 2.3.0集群通信。</li><li>还需要客户端 - 服务器兼容性，以允许用户在升级服务器（群集）之前升级客户端。例如，Hadoop 2.4.0客户端与Hadoop 2.3.0集群通信。这允许在完全集群升级之前部署客户端错误修复。请注意，新客户端API或shell命令调用的新群集功能将无法使用。尝试使用尚未部署到群集的新API（包括数据结构中的新字段）的YARN应用程序可能会出现链接异常。</li><li>还需要客户端 - 服务器兼容性，以允许升级单个组件而不升级其他组件。例如，在不升级MapReduce的情况下将HDFS从版本2.1.0升级到2.2.0。</li><li>需要服务器 - 服务器兼容性以允许活动集群中的混合版本，以便可以在不停机的情况下以滚动方式升级集群。</li></ul><h4 id="政策-2"><a href="#政策-2" class="headerlink" title="政策"></a>政策</h4><ul><li>Client-Server和Server-Server兼容性都保留在主要版本中。（不同类别的不同政策尚待考虑。）</li><li>兼容性只能在主要版本中打破，但即使在主要版本中破坏兼容性也会产生严重后果，应在Hadoop社区中进行讨论。</li><li>Hadoop协议在.proto（ProtocolBuffers）文件中定义。客户端 - 服务器协议和服务器 - 服务器协议.proto文件标记为稳定。当.proto文件标记为稳定时，意味着应以兼容的方式进行更改，如下所述：<ul><li>以下更改是兼容的，并且随时允许：<ul><li>添加一个可选字段，期望代码处理由于与旧版本代码的通信而丢失的字段。</li><li>向服务添加新的rpc /方法</li><li>向Message添加新的可选请求</li><li>重命名字段</li><li>重命名.proto文件</li><li>更改影响代码生成的.proto注释（例如java包的名称）</li></ul></li><li>以下更改不兼容，但只能在主要版本中考虑<ul><li>更改rpc /方法名称</li><li>更改rpc / method参数类型或返回类型</li><li>删除rpc /方法</li><li>更改服务名称</li><li>更改消息的名称</li><li>以不兼容的方式修改字段类型（以递归方式定义）</li><li>将可选字段更改为必需</li><li>添加或删除必填字段</li><li>只要可选字段具有允许删除的合理默认值，就删除可选字段</li></ul></li><li>以下更改不兼容，因此从不允许<ul><li>更改字段ID</li><li>重用以前删除的旧字段。</li><li>字段数字很便宜，改变和重用并不是一个好主意。</li></ul></li></ul></li></ul><h3 id="最终用户应用程序的Java二进制兼容性，即Apache-Hadoop-ABI"><a href="#最终用户应用程序的Java二进制兼容性，即Apache-Hadoop-ABI" class="headerlink" title="最终用户应用程序的Java二进制兼容性，即Apache Hadoop ABI"></a>最终用户应用程序的Java二进制兼容性，即Apache Hadoop ABI</h3><p>随着Apache Hadoop修订版的升级，最终用户合理地期望他们的应用程序在没有任何修改的情况下继续工作。这是通过支持API兼容性，语义兼容性和线路兼容性来实现的。</p><p>但是，Apache Hadoop是一个非常复杂的分布式系统，可以为各种各样的用例提供服务。特别是，Apache Hadoop MapReduce是一个非常非常广泛的API; 从某种意义上说，最终用户可以做出广泛的假设，例如当他们的map / reduce任务执行时本地磁盘的布局，他们的任务的环境变量等。在这种情况下，很难完全指定和支持，绝对兼容性。</p><h4 id="用例-2"><a href="#用例-2" class="headerlink" title="用例"></a>用例</h4><ul><li>现有的MapReduce应用程序，包括现有打包的最终用户应用程序罐和Apache Pig，Apache Hive，Cascading等项目，在指向主要版本中升级的Apache Hadoop集群时，应该不加修改。</li><li>现有的YARN应用程序，包括现有打包的最终用户应用程序罐和Apache Tez等项目，在指向主要版本中升级的Apache Hadoop集群时，应该不加修改。</li><li>将数据传入/传出HDFS的现有应用程序（包括现有打包的最终用户应用程序罐和Apache Flume等框架）在指向主要版本中的升级后的Apache Hadoop集群时应该不加修改。</li></ul><h4 id="政策-3"><a href="#政策-3" class="headerlink" title="政策"></a>政策</h4><ul><li>现有的MapReduce，YARN和HDFS应用程序和框架应该在主要版本中不加修改，即支持Apache Hadoop ABI。</li><li>很少一部分应用程序可能会受到磁盘布局等变化的影响，开发人员社区将尽力减少这些变化，并且不会将它们放在次要版本中。在更加严重的情况下，我们将考虑强烈恢复这些重大变更，并在必要时使违规版本无效。</li><li>特别是对于MapReduce应用程序，开发人员社区将尽力支持跨主要版本提供二进制兼容性，例如使用org.apache.hadoop.mapred的应用程序。</li><li>在hadoop-1.x和hadoop-2.x之间兼容支持API。有关详细信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50L2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50LWNvcmUvTWFwUmVkdWNlX0NvbXBhdGliaWxpdHlfSGFkb29wMV9IYWRvb3AyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">hadoop-1.x和hadoop-2.x之间的MapReduce应用程序的兼容性<i class="fa fa-external-link"></i></span>。</li></ul><h3 id="REST-API"><a href="#REST-API" class="headerlink" title="REST API"></a>REST API</h3><p>REST API兼容性对应于请求（URL）和对每个请求的响应（内容，可能包含其他URL）。Hadoop REST API专门用于发布版本（甚至是主要版本）的客户端的稳定使用。以下是公开的REST API：</p><ul><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvV2ViSERGUy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS<i class="fa fa-external-link"></i></span> - 稳定</li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvUmVzb3VyY2VNYW5hZ2VyUmVzdC5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">的ResourceManager<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvTm9kZU1hbmFnZXJSZXN0Lmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">节点管理器<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50L2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50LWNvcmUvTWFwcmVkQXBwTWFzdGVyUmVzdC5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50L2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50LWhzL0hpc3RvcnlTZXJ2ZXJSZXN0Lmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">历史服务器<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvVGltZWxpbmVTZXJ2ZXIuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServer.html">时间轴服务器v1 REST API<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvVGltZWxpbmVTZXJ2aWNlVjIuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">时间轴服务v2 REST API<i class="fa fa-external-link"></i></span></li></ul><h4 id="政策-4"><a href="#政策-4" class="headerlink" title="政策"></a>政策</h4><p>上面文本中注释稳定的API保留了至少一个主要版本的兼容性，并且可能在主要版本中由较新版本的REST API弃用。、</p><h3 id="度量-JMX"><a href="#度量-JMX" class="headerlink" title="度量/ JMX"></a>度量/ JMX</h3><p>虽然Metrics API兼容性受Java API兼容性的约束，但Hadoop公开的实际度量标准需要兼容，以便用户能够自动使用它们（脚本等）。添加其他指标是兼容的。修改（例如，更改单位或测量）或删除现有指标会破坏兼容性。同样，对JMX MBean对象名称的更改也会破坏兼容性。</p><h4 id="政策-5"><a href="#政策-5" class="headerlink" title="政策"></a>政策</h4><p>度量标准应保留主要版本中的兼容性。</p><h3 id="文件格式和元数据"><a href="#文件格式和元数据" class="headerlink" title="文件格式和元数据"></a>文件格式和元数据</h3><p>用户和系统级数据（包括元数据）存储在不同格式的文件中。对元数据或用于存储数据/元数据的文件格式的更改可能导致版本之间不兼容。</p><h4 id="用户级文件格式"><a href="#用户级文件格式" class="headerlink" title="用户级文件格式"></a>用户级文件格式</h4><p>对最终用户用于存储其数据的格式的更改可能会阻止他们在以后的版本中访问数据，因此保持这些文件格式兼容非常重要。人们总是可以添加一种改进现有格式的“新”格式。这些格式的示例包括har，war，SequenceFileFormat等。</p><h5 id="政策-6"><a href="#政策-6" class="headerlink" title="政策"></a>政策</h5><ul><li>非正向兼容的用户文件格式更改仅限于主要版本。当用户文件格式发生变化时，预计新版本将读取现有格式，但可能会以与先前版本不兼容的格式写入数据。此外，社区应优先创建程序必须选择的新格式，而不是对现有格式进行不兼容的更改。</li></ul><h4 id="系统内部文件格式"><a href="#系统内部文件格式" class="headerlink" title="系统内部文件格式"></a>系统内部文件格式</h4><p>Hadoop内部数据也存储在文件中，再次更改这些格式可能会导致不兼容。虽然此类更改不像用户级文件格式那样具有破坏性，但是关于何时可以破坏兼容性的策略很重要。</p><h5 id="MapReduce的"><a href="#MapReduce的" class="headerlink" title="MapReduce的"></a>MapReduce的</h5><p>MapReduce使用I-File等格式来存储特定于MapReduce的数据。</p><h5 id="政策-7"><a href="#政策-7" class="headerlink" title="政策"></a>政策</h5><p>MapReduce内部格式（如IFile）在主要版本中保持兼容性。对这些格式的更改可能导致正在进行的作业失败，因此我们应该确保较新的客户端能够以兼容的方式从旧服务器获取随机数据。</p><h5 id="HDFS元数据"><a href="#HDFS元数据" class="headerlink" title="HDFS元数据"></a>HDFS元数据</h5><p>HDFS以特定格式保存元数据（图像和编辑日志）。对格式或元数据的不兼容更改会阻止后续版本读取旧元数据。此类不兼容的更改可能需要HDFS“升级”才能转换元数据以使其可访问。某些更改可能需要多个此类“升级”。</p><p>根据更改中的不兼容程度，可能会出现以下可能的情况：</p><ul><li>自动：图像自动升级，无需显式“升级”。</li><li>直接：图像可升级，但可能需要一个显式版本“升级”。</li><li>间接：图像可升级，但可能需要先升级到中间版本。</li><li>无法升级：图像无法升级。</li></ul><h5 id="政策-8"><a href="#政策-8" class="headerlink" title="政策"></a>政策</h5><ul><li>版本升级必须允许群集回滚到旧版本及其旧磁盘格式。回滚需要还原原始数据，但不需要还原更新的数据。</li><li>HDFS元数据更改必须可通过任何升级路径进行升级 - 自动，直接或间接。</li><li>尚未考虑基于升级类型的更详细的策略。</li></ul><h3 id="命令行界面（CLI）"><a href="#命令行界面（CLI）" class="headerlink" title="命令行界面（CLI）"></a>命令行界面（CLI）</h3><p>Hadoop命令行程序可以直接通过系统shell或shell脚本使用。更改命令的路径，删除或重命名命令行选项，参数的顺序，或命令返回代码和输出中断兼容性，并可能对用户产生负面影响。</p><h4 id="政策-9"><a href="#政策-9" class="headerlink" title="政策"></a>政策</h4><p>在将一个主要版本删除或在后续主要版本中进行不兼容修改之前，将弃用CLI命令（使用时发出警告）。</p><h3 id="Web-UI"><a href="#Web-UI" class="headerlink" title="Web UI"></a>Web UI</h3><p>Web UI，尤其是网页的内容和布局，更改可能会干扰屏幕抓取网页信息的尝试。</p><h4 id="政策-10"><a href="#政策-10" class="headerlink" title="政策"></a>政策</h4><p>网页并不意味着被删除，因此允许随时对它们进行不兼容的更改。期望用户使用REST API来获取任何信息。</p><h3 id="Hadoop配置文件"><a href="#Hadoop配置文件" class="headerlink" title="Hadoop配置文件"></a>Hadoop配置文件</h3><p>用户使用（1）Hadoop定义的属性来配置和提供Hadoop的提示，以及（2）自定义属性以将信息传递给作业。因此，配置属性的兼容性是双重的：</p><ul><li>修改Hadoop定义的属性的键名，值单位和默认值。</li><li>自定义配置属性键不应与Hadoop定义的属性的命名空间冲突。通常，用户应避免使用Hadoop使用的前缀：hadoop，io，ipc，fs，net，file，ftp，s3，kfs，ha，file，dfs，mapred，mapreduce，yarn。</li></ul><h4 id="政策-11"><a href="#政策-11" class="headerlink" title="政策"></a>政策</h4><ul><li>Hadoop定义的属性至少在一个主要版本中被弃用，然后才会被删除。不允许修改现有属性的单位。</li><li>Hadoop定义的属性的默认值可以在次要/主要版本中更改，但在次要版本中的点版本中保持不变。</li><li>目前，没有明确的政策可以添加/删除新的前缀，以及自定义配置属性要避免的前缀列表。但是，如上所述，用户应避免使用Hadoop使用的前缀：hadoop，io，ipc，fs，net，file，ftp，s3，kfs，ha，file，dfs，mapred，mapreduce，yarn。</li></ul><h3 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h3><p>源代码，工件（源和测试），用户日志，配置文件，输出和作业历史记录都存储在本地文件系统或HDFS的磁盘上。更改这些用户可访问文件的目录结构会破坏兼容性，即使在通过符号链接保留原始路径的情况下（例如，如果路径由配置为不遵循符号链接的servlet访问）。</p><h4 id="政策-12"><a href="#政策-12" class="headerlink" title="政策"></a>政策</h4><ul><li>源代码和构建工件的布局可以随时更改，尤其是在主要版本中。在主要版本中，开发人员将尝试（不保证）保留目录结构; 但是，可以添加/移动/删除单个文件。确保补丁与代码保持同步的最佳方法是将它们提交到Apache源代码树。</li><li>配置文件，用户日志和作业历史记录的目录结构将在主要版本中的次要版本和点版本中保留。</li></ul><h3 id="Java-Classpath"><a href="#Java-Classpath" class="headerlink" title="Java Classpath"></a>Java Classpath</h3><p>针对Hadoop构建的用户应用程序可能会将所有Hadoop jar（包括Hadoop的库依赖项）添加到应用程序的类路径中。添加新依赖项或更新现有依赖项的版本可能会干扰应用程序类路径中的依赖项。</p><h4 id="政策-13"><a href="#政策-13" class="headerlink" title="政策"></a>政策</h4><p>目前，没有关于Hadoop的依赖关系何时可以改变的政策。</p><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>用户和相关项目通常使用导出的环境变量（例如HADOOP_CONF_DIR），因此删除或重命名环境变量是一种不兼容的更改。</p><h4 id="政策-14"><a href="#政策-14" class="headerlink" title="政策"></a>政策</h4><p>目前，没有关于环境变量何时可以改变的政策。开发人员尝试限制对主要版本的更改。</p><h3 id="构建工件"><a href="#构建工件" class="headerlink" title="构建工件"></a>构建工件</h3><p>Hadoop使用maven进行项目管理，更改工件可能会影响现有的用户工作流程。</p><h4 id="政策-15"><a href="#政策-15" class="headerlink" title="政策"></a>政策</h4><ul><li>测试工件：生成的测试jar严格用于内部使用，预计不会在Hadoop之外使用，类似于注释@Private，@ Unstable的API。</li><li>构建工件：hadoop-client工件（maven groupId：artifactId）在主要版本中保持兼容，而其他工件可以以不兼容的方式更改。</li></ul><h3 id="硬件-软件要求"><a href="#硬件-软件要求" class="headerlink" title="硬件/软件要求"></a>硬件/软件要求</h3><p>为了跟上硬件，操作系统，JVM和其他软件的最新进展，新的Hadoop版本或其某些功能可能需要更高版本的版本。对于特定环境，升级Hadoop可能需要升级其他相关软件组件。</p><h4 id="政策-16"><a href="#政策-16" class="headerlink" title="政策"></a>政策</h4><ul><li>硬件<ul><li>架构：社区没有计划将Hadoop限制为特定架构，但可以进行特定于系列的优化。</li><li>最小资源：虽然无法保证Hadoop守护程序所需的最低资源，但社区尝试不在次要版本中增加要求。</li></ul></li><li>操作系统：社区将尝试在次要版本中维护相同的操作系统要求（操作系统内核版本）。目前，GNU / Linux和Microsoft Windows是社区正式支持的操作系统，而Apache Hadoop在其他操作系统（如Apple MacOSX，Solaris等）上运行良好。</li><li>除非所讨论的JVM版本不受支持，否则JVM要求不会在同一次要版本中的点版本之间发生更改。次要/主要版本可能需要更高版本的JVM用于部分/全部受支持的操作系统。</li><li>其他软件：社区尝试维护Hadoop所需的其他软件的最低版本。例如，ssh，kerberos等。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以下是与该主题相关的一些相关JIRA和页面：</p><ul><li>该文件的演变 - <span class="exturl" data-url="aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQURPT1AtOTUxNw==" title="https://issues.apache.org/jira/browse/HADOOP-9517">HADOOP-9517<i class="fa fa-external-link"></i></span></li><li>hadoop-1.x和hadoop-2.x之间MapReduce最终用户应用程序的二进制兼容性 - <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50L2hhZG9vcC1tYXByZWR1Y2UtY2xpZW50LWNvcmUvTWFwUmVkdWNlX0NvbXBhdGliaWxpdHlfSGFkb29wMV9IYWRvb3AyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">MapReduce hadoop-1.x和hadoop-2.x之间的兼容性<i class="fa fa-external-link"></i></span></li><li>根据接口分类计划的接口注释 - <span class="exturl" data-url="aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQURPT1AtNzM5MQ==" title="https://issues.apache.org/jira/browse/HADOOP-7391">HADOOP-7391 <i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9JbnRlcmZhY2VDbGFzc2lmaWNhdGlvbi5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/InterfaceClassification.html">Hadoop接口分类<i class="fa fa-external-link"></i></span></li><li>兼容Hadoop 1.x版本 - <span class="exturl" data-url="aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQURPT1AtNTA3MQ==" title="https://issues.apache.org/jira/browse/HADOOP-5071">HADOOP-5071<i class="fa fa-external-link"></i></span></li><li>在<span class="exturl" data-url="aHR0cDovL3dpa2kuYXBhY2hlLm9yZy9oYWRvb3AvUm9hZG1hcA==" title="http://wiki.apache.org/hadoop/Roadmap">Hadoop的路线图<i class="fa fa-external-link"></i></span>，捕捉其他发行政策页</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h2&gt;&lt;p&gt;本文档介绍了Apache Hadoop项目的兼容性目标。枚举了影响Hadoop开发人员，下游项目和最终用户的Hadoop版本之间的不同类型的
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop: FileSystem Shell</title>
    <link href="https://tangguangen.com/Hadoop-FileSystem-Shell/"/>
    <id>https://tangguangen.com/Hadoop-FileSystem-Shell/</id>
    <published>2018-11-25T12:39:27.000Z</published>
    <updated>2018-11-29T00:35:29.698Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>文件系统（FS）shell包括各种类似shell的命令，这些命令直接与Hadoop分布式文件系统（HDFS）以及Hadoop支持的其他文件系统交互，例如本地FS，HFTP FS，S3 FS等。FS shell由以下方式调用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hadoop fs &lt;args&gt;</span><br></pre></td></tr></table></figure><p>所有FS shell命令都将路径URI作为参数。URI格式为<code>scheme：// authority / path</code>。对于HDFS，方案是<code>hdfs</code>，对于本地FS，方案是<code>文件</code>。该计划和权限是可选的。如果未指定，则使用配置中指定的默认方案。可以将HDFS文件或目录（例如/ parent / child）指定为<code>hdfs：// namenodehost / parent / child</code>或简单地指定为<code>/ parent / child</code>（假设您的配置设置为指向<code>hdfs：// namenodehost</code>）。</p><p>FS shell中的大多数命令都表现得像对应的Unix命令。使用每个命令描述差异。错误信息发送到stderr，输出发送到stdout。</p><p>如果正在使用HDFS，则<code>hdfs dfs</code>是同义词。</p><p>可以使用相对路径。对于HDFS，当前工作目录是HDFS主目录<code>/ user / &lt;username&gt;</code>，通常必须手动创建。也可以隐式访问HDFS主目录，例如，当使用HDFS垃圾文件夹时，主目录中的<code>.Trash</code>目录。</p><p>有关通用shell选项，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9Db21tYW5kc01hbnVhbC5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CommandsManual.html">命令手册<i class="fa fa-external-link"></i></span>。</p><h2 id="appendToFile"><a href="#appendToFile" class="headerlink" title="appendToFile"></a>appendToFile</h2><p>用法：<code>hadoop fs -appendToFile &lt;localsrc&gt; ... &lt;dst&gt;</code></p><p>将单个src或多个srcs从本地文件系统附加到目标文件系统。还从stdin读取输入并附加到目标文件系统。</p><ul><li><code>hadoop fs -appendToFile localfile /user/hadoop/hadoopfile</code></li><li><code>hadoop fs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile</code></li><li><code>hadoop fs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile</code></li><li><code>hadoop fs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile</code> Reads the input from stdin.</li></ul><p>退出代码：</p><p>成功时返回0，错误时返回1。</p><h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h2><p>用法：<code>hadoop fs -cat [-ignoreCrc] URI [URI ...]</code></p><p>将源路径复制到stdout。</p><p>选项</p><ul><li>该<code>-ignoreCrc</code>选项禁用checkshum验证。</li></ul><p>例：</p><ul><li><code>hadoop fs -cat hdfs：//nn1.example.com/file1 hdfs：//nn2.example.com/file2</code></li><li><code>hadoop fs -cat file：/// file3 / user / hadoop / file4</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="checksum"><a href="#checksum" class="headerlink" title="checksum"></a>checksum</h2><p>用法：<code>hadoop fs -checksum URI</code></p><p>返回文件的校验和信息。</p><p>例：</p><ul><li><code>hadoop fs -checksum hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -checksum file：/// etc / hosts</code></li></ul><h2 id="chgrp"><a href="#chgrp" class="headerlink" title="chgrp"></a>chgrp</h2><p>用法：<code>hadoop fs -chgrp [-R] GROUP URI [URI ...]</code></p><p>更改文件的组关联。用户必须是文件的所有者，否则必须是超级用户。其他信息在“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1Blcm1pc3Npb25zR3VpZGUuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">权限指南”中<i class="fa fa-external-link"></i></span>。</p><p>选项</p><ul><li>-R选项将通过目录结构递归地进行更改。</li></ul><h2 id="CHMOD"><a href="#CHMOD" class="headerlink" title="CHMOD"></a>CHMOD</h2><p>用法：<code>hadoop fs -chmod [-R] &lt;MODE [，MODE] ... | OCTALMODE&gt; URI [URI ...]</code></p><p>更改文件的权限。使用-R，通过目录结构递归更改。用户必须是文件的所有者，否则必须是超级用户。其他信息在“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1Blcm1pc3Npb25zR3VpZGUuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">权限指南”中<i class="fa fa-external-link"></i></span>。</p><p>选项</p><ul><li>-R选项将通过目录结构递归地进行更改。</li></ul><h2 id="CHOWN"><a href="#CHOWN" class="headerlink" title="CHOWN"></a>CHOWN</h2><p>用法：<code>hadoop fs -chown [-R] [OWNER] [：[GROUP]] URI [URI]</code></p><p>更改文件的所有者。用户必须是超级用户。其他信息在“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1Blcm1pc3Npb25zR3VpZGUuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">权限指南”中<i class="fa fa-external-link"></i></span>。</p><p>选项</p><ul><li>-R选项将通过目录结构递归地进行更改。</li></ul><h2 id="copyFromLocal"><a href="#copyFromLocal" class="headerlink" title="copyFromLocal"></a>copyFromLocal</h2><p>用法：<code>hadoop fs -copyFromLocal &lt;localsrc&gt; URI</code></p><p>与<code>fs -put</code>命令类似，但源仅限于本地文件引用。</p><p>选项：</p><ul><li><code>-p</code>：保留访问和修改时间，所有权和权限。（假设权限可以跨文件系统传播）</li><li><code>-f</code>：覆盖目标（如果已存在）。</li><li><code>-l</code>：允许DataNode懒惰地将文件持久保存到磁盘，强制复制因子为1.此标志将导致持久性降低。小心使用。</li><li><code>-d</code>：使用后缀<code>._COPYING_</code>跳过创建临时文件。</li></ul><h2 id="copyToLocal"><a href="#copyToLocal" class="headerlink" title="copyToLocal"></a>copyToLocal</h2><p>用法：<code>hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;</code></p><p>与get命令类似，但目标仅限于本地文件引用。</p><h2 id="count"><a href="#count" class="headerlink" title="count"></a>count</h2><p>用法：<code>hadoop fs -count [-q] [-h] [-v] [-x] [-t [&lt;存储类型&gt;]] [-u] &lt;路径&gt;</code></p><p>计算与指定文件模式匹配的路径下的目录，文件和字节数。获取配额和使用情况。-count的输出列为：DIR_COUNT，FILE_COUNT，CONTENT_SIZE，PATHNAME</p><p>-u和-q选项控制输出包含的列。-q表示显示配额，-u限制输出以仅显示配额和使用情况。</p><p>-count -q的输出列为：QUOTA，REMAINING_QUOTA，SPACE_QUOTA，REMAINING_SPACE_QUOTA，DIR_COUNT，FILE_COUNT，CONTENT_SIZE，PATHNAME</p><p>-count -u的输出列为：QUOTA，REMAINING_QUOTA，SPACE_QUOTA，REMAINING_SPACE_QUOTA，PATHNAME</p><p>-t选项显示每种存储类型的配额和使用情况。如果未给出-u或-q选项，则忽略-t选项。可以在-t选项中使用的可能参数列表（除参数“”之外不区分大小写）：“”，“all”，“ram_disk”，“ssd”，“disk”或“archive”。</p><p>-h选项以人类可读格式显示大小。</p><p>-v选项显示标题行。</p><p>-x选项从结果计算中排除快照。如果没有-x选项（默认），则始终从所有INode计算结果，包括给定路径下的所有快照。如果给出-u或-q选项，则忽略-x选项。</p><p>例：</p><ul><li><code>hadoop fs -count hdfs：//nn1.example.com/file1 hdfs：//nn2.example.com/file2</code></li><li><code>hadoop fs -count -q hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -q -h hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -q -h -v hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -u hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -u -h hdfs：//nn1.example.com/file1</code></li><li><code>hadoop fs -count -u -h -v hdfs：//nn1.example.com/file1</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1</p><h2 id="CP"><a href="#CP" class="headerlink" title="CP"></a>CP</h2><p>用法：<code>hadoop fs -cp [-f] [-p | -p [topax]] URI [URI ...] &lt;dest&gt;</code></p><p>将文件从源复制到目标。此命令也允许多个源，在这种情况下，目标必须是目录。</p><p>如果（1）源文件系统和目标文件系统支持它们（仅限HDFS），并且（2）所有源和目标路径名都在/.reserved/raw层次结构中，则保留’raw。<em>‘命名空间扩展属性。是否保留raw。</em> namespace xattrs的确定与-p（保留）标志无关。</p><p>选项：</p><ul><li>如果目标已存在，则-f选项将覆盖目标。</li><li>-p选项将保留文件属性[topx]（时间戳，所有权，权限，ACL，XAttr）。如果指定了-p而没有<em>arg</em>，则保留时间戳，所有权和权限。如果指定了-pa，则还保留权限，因为ACL是一组超级权限。确定是否保留原始命名空间扩展属性与-p标志无关。</li></ul><p>例：</p><ul><li><code>hadoop fs -cp / user / hadoop / file1 / user / hadoop / file2</code></li><li><code>hadoop fs -cp / user / hadoop / file1 / user / hadoop / file2 / user / hadoop / dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="createSnapshot"><a href="#createSnapshot" class="headerlink" title="createSnapshot"></a>createSnapshot</h2><p>请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1NuYXBzaG90cy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">HDFS快照指南”<i class="fa fa-external-link"></i></span>。</p><h2 id="deleteSnapshot"><a href="#deleteSnapshot" class="headerlink" title="deleteSnapshot"></a>deleteSnapshot</h2><p>请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1NuYXBzaG90cy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">HDFS快照指南”<i class="fa fa-external-link"></i></span>。</p><h2 id="df"><a href="#df" class="headerlink" title="df"></a>df</h2><p>用法：<code>hadoop fs -df [-h] URI [URI ...]</code></p><p>显示可用空间。</p><p>选项：</p><ul><li>-h选项将以“人类可读”的方式格式化文件大小（例如64.0m而不是67108864）</li></ul><p>例：</p><ul><li><code>hadoop dfs -df / user / hadoop / dir1</code></li></ul><h2 id="du"><a href="#du" class="headerlink" title="du"></a>du</h2><p>用法：<code>hadoop fs -du [-s] [-h] [-x] URI [URI ...]</code></p><p>显示给定目录中包含的文件和目录的大小或文件的长度，以防它只是一个文件。</p><p>选项：</p><ul><li>-s选项将导致显示文件长度的汇总摘要，而不是单个文件。如果没有-s选项，则通过从给定路径向上移动1级来完成计算。</li><li>-h选项将以“人类可读”的方式格式化文件大小（例如64.0m而不是67108864）</li><li>-x选项将从结果计算中排除快照。如果没有-x选项（默认），则始终从所有INode计算结果，包括给定路径下的所有快照。</li></ul><p>du返回三列，格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">size disk_space_consumed_with_all_replicas full_path_name</span><br></pre></td></tr></table></figure><p>例：</p><ul><li><code>hadoop fs -du / user / hadoop / dir1 / user / hadoop / file1 hdfs：//nn.example.com/user/hadoop/dir1</code></li></ul><p>退出代码：成功时返回0，错误时返回-1。</p><h2 id="dus"><a href="#dus" class="headerlink" title="dus"></a>dus</h2><p>用法：<code>hadoop fs -dus &lt;args&gt;</code></p><p>显示文件长度的摘要。</p><p><strong>注意：</strong>不推荐使用此命令。而是使用<code>hadoop fs -du -s</code>。</p><h2 id="expunge"><a href="#expunge" class="headerlink" title="expunge"></a>expunge</h2><p>用法：<code>hadoop fs -expunge</code></p><p>从trash目录中永久删除早于保留阈值的检查点中的文件，并创建新的检查点。</p><p>创建检查点时，垃圾箱中最近删除的文件将移动到检查点下。早于<code>fs.trash.interval的</code>检查点中的文件将在下次调用<code>-expunge</code>命令时被永久删除。</p><p>如果文件系统支持该功能，则用户可以配置为通过存储为<code>fs.trash.checkpoint.interval</code>的参数（在core-site.xml中）定期创建和删除检查点。该值应小于或等于<code>fs.trash.interval</code>。</p><p>有关<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc0Rlc2lnbi5odG1sI0ZpbGVfRGVsZXRlc19hbmRfVW5kZWxldGVz" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes">HDFS<i class="fa fa-external-link"></i></span>垃圾功能的更多信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc0Rlc2lnbi5odG1sI0ZpbGVfRGVsZXRlc19hbmRfVW5kZWxldGVz" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes">HDFS体系结构指南<i class="fa fa-external-link"></i></span>。</p><h2 id="find"><a href="#find" class="headerlink" title="find"></a>find</h2><p>用法：<code>hadoop fs -find &lt;path&gt; ... &lt;expression&gt; ...</code></p><p>查找与指定表达式匹配的所有文件，并将选定的操作应用于它们。如果未指定<em>路径</em>，则默认为当前工作目录。如果未指定表达式，则默认为-print。</p><p>识别以下主要表达式：</p><ul><li><p>-name pattern<br>-iname pattern</p><p>如果文件的基名与使用标准文件系统通配符的模式匹配，则求值为true。如果使用-iname，则匹配不区分大小写。</p></li><li><p>-print<br>-print0</p><p>始终评估为true。导致将当前路径名写入标准输出。如果使用-print0表达式，则附加ASCII NULL字符。</p></li></ul><p>识别以下运算符：</p><ul><li><p>表达式-a表达式<br>表达式和表达式<br>表达式表达式</p><p>用于连接两个表达式的逻辑AND运算符。如果两个子表达式都返回true，则返回true。由两个表达式的并置所暗示，因此不需要明确指定。如果第一个表达式失败，则不会应用第二个表达式。</p></li></ul><p>例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -find / -name <span class="built_in">test</span> -<span class="built_in">print</span></span><br></pre></td></tr></table></figure><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="get"><a href="#get" class="headerlink" title="get"></a>get</h2><p>用法：<code>hadoop fs -get [-ignorecrc] [-crc] [-p] [-f] &lt;src&gt; &lt;localdst&gt;</code></p><p>将文件复制到本地文件系统。可以使用-ignorecrc选项复制CRC校验失败的文件。可以使用-crc选项复制文件和CRC。</p><p>例：</p><ul><li><code>hadoop fs -get / user / hadoop / file localfile</code></li><li><code>hadoop fs -get hdfs：//nn.example.com/user/hadoop/file localfile</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><p>选项：</p><ul><li><code>-p</code>：保留访问和修改时间，所有权和权限。（假设权限可以跨文件系统传播）</li><li><code>-f</code>：覆盖目标（如果已存在）。</li><li><code>-ignorecrc</code>：对下载的文件进行跳过CRC校验。</li><li><code>-crc</code>：为下载的文件写入CRC校验和。</li></ul><h2 id="getfacl"><a href="#getfacl" class="headerlink" title="getfacl"></a>getfacl</h2><p>用法：<code>hadoop fs -getfacl [-R] &lt;path&gt;</code></p><p>显示文件和目录的访问控制列表（ACL）。如果目录具有默认ACL，则getfacl还会显示默认ACL。</p><p>选项：</p><ul><li>-R：递归列出所有文件和目录的ACL。</li><li><em>path</em>：要列出的文件或目录。</li></ul><p>例子：</p><ul><li><code>hadoop fs -getfacl / file</code></li><li><code>hadoop fs -getfacl -R / dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="getfattr"><a href="#getfattr" class="headerlink" title="getfattr"></a>getfattr</h2><p>用法：<code>hadoop fs -getfattr [-R] -n name | -d [-e en] &lt;path&gt;</code></p><p>显示文件或目录的扩展属性名称和值（如果有）。</p><p>选项：</p><ul><li>-R：递归列出所有文件和目录的属性。</li><li>-n name：转储指定的扩展属性值。</li><li>-d：转储与pathname关联的所有扩展属性值。</li><li>-e <em>encoding</em>：检索后<em>对代码</em>值进行编码。有效编码为“text”，“hex”和“base64”。编码为文本字符串的值用双引号（“）括起来，编码为十六进制和base64的值分别以0x和0为前缀。</li><li><em>path</em>：文件或目录。</li></ul><p>例子：</p><ul><li><code>hadoop fs -getfattr -d / file</code></li><li><code>hadoop fs -getfattr -R -n user.myAttr / dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="getmerge"><a href="#getmerge" class="headerlink" title="getmerge"></a>getmerge</h2><p>用法：<code>hadoop fs -getmerge [-nl] &lt;src&gt; &lt;localdst&gt;</code></p><p>将源目录和目标文件作为输入，并将src中的文件连接到目标本地文件。可选地，-nl可以设置为在每个文件的末尾添加换行符（LF）。-skip-empty-file可用于在空文件的情况下避免不需要的换行符。</p><p>例子：</p><ul><li><code>hadoop fs -getmerge -nl / src /opt/output.txt</code></li><li><code>hadoop fs -getmerge -nl /src/file1.txt /src/file2.txt /output.txt</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="help"><a href="#help" class="headerlink" title="help"></a>help</h2><p>用法：<code>hadoop fs -help</code></p><p>返回使用输出。</p><h2 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h2><p>用法：<code>hadoop fs -ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] &lt;args&gt;</code></p><p>选项：</p><ul><li>-C：仅显示文件和目录的路径。</li><li>-d：目录列为纯文件。</li><li>-h：以人类可读的方式格式化文件大小（例如64.0m而不是67108864）。</li><li>-q：打印？而不是不可打印的字符。</li><li>-R：递归列出遇到的子目录。</li><li>-t：按修改时间排序输出（最近的第一个）。</li><li>-S：按文件大小排序输出。</li><li>-r：反转排序顺序。</li><li>-u：使用访问时间而不是修改时间进行显示和排序。</li></ul><p>对于文件，ls使用以下格式返回文件的stat：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permissions number_of_replicas userid groupid filesize modification_date modification_time filename</span><br></pre></td></tr></table></figure><p>对于目录，它返回其直接子节点的列表，如在Unix中。目录列为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permissions userid groupid modification_date modification_time dirname</span><br></pre></td></tr></table></figure><p>默认情况下，目录中的文件按文件名排序。</p><p>例：</p><ul><li><code>hadoop fs -ls / user / hadoop / file1</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="lsr"><a href="#lsr" class="headerlink" title="lsr"></a>lsr</h2><p>用法：<code>hadoop fs -lsr &lt;args&gt;</code></p><p>ls的递归版本。</p><p><strong>注意：</strong>不推荐使用此命令。而是使用<code>hadoop fs -ls -R</code></p><h2 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h2><p>用法：<code>hadoop fs -mkdir [-p] &lt;paths&gt;</code></p><p>将路径uri作为参数并创建目录。</p><p>选项：</p><ul><li>-p选项行为与Unix mkdir -p非常相似，沿路径创建父目录。</li></ul><p>例：</p><ul><li><code>hadoop fs -mkdir / user / hadoop / dir1 / user / hadoop / dir2</code></li><li><code>hadoop fs -mkdir hdfs：//nn1.example.com/user/hadoop/dir hdfs：//nn2.example.com/user/hadoop/dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1</p><h2 id="moveFromLocal"><a href="#moveFromLocal" class="headerlink" title="moveFromLocal"></a>moveFromLocal</h2><p>用法：<code>hadoop fs -moveFromLocal &lt;localsrc&gt; &lt;dst&gt;</code></p><p>与put命令类似，只是在复制后删除了源localsrc。</p><h2 id="moveToLocal"><a href="#moveToLocal" class="headerlink" title="moveToLocal"></a>moveToLocal</h2><p>用法：<code>hadoop fs -moveToLocal [-crc] &lt;src&gt; &lt;dst&gt;</code></p><p>显示“尚未实现”消息。</p><h2 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h2><p>用法：<code>hadoop fs -mv URI [URI ...] &lt;dest&gt;</code></p><p>将文件从源移动到目标。此命令也允许多个源，在这种情况下，目标需要是目录。不允许跨文件系统移动文件。</p><p>例：</p><ul><li><code>hadoop fs -mv / user / hadoop / file1 / user / hadoop / file2</code></li><li><code>hadoop fs -mv hdfs：//nn.example.com/file1 hdfs：//nn.example.com/file2 hdfs：//nn.example.com/file3 hdfs：//nn.example.com/dir1</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="put"><a href="#put" class="headerlink" title="put"></a>put</h2><p>用法：<code>hadoop fs -put [-f] [-p] [-l] [-d] [ - | &lt;localsrc1&gt; ..]。&lt;DST&gt;</code></p><p>将单个src或多个srcs从本地文件系统复制到目标文件系统。如果源设置为“ - ”，还从stdin读取输入并写入目标文件系统</p><p>如果文件已存在，则复制失败，除非给出-f标志。</p><p>选项：</p><ul><li><code>-p</code>：保留访问和修改时间，所有权和权限。（假设权限可以跨文件系统传播）</li><li><code>-f</code>：覆盖目标（如果已存在）。</li><li><code>-l</code>：允许DataNode懒惰地将文件持久保存到磁盘，强制复制因子为1.此标志将导致持久性降低。小心使用。</li><li><code>-d</code>：使用后缀<code>._COPYING_</code>跳过创建临时文件。</li></ul><p>例子：</p><ul><li><code>hadoop fs -put localfile / user / hadoop / hadoopfile</code></li><li><code>hadoop fs -put -f localfile1 localfile2 / user / hadoop / hadoopdir</code></li><li><code>hadoop fs -put -d localfile hdfs：//nn.example.com/hadoop/hadoopfile</code></li><li><code>hadoop fs -put - hdfs：//nn.example.com/hadoop/hadoopfile</code>从stdin读取输入。</li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="renameSnapshot"><a href="#renameSnapshot" class="headerlink" title="renameSnapshot"></a>renameSnapshot</h2><p>请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1NuYXBzaG90cy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">HDFS快照指南”<i class="fa fa-external-link"></i></span>。</p><h2 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h2><p>用法：<code>hadoop fs -rm [-f] [-r | -R] [-skipTrash] [-safely] URI [URI ...]</code></p><p>删除指定为args的文件。</p><p>如果启用了垃圾箱，则文件系统会将已删除的文件移动到垃圾箱目录（由<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2FwaS9vcmcvYXBhY2hlL2hhZG9vcC9mcy9GaWxlU3lzdGVtLmh0bWw=" title="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html">FileSystem＃getTrashRoot提供<i class="fa fa-external-link"></i></span>）。</p><p>目前，默认情况下禁用垃圾箱功能。用户可以通过为参数<code>fs.trash.interval</code>（在core-site.xml中）设置大于零的值来启用垃圾。</p><p>请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9GaWxlU3lzdGVtU2hlbGwuaHRtbCNleHB1bmdl" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge">删除<i class="fa fa-external-link"></i></span>有关删除垃圾箱中文件的信息。</p><p>选项：</p><ul><li>如果文件不存在，-f选项将不显示诊断消息或修改退出状态以反映错误。</li><li>-R选项以递归方式删除目录及其下的任何内容。</li><li>-r选项等效于-R。</li><li>-skipTrash选项将绕过垃圾桶（如果已启用），并立即删除指定的文件。当需要从超配额目录中删除文件时，这非常有用。</li><li>-safely选项在删除目录之前需要安全确认，文件总数大于<code>hadoop.shell.delete.limit.num.files</code>（在core-site.xml中，默认值：100）。它可以与-skipTrash一起使用，以防止意外删除大目录。当递归地遍历大目录以计算在确认之前要删除的文件的数量时，预期延迟。</li></ul><p>例：</p><ul><li><code>hadoop fs -rm hdfs：//nn.example.com/file / user / hadoop / emptydir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir"></a>rmdir</h2><p>用法：<code>hadoop fs -rmdir [--ignore-fail-on-non-empty] URI [URI ...]</code></p><p>删除目录。</p><p>选项：</p><ul><li><code>--ignore-fail-on-non-empty</code>：使用通配符时，如果目录仍包含文件，请不要失败。</li></ul><p>例：</p><ul><li><code>hadoop fs -rmdir / user / hadoop / emptydir</code></li></ul><h2 id="rmr"><a href="#rmr" class="headerlink" title="rmr"></a>rmr</h2><p>用法：<code>hadoop fs -rmr [-skipTrash] URI [URI ...]</code></p><p>删除的递归版本。</p><p><strong>注意：</strong>不推荐使用此命令。而是使用<code>hadoop fs -rm -r</code></p><h2 id="setfacl"><a href="#setfacl" class="headerlink" title="setfacl"></a>setfacl</h2><p>用法：<code>hadoop fs -setfacl [-R] [-b | -k -m | -x &lt;acl_spec&gt; &lt;path&gt;] | [ - set &lt;acl_spec&gt; &lt;path&gt;]</code></p><p>设置文件和目录的访问控制列表（ACL）。</p><p>选项：</p><ul><li>-b：删除除基本ACL条目之外的所有条目。保留用户，组和其他条目以与权限位兼容。</li><li>-k：删除默认ACL。</li><li>-R：递归地对所有文件和目录应用操作。</li><li>-m：修改ACL。新条目将添加到ACL，并保留现有条目。</li><li>-x：删除指定的ACL条目。保留其他ACL条目。</li><li><code>--set</code>：完全替换ACL，丢弃所有现有条目。所述<em>acl_spec</em>必须包括用户，组条目和其他用于与权限位兼容性。</li><li><em>acl_spec</em>：以逗号分隔的ACL条目列表。</li><li><em>path</em>：要修改的文件或目录。</li></ul><p>例子：</p><ul><li><code>hadoop fs -setfacl -m user：hadoop：rw- / file</code></li><li><code>hadoop fs -setfacl -x user：hadoop / file</code></li><li><code>hadoop fs -setfacl -b / file</code></li><li><code>hadoop fs -setfacl -k / dir</code></li><li><code>hadoop fs -setfacl --set user :: rw-，user：hadoop：rw-，group :: r - ，other :: r-- / file</code></li><li><code>hadoop fs -setfacl -R -m user：hadoop：rx / dir</code></li><li><code>hadoop fs -setfacl -m default：user：hadoop：rx / dir</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="setfattr"><a href="#setfattr" class="headerlink" title="setfattr"></a>setfattr</h2><p>用法：<code>hadoop fs -setfattr -n name [-v value] | -x name &lt;path&gt;</code></p><p>设置文件或目录的扩展属性名称和值。</p><p>选项：</p><ul><li>-n name：扩展属性名称。</li><li>-v value：扩展属性值。该值有三种不同的编码方法。如果参数用双引号括起来，那么值就是引号内的字符串。如果参数的前缀为0x或0X，则将其视为十六进制数。如果参数以0或0S开头，则将其视为base64编码。</li><li>-x name：删除扩展属性。</li><li><em>path</em>：文件或目录。</li></ul><p>例子：</p><ul><li><code>hadoop fs -setfattr -n user.myAttr -v myValue / file</code></li><li><code>hadoop fs -setfattr -n user.noValue / file</code></li><li><code>hadoop fs -setfattr -x user.myAttr / file</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回非零。</p><h2 id="setrep"><a href="#setrep" class="headerlink" title="setrep"></a>setrep</h2><p>用法：<code>hadoop fs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt;</code></p><p>更改文件的复制因子。如果<em>path</em>是目录，则命令以递归方式更改以<em>path为</em>根的目录树下的所有文件的复制因子。</p><p>选项：</p><ul><li>-w标志请求命令等待复制完成。这可能需要很长时间。</li><li>接受-R标志是为了向后兼容。它没有效果。</li></ul><p>例：</p><ul><li><code>hadoop fs -setrep -w 3 / user / hadoop / dir1</code></li></ul><p>退出代码：</p><p>成功时返回0，错误时返回-1。</p><h2 id="stat"><a href="#stat" class="headerlink" title="stat"></a>stat</h2><p>用法：<code>hadoop fs -stat [格式] &lt;路径&gt; ...</code></p><p>以指定格式打印有关<path></path>的文件/目录的统计信息。格式接受八进制（％a）和符号（％A），文件大小（字节）（％b），类型（％F），所有者组名（％g），名称（％n），块大小（％o）的权限），复制（％r），所有者的用户名（％u），访问日期（％x，％X）和修改日期（％y，％Y）。％x和％y将UTC日期显示为“yyyy-MM-dd HH：mm：ss”，％X和％Y显示自1970年1月1日UTC以来的毫秒数。如果未指定格式，则默认使用％y。</p><p>例：</p><ul><li><code>hadoop fs -stat“type：％F perm：％a％u：％g size：％b mtime：％y atime：％x name：％n”/ file</code></li></ul><p>退出代码：成功时返回0，错误时返回-1。</p><h2 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h2><p>用法：<code>hadoop fs -tail [-f] URI</code></p><p>显示文件的最后一千字节到stdout。</p><p>选项：</p><ul><li>-f选项将在文件增长时输出附加数据，如在Unix中一样。</li></ul><p>例：</p><ul><li><code>hadoop fs -tail路径名</code></li></ul><p>退出代码：成功时返回0，错误时返回-1。</p><h2 id="test"><a href="#test" class="headerlink" title="test"></a>test</h2><p>用法：<code>hadoop fs -test - [defsz] URI</code></p><p>选项：</p><ul><li>-d：f路径是目录，返回0。</li><li>-e：如果路径存在，则返回0。</li><li>-f：如果路径是文件，则返回0。</li><li>-s：如果路径不为空，则返回0。</li><li>-r：如果路径存在且授予读权限，则返回0。</li><li>-w：如果路径存在且授予写入权限，则返回0。</li><li>-z：如果文件长度为零，则返回0。</li></ul><p>例：</p><ul><li><code>hadoop fs -test -e filename</code></li></ul><h2 id="text"><a href="#text" class="headerlink" title="text"></a>text</h2><p>用法：<code>hadoop fs -text &lt;src&gt;</code></p><p>获取源文件并以文本格式输出文件。允许的格式是zip和TextRecordInputStream。</p><h2 id="touchz"><a href="#touchz" class="headerlink" title="touchz"></a>touchz</h2><p>用法：<code>hadoop fs -touchz URI [URI ...]</code></p><p>创建一个零长度的文件。如果文件存在非零长度，则返回错误。</p><p>例：</p><ul><li><code>hadoop fs -touchz pathname</code></li></ul><p>退出代码：成功时返回0，错误时返回-1。</p><h2 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h2><p>用法：<code>hadoop fs -truncate [-w] &lt;length&gt; &lt;paths&gt;</code></p><p>将与指定文件模式匹配的所有文件截断为指定的长度。</p><p>选项：</p><ul><li>该<code>-w</code>标志的要求，对块恢复命令如有必要，等待完成。如果没有-w标志，则在恢复过程中文件可能会保持一段时间不闭合。在此期间，无法重新打开文件以进行追加。</li></ul><p>例：</p><ul><li><code>hadoop fs -truncate 55 / user / hadoop / file1 / user / hadoop / file2</code></li><li><code>hadoop fs -truncate -w 127 hdfs：//nn1.example.com/user/hadoop/file1</code></li></ul><h3 id="usage"><a href="#usage" class="headerlink" title="usage"></a>usage</h3><p>用法：<code>hadoop fs -usage命令</code></p><p>返回单个命令的帮助。</p><h2 id="使用对象存储"><a href="#使用对象存储" class="headerlink" title="使用对象存储"></a>使用对象存储</h2><p>Hadoop FileSystem shell可与Object Stores（如Amazon S3，Azure WASB和OpenStack Swift）配合使用。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">＃创建一个目录</span><br><span class="line">hadoop fs -mkdir s3a：// bucket / datasets /</span><br><span class="line"></span><br><span class="line">＃从群集文件系统上传文件</span><br><span class="line">hadoop fs -put /datasets/example.orc s3a：// bucket / datasets /</span><br><span class="line"></span><br><span class="line">＃触摸文件</span><br><span class="line">hadoop fs -touchz wasb：//yourcontainer@youraccount.blob.core.windows.net/touched</span><br></pre></td></tr></table></figure><p>与普通文件系统不同，重命名对象存储中的文件和目录通常需要与被操作对象的大小成比例的时间。由于许多文件系统shell操作使用重命名作为操作的最后阶段，因此跳过该阶段可以避免长时间的延迟。</p><p>特别是，<code>put</code>和<code>copyFromLocal</code>命令都应该为直接上载设置<code>-d</code>选项。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Upload a file from the cluster filesystem</span></span><br><span class="line">hadoop fs -put -d /datasets/example.orc s3a://bucket/datasets/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Upload a file from under the user's home directory in the local filesystem.</span></span><br><span class="line"><span class="comment"># Note it is the shell expanding the "~", not the hadoop fs command</span></span><br><span class="line">hadoop fs -copyFromLocal -d -f ~/datasets/devices.orc s3a://bucket/datasets/</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a file from stdin</span></span><br><span class="line"><span class="comment"># the special "-" source means "use stdin"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello"</span> | hadoop fs -put -d -f - wasb://yourcontainer@youraccount.blob.core.windows.net/hello.txt</span><br></pre></td></tr></table></figure><p>可以下载和查看对象：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">＃将目录复制到本地文件系统</span><br><span class="line">hadoop fs -copyToLocal s3a：// bucket / datasets /</span><br><span class="line"></span><br><span class="line">＃将文件从对象库复制到集群文件系统。</span><br><span class="line">hadoop fs -get wasb：//yourcontainer@youraccount.blob.core.windows.net/hello.txt / examples</span><br><span class="line"></span><br><span class="line">＃打印对象</span><br><span class="line">hadoop fs -cat wasb：//yourcontainer@youraccount.blob.core.windows.net/hello.txt</span><br><span class="line"></span><br><span class="line">＃打印对象，必要时解压缩</span><br><span class="line">hadoop fs -text wasb：//yourcontainer@youraccount.blob.core.windows.net/hello.txt</span><br><span class="line"></span><br><span class="line"><span class="comment">##将日志文件下载到本地文件中</span></span><br><span class="line">hadoop fs -getmerge wasb：//yourcontainer@youraccount.blob.core.windows.net/logs \ * log.txt</span><br></pre></td></tr></table></figure><p>列出许多文件的命令往往比使用HDFS或其他文件系统时慢得多</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -count s3a://bucket/</span><br><span class="line">hadoop fs -du s3a://bucket/</span><br></pre></td></tr></table></figure><p>其他慢速命令包括<code>find</code>，<code>mv</code>，<code>cp</code>和<code>rm</code>。</p><h2 id="Find"><a href="#Find" class="headerlink" title="Find"></a><strong>Find</strong></h2><p>在提供路径下有许多目录的大型商店中，这可能会非常慢。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># enumerate all files in the object store's container.</span></span><br><span class="line">hadoop fs -find s3a://bucket/ -<span class="built_in">print</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># remember to escape the wildcards to stop the shell trying to expand them first</span></span><br><span class="line">hadoop fs -find s3a://bucket/datasets/ -name \*.txt -<span class="built_in">print</span></span><br></pre></td></tr></table></figure><h3 id="Rename"><a href="#Rename" class="headerlink" title="Rename"></a><strong>Rename</strong></h3><p>重命名文件的时间取决于其大小。</p><p>重命名目录的时间取决于该目录下所有文件的数量和大小。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv s3a：// bucket / datasets s3a：// bucket / historical</span><br></pre></td></tr></table></figure><p>如果操作中断，则对象存储将处于未定义状态。</p><h3 id="Copy"><a href="#Copy" class="headerlink" title="Copy"></a><strong>Copy</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp s3a://bucket/datasets s3a://bucket/historical</span><br></pre></td></tr></table></figure><p>复制操作读取每个文件，然后将其写回对象存储区; 完成的时间取决于要复制的数据量，以及本地计算机和对象存储库之间双向带宽。</p><p><strong>计算机离对象存储器越远，复制所用的时间越长</strong></p><h2 id="删除对象"><a href="#删除对象" class="headerlink" title="删除对象"></a>删除对象</h2><p>该<code>RM</code>命令删除对象和目录满对象。如果对象存储<em>最终</em>是<em>一致的</em>，则<code>fs ls</code>命令和其他访问器可能会暂时返回现在删除的对象的详细信息; 这是对象存储的工件，无法避免。</p><p>如果文件系统客户端配置为将文件复制到废纸篓目录，则该文件系统将位于存储桶中; 然后，<code>rm</code>操作将花费与数据大小成比例的时间。此外，删除的文件将继续产生存储成本。</p><p>要避免这种情况，请使用<code>-skipTrash</code>选项。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm -skipTrash s3a：// bucket / dataset</span><br></pre></td></tr></table></figure><p>可以使用<code>expunge</code>命令清除移动到<code>.Trash</code>目录的数据。由于此命令仅适用于默认文件系统，因此必须将其配置为使默认文件系统成为目标对象库。<code></code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -expunge -D fs.defaultFS = s3a：// bucket /</span><br></pre></td></tr></table></figure><h2 id="覆盖对象"><a href="#覆盖对象" class="headerlink" title="覆盖对象"></a>覆盖对象</h2><p>如果对象存储<em>最终</em>是<em>一致的</em>，则任何覆盖现有对象的操作可能不会立即对所有客户端/查询可见。即：稍后查询相同对象的状态或内容的操作可以获得前一个对象。在读取单个对象时，这有时可以在同一客户端中显示。</p><p>避免使用一系列覆盖对象的命令，然后立即处理更新的数据; 存在以下风险：将使用先前的数据。</p><h2 id="时间戳"><a href="#时间戳" class="headerlink" title="时间戳"></a>时间戳</h2><p>对象存储中对象和目录的时间戳可能不遵循HDFS中文件和目录的行为。</p><ol><li>对象的创建和初始修改时间将是它在对象库上创建的时间; 这将是写入过程的结束，而不是开始。</li><li>时间戳将取自对象存储基础架构的时钟，而不是客户端的时钟。</li><li>如果覆盖了对象，则将更新修改时间。</li><li>目录可能有也可能没有有效的时间戳。当更新下面的对象时，他们不太可能更新修改时间。</li><li>该<code>atime的</code>访问时间特征不被任何在Apache Hadoop的代码库中找到的对象存储的支持。</li></ol><p>有关这可能如何影响<code>distcp -update</code>操作的详细信息，请参阅<code>DistCp</code>文档。</p><h2 id="安全模型和操作"><a href="#安全模型和操作" class="headerlink" title="安全模型和操作"></a>安全模型和操作</h2><p>对象存储的安全性和权限模型通常与Unix风格的文件系统非常不同; 查询或操纵权限的操作通常不受支持。</p><p>适用的操作包括：<code>chgrp</code>，<code>chmod</code>，<code>chown</code>，<code>getfacl</code>和<code>setfacl</code>。相关属性命令<code>getfattr</code>和<code>setfattr</code>通常也不可用。</p><ul><li>列出权限和用户/组详细信息的文件系统命令通常模拟这些详细信息。</li><li>尝试保留权限的操作（例如<code>fs -put -p</code>）不会因此原因保留权限。（特例：<code>wasb：//</code>，它保留权限但不强制执行）。</li></ul><p>当与只读对象存储交互时，“list”和“stat”命令中的权限可以指示用户具有写访问权限，而实际上他们没有。</p><p>对象存储通常具有自己的权限模型，模型可以通过特定于商店的工具进行操作。请注意，对象存储可能提供的某些权限（例如只写路径或根路径上的不同权限）可能与Hadoop文件系统客户端不兼容。这些往往需要对它们写入数据的整个对象存储桶/容器进行完全读写访问。</p><p>作为如何模拟权限的示例，这里是亚马逊的公共，只读桶Landsat图像的列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls s3a://landsat-pds/</span><br><span class="line">Found 10 items</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/L8</span><br><span class="line">-rw-rw-rw-   1 mapred      23764 2015-01-28 18:13 s3a://landsat-pds/index.html</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/landsat-pds_stats</span><br><span class="line">-rw-rw-rw-   1 mapred        105 2016-08-19 18:12 s3a://landsat-pds/robots.txt</span><br><span class="line">-rw-rw-rw-   1 mapred         38 2016-09-26 12:16 s3a://landsat-pds/run_info.json</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/runs</span><br><span class="line">-rw-rw-rw-   1 mapred   27458808 2016-09-26 12:16 s3a://landsat-pds/scene_list.gz</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/tarq</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/tarq_corrupt</span><br><span class="line">drwxrwxrwx   - mapred          0 2016-09-26 12:16 s3a://landsat-pds/<span class="built_in">test</span></span><br></pre></td></tr></table></figure><ol><li>所有文件都列为具有完全读/写权限。</li><li>所有目录似乎都具有完整的<code>rwx</code>权限。</li><li>所有文件的复制计数为“1”。</li><li>所有文件和目录的所有者被声明为当前用户（<code>mapred</code>）。</li><li>所有目录的时间戳实际上是执行<code>-ls</code>操作的时间戳。这是因为这些目录不是商店中的实际对象; 它们是基于路径下对象存在的模拟目录。</li></ol><p>当尝试删除其中一个文件时，操作失败 - 尽管<code>ls</code>命令显示的权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -rm s3a://landsat-pds/scene_list.gz</span><br><span class="line">rm: s3a://landsat-pds/scene_list.gz: delete on s3a://landsat-pds/scene_list.gz:</span><br><span class="line">  com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3;</span><br><span class="line">  Status Code: 403; Error Code: AccessDenied; Request ID: 1EF98D5957BCAB3D),</span><br><span class="line">  S3 Extended Request ID: wi3veOXFuFqWBUCJgV3Z+NQVj9gWgZVdXlPU4KBbYMsw/gA+hyhRXcaQ+PogOsDgHh31HlTCebQ=</span><br></pre></td></tr></table></figure><p>这表明列出的权限不能作为写访问的证据; 只有对象操作才能确定这一点。</p><p>请注意，Microsoft Azure WASB文件系统允许设置和检查权限，但实际上并未强制实施权限。此功能提供了使用DistCp备份HDFS目录树的功能，其权限得以保留，权限可在将目录复制回HDFS时恢复。但是，为了保护对对象存储中数据的访问，<span class="exturl" data-url="aHR0cHM6Ly9henVyZS5taWNyb3NvZnQuY29tL2VuLXVzL2RvY3VtZW50YXRpb24vYXJ0aWNsZXMvc3RvcmFnZS1zZWN1cml0eS1ndWlkZS8=" title="https://azure.microsoft.com/en-us/documentation/articles/storage-security-guide/">必须使用<i class="fa fa-external-link"></i></span> Azure <span class="exturl" data-url="aHR0cHM6Ly9henVyZS5taWNyb3NvZnQuY29tL2VuLXVzL2RvY3VtZW50YXRpb24vYXJ0aWNsZXMvc3RvcmFnZS1zZWN1cml0eS1ndWlkZS8=" title="https://azure.microsoft.com/en-us/documentation/articles/storage-security-guide/">自己的模型和工具<i class="fa fa-external-link"></i></span>。</p><h2 id="Commands-of-limited-value"><a href="#Commands-of-limited-value" class="headerlink" title="Commands of limited value"></a>Commands of limited value</h2><p>以下是通常没有效果的shell命令列表 - 实际上可能会失败。</p><table><thead><tr><th>command</th><th>limitations</th></tr></thead><tbody><tr><td><code>appendToFile</code></td><td>generally unsupported</td></tr><tr><td><code>checksum</code></td><td>the usual checksum is “NONE”</td></tr><tr><td><code>chgrp</code></td><td>generally unsupported permissions model; no-op</td></tr><tr><td><code>chmod</code></td><td>generally unsupported permissions model; no-op</td></tr><tr><td><code>chown</code></td><td>generally unsupported permissions model; no-op</td></tr><tr><td><code>createSnapshot</code></td><td>generally unsupported</td></tr><tr><td><code>deleteSnapshot</code></td><td>generally unsupported</td></tr><tr><td><code>df</code></td><td>default values are normally displayed</td></tr><tr><td><code>getfacl</code></td><td>may or may not be supported</td></tr><tr><td><code>getfattr</code></td><td>generally supported</td></tr><tr><td><code>renameSnapshot</code></td><td>generally unsupported</td></tr><tr><td><code>setfacl</code></td><td>generally unsupported permissions model</td></tr><tr><td><code>setfattr</code></td><td>generally unsupported permissions model</td></tr><tr><td><code>setrep</code></td><td>has no effect</td></tr><tr><td><code>truncate</code></td><td>generally unsupported</td></tr></tbody></table><p>不同的对象存储客户端<em>可能</em>支持这些命令：请查阅文档并针对目标存储进行测试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概览&quot;&gt;&lt;a href=&quot;#概览&quot; class=&quot;headerlink&quot; title=&quot;概览&quot;&gt;&lt;/a&gt;概览&lt;/h2&gt;&lt;p&gt;文件系统（FS）shell包括各种类似shell的命令，这些命令直接与Hadoop分布式文件系统（HDFS）以及Hadoop支持的其他文件系
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop集群设置</title>
    <link href="https://tangguangen.com/Hadoop%E9%9B%86%E7%BE%A4%E8%AE%BE%E7%BD%AE/"/>
    <id>https://tangguangen.com/Hadoop集群设置/</id>
    <published>2018-11-24T07:34:01.000Z</published>
    <updated>2018-11-26T12:08:36.507Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>本文档描述了如何安装和配置Hadoop集群，范围从几个节点到具有数千个节点的极大集群。要使用Hadoop，您可能首先要将其安装在一台计算机上（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>）。</p><p>本文档不包括<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TZWN1cmVNb2RlLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html">安全性<i class="fa fa-external-link"></i></span>或高可用性等高级主题。</p><h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><ul><li>安装Java。有关已知的好版本，请参阅<span class="exturl" data-url="aHR0cDovL3dpa2kuYXBhY2hlLm9yZy9oYWRvb3AvSGFkb29wSmF2YVZlcnNpb25z" title="http://wiki.apache.org/hadoop/HadoopJavaVersions">Hadoop Wiki<i class="fa fa-external-link"></i></span>。</li><li>从Apache镜像下载稳定版本的Hadoop。</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>安装Hadoop集群通常涉及在集群中的所有计算机上解压缩软件，或者通过适合您的操作系统的打包系统进行安装。将硬件划分为几个功能非常重要。</p><p>通常，群集中的<strong>一台计算机被指定为NameNode</strong>，而<strong>一台计算机则被指定为ResourceManager</strong>。这些都是<strong>masters</strong>。其他服务（例如Web App Proxy Server和MapReduce Job History server）通常在专用硬件或共享基础架构上运行，具体取决于负载。</p><p>集群中的<strong>其余计算机充当DataNode和NodeManager</strong>。这些是<strong>slaves</strong>。</p><h2 id="在非安全模式下配置Hadoop"><a href="#在非安全模式下配置Hadoop" class="headerlink" title="在非安全模式下配置Hadoop"></a>在非安全模式下配置Hadoop</h2><p>Hadoop的 Java 配置由两种类型的重要配置文件驱动：</p><ul><li><strong>只读默认配置</strong>  - <code>core-default.xml</code>, <code>hdfs-default.xml</code>, <code>yarn-default.xml</code> 和<code>mapred-default.xml</code>.</li><li><strong>具体站点的配置</strong>- <code>etc/hadoop/core-site.xml</code>, <code>etc/hadoop/hdfs-site.xml</code>, <code>etc/hadoop/yarn-site.xml</code> 和<code>etc/hadoop/mapred-site.xml</code>.</li></ul><p>此外，您可以通过<code>etc/hadoop/hadoop-env.sh</code>和<code>etc/hadoop/yarn-env.sh</code>设置特定于站点的值来控制分发的 bin/ 目录中的Hadoop脚本。</p><p>要配置Hadoop集群，您需要配置Hadoop后台进程执行的<code>environment</code>以及Hadoop后台进程的<code>configuration parameters</code>。</p><p><strong>HDFS</strong>后台进程是 <strong>NameNode</strong>，<strong>SecondaryNameNode</strong> 和 <strong>DataNode</strong>。<strong>YARN</strong>后台进程是<strong>ResourceManager</strong>，<strong>NodeManager</strong>和<strong>WebAppProxy</strong>。如果要使用MapReduce，则MapReduce Job History Server也将运行。对于大型安装，这些通常运行在不同的主机上。</p><h3 id="Hadoop的环境配置"><a href="#Hadoop的环境配置" class="headerlink" title="Hadoop的环境配置"></a>Hadoop的环境配置</h3><p>管理员应该使用<code>etc/hadoop/hadoop-env.sh</code>和<code>etc/hadoop/mapred-env.sh</code>以及<code>etc/hadoop/yarn-env.sh</code>脚本来对Hadoop环境进行特定于站点的自定义。</p><p>至少，您必须指定<code>JAVA_HOME，</code>以便在每个远程节点上正确定义它。</p><p>管理员可以使用下表中显示的配置选项配置各个后台进程：</p><table><thead><tr><th>Daemon</th><th>Environment Variable</th></tr></thead><tbody><tr><td>NameNode</td><td>HADOOP_NAMENODE_OPTS</td></tr><tr><td>DataNode</td><td>HADOOP_DATANODE_OPTS</td></tr><tr><td>Secondary NameNode</td><td>HADOOP_SECONDARYNAMENODE_OPTS</td></tr><tr><td>ResourceManager</td><td>YARN_RESOURCEMANAGER_OPTS</td></tr><tr><td>NodeManager</td><td>YARN_NODEMANAGER_OPTS</td></tr><tr><td>WebAppProxy</td><td>YARN_PROXYSERVER_OPTS</td></tr><tr><td>Map Reduce Job History Server</td><td>HADOOP_JOB_HISTORYSERVER_OPTS</td></tr></tbody></table><p>例如，要将NameNode配置为使用parallelGC，应在hadoop-env.sh中添加以下语句：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_NAMENODE_OPTS=<span class="string">"-XX:+UseParallelGC"</span></span><br></pre></td></tr></table></figure><p>有关其他示例，请参阅 etc/hadoop/hadoop-env.sh</p><p>您可以自定义的其他有用配置参数包括：</p><ul><li><code>HADOOP_PID_DIR</code> - 存储后台进程的进程标识文件的目录。</li><li><code>HADOOP_LOG_DIR</code> - 存储后台程序日志文件的目录。如果日志文件不存在，则会自动创建日志文件。</li><li><code>HADOOP_HEAPSIZE</code> / <code>YARN_HEAPSIZE</code> - 要使用的最大堆大小，以MB为单位，例如，如果varibale设置为1000，则堆将设置为1000MB。这用于配置守护程序的堆大小。默认情况下，该值为1000.如果要为每个可以使用的守护程序单独配置值。</li></ul><p>在大多数情况下，您应该指定<code>HADOOP_PID_DIR</code>和<code>HADOOP_LOG_DIR</code>目录，以便它们只能由将要运行hadoop守护程序的用户写入。否则就有可能发生符号链接攻击。</p><p>在系统范围的shell环境配置中配置<code>HADOOP_PREFIX</code>也是传统的。例如，在<code>/etc/profile.d中</code>有一个简单的脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_PREFIX=/path/to/hadoop</span><br><span class="line">export HADOOP_PREFIX</span><br></pre></td></tr></table></figure><table><thead><tr><th>Daemon</th><th>Environment Variable</th></tr></thead><tbody><tr><td>ResourceManager</td><td>YARN_RESOURCEMANAGER_HEAPSIZE</td></tr><tr><td>NodeManager</td><td>YARN_NODEMANAGER_HEAPSIZE</td></tr><tr><td>WebAppProxy</td><td>YARN_PROXYSERVER_HEAPSIZE</td></tr><tr><td>Map Reduce Job History Server</td><td>HADOOP_JOB_HISTORYSERVER_HEAPSIZE</td></tr></tbody></table><h3 id="配置Hadoop"><a href="#配置Hadoop" class="headerlink" title="配置Hadoop"></a>配置Hadoop</h3><p>本节介绍在给定配置文件中指定的重要参数：</p><ul><li><code>etc/hadoop/core-site.xml</code></li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>fs.defaultFS</code></td><td>NameNode URI</td><td><span class="exturl" data-url="aGRmczovL2hvc3Q6cG9ydC8=" title="hdfs://host:port/">hdfs://host:port/<i class="fa fa-external-link"></i></span></td></tr><tr><td><code>io.file.buffer.size</code></td><td>131072</td><td>SequenceFiles中使用的读/写缓冲区的大小。</td></tr></tbody></table><ul><li><code>etc/hadoop/hdfs-site.xml</code></li><li>NameNode的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>dfs.namenode.name.dir</code></td><td>NameNode存储名称空间和持续事务日志的本地文件系统上的路径。</td><td>如果这是一个以逗号分隔的目录列表，那么名称表将复制到所有目录中，以实现冗余。</td></tr><tr><td><code>dfs.hosts</code> / <code>dfs.hosts.exclude</code></td><td>List of permitted/excluded DataNodes.</td><td>If necessary, use these files to control the list of allowable datanodes.</td></tr><tr><td><code>dfs.blocksize</code></td><td>268435456</td><td>HDFS blocksize of 256MB for large file-systems.</td></tr><tr><td><code>dfs.namenode.handler.count</code></td><td>100</td><td>More NameNode server threads to handle RPCs from large number of DataNodes.</td></tr></tbody></table><ul><li>Configurations for DataNode:</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>dfs.datanode.data.dir</code></td><td>Comma separated list of paths on the local filesystem of a <code>DataNode</code> where it should store its blocks.</td><td>If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices.</td></tr></tbody></table><ul><li><code>etc/hadoop/yarn-site.xml</code></li><li>ResourceManager和NodeManager的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.acl.enable</code></td><td><code>true</code> / <code>false</code></td><td>Enable ACLs? Defaults to <em>false</em>.</td></tr><tr><td><code>yarn.admin.acl</code></td><td>Admin ACL</td><td>ACL to set admins on the cluster. ACLs are of for <em>comma-separated-usersspacecomma-separated-groups</em>. Defaults to special value of <strong>*</strong> which means <em>anyone</em>. Special value of just <em>space</em> means no one has access.</td></tr><tr><td><code>yarn.log-aggregation-enable</code></td><td><em>false</em></td><td>Configuration to enable or disable log aggregation</td></tr></tbody></table><ul><li>ResourceManager的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.resourcemanager.address</code></td><td><code>ResourceManager</code>host:port for clients to submit jobs.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.scheduler.address</code></td><td><code>ResourceManager</code>host:port for ApplicationMasters to talk to Scheduler to obtain resources.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.resource-tracker.address</code></td><td><code>ResourceManager</code>host:port for NodeManagers.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.admin.address</code></td><td><code>ResourceManager</code>host:port for administrative commands.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.webapp.address</code></td><td><code>ResourceManager</code>web-ui host:port.</td><td><em>host:port</em> If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>.</td></tr><tr><td><code>yarn.resourcemanager.hostname</code></td><td><code>ResourceManager</code>host.</td><td><em>host</em> Single hostname that can be set in place of setting all <code>yarn.resourcemanager*address</code> resources. Results in default ports for ResourceManager components.</td></tr><tr><td><code>yarn.resourcemanager.scheduler.class</code></td><td><code>ResourceManager</code>Scheduler class.</td><td><code>CapacityScheduler</code> (recommended), <code>FairScheduler</code> (also recommended), or <code>FifoScheduler</code>. Use a fully qualified class name, e.g., <code>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</code>.</td></tr><tr><td><code>yarn.scheduler.minimum-allocation-mb</code></td><td>Minimum limit of memory to allocate to each container request at the <code>Resource Manager</code>.</td><td>In MBs</td></tr><tr><td><code>yarn.scheduler.maximum-allocation-mb</code></td><td>Maximum limit of memory to allocate to each container request at the <code>Resource Manager</code>.</td><td>In MBs</td></tr><tr><td><code>yarn.resourcemanager.nodes.include-path</code> / <code>yarn.resourcemanager.nodes.exclude-path</code></td><td>List of permitted/excluded NodeManagers.</td><td>If necessary, use these files to control the list of allowable NodeManagers.</td></tr></tbody></table><ul><li>NodeManager的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.nodemanager.resource.memory-mb</code></td><td>Resource i.e. available physical memory, in MB, for given <code>NodeManager</code></td><td>Defines total available resources on the <code>NodeManager</code> to be made available to running containers</td></tr><tr><td><code>yarn.nodemanager.vmem-pmem-ratio</code></td><td>Maximum ratio by which virtual memory usage of tasks may exceed physical memory</td><td>The virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio.</td></tr><tr><td><code>yarn.nodemanager.local-dirs</code></td><td>Comma-separated list of paths on the local filesystem where intermediate data is written.</td><td>Multiple paths help spread disk i/o.</td></tr><tr><td><code>yarn.nodemanager.log-dirs</code></td><td>Comma-separated list of paths on the local filesystem where logs are written.</td><td>Multiple paths help spread disk i/o.</td></tr><tr><td><code>yarn.nodemanager.log.retain-seconds</code></td><td><em>10800</em></td><td>Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled.</td></tr><tr><td><code>yarn.nodemanager.remote-app-log-dir</code></td><td><em>/logs</em></td><td>HDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled.</td></tr><tr><td><code>yarn.nodemanager.remote-app-log-dir-suffix</code></td><td><em>logs</em></td><td>Suffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled.</td></tr><tr><td><code>yarn.nodemanager.aux-services</code></td><td>mapreduce_shuffle</td><td>Shuffle service that needs to be set for Map Reduce applications.</td></tr></tbody></table><ul><li>Configurations for History Server (Needs to be moved elsewhere):</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.log-aggregation.retain-seconds</code></td><td><em>-1</em></td><td>How long to keep aggregation logs before deleting them. -1 disables. Be careful, set this too small and you will spam the name node.</td></tr><tr><td><code>yarn.log-aggregation.retain-check-interval-seconds</code></td><td><em>-1</em></td><td>Time between checks for aggregated log retention. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful, set this too small and you will spam the name node.</td></tr></tbody></table><ul><li><code>etc/hadoop/mapred-site.xml</code></li><li>MapReduce应用程序的配置：</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>mapreduce.framework.name</code></td><td>yarn</td><td>Execution framework set to Hadoop YARN.</td></tr><tr><td><code>mapreduce.map.memory.mb</code></td><td>1536</td><td>Larger resource limit for maps.</td></tr><tr><td><code>mapreduce.map.java.opts</code></td><td>-Xmx1024M</td><td>Larger heap-size for child jvms of maps.</td></tr><tr><td><code>mapreduce.reduce.memory.mb</code></td><td>3072</td><td>Larger resource limit for reduces.</td></tr><tr><td><code>mapreduce.reduce.java.opts</code></td><td>-Xmx2560M</td><td>Larger heap-size for child jvms of reduces.</td></tr><tr><td><code>mapreduce.task.io.sort.mb</code></td><td>512</td><td>Higher memory-limit while sorting data for efficiency.</td></tr><tr><td><code>mapreduce.task.io.sort.factor</code></td><td>100</td><td>More streams merged at once while sorting files.</td></tr><tr><td><code>mapreduce.reduce.shuffle.parallelcopies</code></td><td>50</td><td>Higher number of parallel copies run by reduces to fetch outputs from very large number of maps.</td></tr></tbody></table><ul><li>Configurations for MapReduce JobHistory Server:</li></ul><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>mapreduce.jobhistory.address</code></td><td>MapReduce JobHistory Server <em>host:port</em></td><td>Default port is 10020.</td></tr><tr><td><code>mapreduce.jobhistory.webapp.address</code></td><td>MapReduce JobHistory Server Web UI <em>host:port</em></td><td>Default port is 19888.</td></tr><tr><td><code>mapreduce.jobhistory.intermediate-done-dir</code></td><td>/mr-history/tmp</td><td>Directory where history files are written by MapReduce jobs.</td></tr><tr><td><code>mapreduce.jobhistory.done-dir</code></td><td>/mr-history/done</td><td>Directory where history files are managed by the MR JobHistory Server.</td></tr></tbody></table><h2 id="监控NodeManager的健康状况"><a href="#监控NodeManager的健康状况" class="headerlink" title="监控NodeManager的健康状况"></a>监控NodeManager的健康状况</h2><p>Hadoop提供了一种机制，管理员可以通过该机制定期运行管理员提供的脚本以确定节点是否健康。</p><p>管理员可以通过在脚本中执行对其选择的任何检查来确定节点是否处于正常状态。如果脚本检测到节点处于不健康状态，则必须以字符串ERROR开头的标准输出行。NodeManager定期生成脚本并检查其输出。如果脚本的输出包含字符串ERROR，如上所述，节点的状态将报告为运行状况<code>不佳</code>并且ResourceManager将节点列入黑名单。不会为此节点分配其他任务。但是，NodeManager继续运行脚本，因此如果节点再次变得健康，它将自动从ResourceManager上的黑名单节点中删除。如果节点不健康，则可以在ResourceManager Web界面中为管理员提供节点的运行状况以及脚本的输出。自节点健康以来的时间也显示在Web界面上。</p><p>以下参数可用于控制<code>etc / hadoop / yarn-site.xml中</code>的节点运行状况监视脚本。</p><table><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>yarn.nodemanager.health-checker.script.path</code></td><td>Node health script</td><td>Script to check for node’s health status.</td></tr><tr><td><code>yarn.nodemanager.health-checker.script.opts</code></td><td>Node health script options</td><td>Options for script to check for node’s health status.</td></tr><tr><td><code>yarn.nodemanager.health-checker.interval-ms</code></td><td>Node health script interval</td><td>Time interval for running health script.</td></tr><tr><td><code>yarn.nodemanager.health-checker.script.timeout-ms</code></td><td>Node health script timeout interval</td><td>Timeout for health script execution.</td></tr></tbody></table><p>如果只有部分本地磁盘变坏，则运行状况检查程序脚本不应该给出错误。NodeManager能够定期检查本地磁盘的运行状况（具体检查nodemanager-local-dirs和nodemanager-log-dirs），并在达到配置属性yarn.nodemanager设置的错误目录数阈值后.disk-health-checker.min-healthy-disks，整个节点被标记为运行状况不佳，此信息也会发送给资源管理器。引导磁盘被突袭或健康检查程序脚本识别引导磁盘中的故障。</p><h2 id="Slaves-文件"><a href="#Slaves-文件" class="headerlink" title="Slaves 文件"></a>Slaves 文件</h2><p>列出<code>etc / hadoop / slaves</code>文件中的所有从属主机名或IP地址，每行一个。Helper脚本（如下所述）将使用<code>etc / hadoop / slaves</code>文件一次在多个主机上运行命令。它不用于任何基于Java的Hadoop配置。为了使用此功能，必须为用于运行Hadoop的帐户建立ssh信任（通过无密码ssh或其他方式，如Kerberos）。</p><h2 id="Hadoop机架意识"><a href="#Hadoop机架意识" class="headerlink" title="Hadoop机架意识"></a>Hadoop机架意识</h2><p>许多Hadoop组件都具有机架感知功能，并利用网络拓扑结构提高性能和安全性。Hadoop守护程序通过调用管理员配置的模块来获取集群中从站的机架信息。有关更多具体信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9SYWNrQXdhcmVuZXNzLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness<i class="fa fa-external-link"></i></span>文档。</p><p>强烈建议在启动HDFS之前配置机架感知。</p><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p>Hadoop 通过Apache Commons Logging框架使用<span class="exturl" data-url="aHR0cDovL2xvZ2dpbmcuYXBhY2hlLm9yZy9sb2c0ai8yLngv" title="http://logging.apache.org/log4j/2.x/">Apache log4j<i class="fa fa-external-link"></i></span>进行日志记录。编辑<code>etc / hadoop / log4j.properties</code>文件以自定义Hadoop守护程序的日志记录配置（日志格式等）。</p><h2 id="操作Hadoop集群"><a href="#操作Hadoop集群" class="headerlink" title="操作Hadoop集群"></a>操作Hadoop集群</h2><p>完成所有必要的配置后，将文件分发到所有计算机上的<code>HADOOP_CONF_DIR</code>目录。这应该是所有计算机上的相同目录。</p><p>通常，建议HDFS和YARN作为单独的用户运行。在大多数安装中，HDFS进程以’hdfs’的形式执行。YARN通常使用’yarn’帐户。、</p><h3 id="Hadoop启动"><a href="#Hadoop启动" class="headerlink" title="Hadoop启动"></a>Hadoop启动</h3><p>要启动Hadoop集群，您需要启动HDFS和YARN集群。</p><p>第一次启动HDFS时，必须对其进行格式化。将新的分布式文件系统格式化为<em>hdfs</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format &lt;cluster_name&gt;</span><br></pre></td></tr></table></figure><p>在指定节点上使用以下命令以<em>hdfs</em>启动HDFS NameNode ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> --script hdfs start namenode</span><br></pre></td></tr></table></figure><p>使用以下命令在每个指定节点上以<em>hdfs</em>启动HDFS DataNode ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemons.sh --config <span class="variable">$HADOOP_CONF_DIR</span> --script hdfs start datanode</span><br></pre></td></tr></table></figure><p>如果配置了<code>etc/hadoop/slaves</code>和ssh trusted access（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>），则可以使用<strong>实用程序脚本启动所有HDFS进程</strong>。作为<em>hdfs</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>使用以下命令启动YARN，在指定的ResourceManager上以<em>yarn形式运行</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start resourcemanager</span><br></pre></td></tr></table></figure><p>运行脚本以在每个指定的主机上启动NodeManager作为<em>yarn</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemons.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start nodemanager</span><br></pre></td></tr></table></figure><p>启动独立的WebAppProxy服务器。以<em>纱线形式</em>在WebAppProxy服务器上运行。如果使用多个服务器进行负载平衡，则应在每个服务器上运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start proxyserver</span><br></pre></td></tr></table></figure><p>如果配置了<code>etc/hadoop/slaves</code>和ssh trusted access（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>），则可以使用实用程序脚本启动所有YARN进程。作为 As <em>yarn</em>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>使用以下命令启动MapReduce JobHistory Server，在指定的服务器上以<em>mapred运行</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[mapred]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/mr-jobhistory-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start historyserver</span><br></pre></td></tr></table></figure><h3 id="Hadoop关闭"><a href="#Hadoop关闭" class="headerlink" title="Hadoop关闭"></a>Hadoop关闭</h3><p>使用以下命令停止NameNode，在指定的NameNode上运行为<em>hdfs</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs] $ $ HADOOP_PREFIX / sbin / hadoop-daemon.sh --config $ HADOOP_CONF_DIR --script hdfs stop namenode</span><br></pre></td></tr></table></figure><p>运行脚本以将DataNode作为<em>hdfs</em>停止：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs] $ $ HADOOP_PREFIX / sbin / hadoop-daemons.sh --config $ HADOOP_CONF_DIR --script hdfs stop datanode</span><br></pre></td></tr></table></figure><p>如果配置了<code>etc / hadoop / slaves</code>和ssh trusted access（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>），则可以使用实用程序脚本停止所有HDFS进程。作为<em>hdfs</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hdfs] $ $ HADOOP_PREFIX / sbin / stop-dfs.sh</span><br></pre></td></tr></table></figure><p>使用以下命令停止ResourceManager，在指定的ResourceManager上作为<em>yarn运行</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn] $ $ HADOOP_YARN_HOME / sbin / yarn-daemon.sh --config $ HADOOP_CONF_DIR stop resourcemanager</span><br></pre></td></tr></table></figure><p>运行脚本以将从站上的NodeManager作为<em>yarn</em>停止：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn] $ $ HADOOP_YARN_HOME / sbin / yarn-daemons.sh --config $ HADOOP_CONF_DIR stop nodemanager</span><br></pre></td></tr></table></figure><p>如果配置了<code>etc / hadoop / slaves</code>和ssh trusted access（请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">单节点设置<i class="fa fa-external-link"></i></span>），则可以使用实用程序脚本停止所有YARN进程。As <em>hdfs</em>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn] $ $ HADOOP_PREFIX / sbin / stop-yarn.sh</span><br></pre></td></tr></table></figure><p>停止WebAppProxy服务器。以<em>纱线形式</em>在WebAppProxy服务器上运行。如果使用多个服务器进行负载平衡，则应在每个服务器上运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[yarn] $ $ HADOOP_YARN_HOME / sbin / yarn-daemon.sh --config $ HADOOP_CONF_DIR stop proxyserver</span><br></pre></td></tr></table></figure><p>使用以下命令停止MapReduce JobHistory Server，在指定的服务器上以<em>mapred运行</em>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[mapred] $ $ HADOOP_PREFIX / sbin / mr-jobhistory-daemon.sh --config $ HADOOP_CONF_DIR stop historyserver</span><br></pre></td></tr></table></figure><h2 id="Web界面"><a href="#Web界面" class="headerlink" title="Web界面"></a>Web界面</h2><p>一旦Hadoop集群启动并运行，请检查组件的web-ui，如下所述：</p><table><thead><tr><th>Daemon</th><th>Web Interface</th><th>Notes</th></tr></thead><tbody><tr><td>NameNode</td><td><span class="exturl" data-url="aHR0cDovL25uX2hvc3Q6cG9ydC8=" title="http://nn_host:port/">http://nn_host:port/<i class="fa fa-external-link"></i></span></td><td>Default HTTP port is 50070.</td></tr><tr><td>ResourceManager</td><td><span class="exturl" data-url="aHR0cDovL3JtX2hvc3Q6cG9ydC8=" title="http://rm_host:port/">http://rm_host:port/<i class="fa fa-external-link"></i></span></td><td>Default HTTP port is 8088.</td></tr><tr><td>MapReduce JobHistory Server</td><td><span class="exturl" data-url="aHR0cDovL2poc19ob3N0OnBvcnQv" title="http://jhs_host:port/">http://jhs_host:port/<i class="fa fa-external-link"></i></span></td><td>Default HTTP port is 19888.</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h2&gt;&lt;p&gt;本文档描述了如何安装和配置Hadoop集群，范围从几个节点到具有数千个节点的极大集群。要使用Hadoop，您可能首先要将其安装在一台计算机上
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop: 设置单个节点集群</title>
    <link href="https://tangguangen.com/Hadoop-%E8%AE%BE%E7%BD%AE%E5%8D%95%E4%B8%AA%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4/"/>
    <id>https://tangguangen.com/Hadoop-设置单个节点集群/</id>
    <published>2018-11-23T06:44:41.000Z</published>
    <updated>2018-11-26T12:08:44.307Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>本文档介绍如何设置和配置单节点Hadoop安装，以便您可以使用Hadoop MapReduce和Hadoop分布式文件系统（HDFS）快速执行简单操作。</p><h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><h3 id="支持的平台"><a href="#支持的平台" class="headerlink" title="支持的平台"></a>支持的平台</h3><ul><li>支持GNU / Linux作为开发和生产平台。已经在具有2000个节点的GNU / Linux集群上演示了Hadoop。</li><li>Windows也是受支持的平台，但以下步骤仅适用于Linux。要在Windows上设置Hadoop，请参阅<span class="exturl" data-url="aHR0cDovL3dpa2kuYXBhY2hlLm9yZy9oYWRvb3AvSGFkb29wMk9uV2luZG93cw==" title="http://wiki.apache.org/hadoop/Hadoop2OnWindows">Wiki页面<i class="fa fa-external-link"></i></span>。</li></ul><h3 id="必备软件"><a href="#必备软件" class="headerlink" title="必备软件"></a>必备软件</h3><p>Linux所需的软件包括：</p><ol><li>必须安装Java™。<span class="exturl" data-url="aHR0cDovL3dpa2kuYXBhY2hlLm9yZy9oYWRvb3AvSGFkb29wSmF2YVZlcnNpb25z" title="http://wiki.apache.org/hadoop/HadoopJavaVersions">HadoopJavaVersions<i class="fa fa-external-link"></i></span>描述了推荐的Java版本。</li><li>必须安装ssh并且必须运行sshd才能使用管理远程Hadoop守护程序的Hadoop脚本。</li></ol><h3 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h3><p>如果您的群集没有必需的软件，则需要安装它。</p><p>例如在Ubuntu Linux上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh</span><br><span class="line">$ sudo apt-get install rsync</span><br></pre></td></tr></table></figure><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>要获得Hadoop发行版，请从其中一个<span class="exturl" data-url="aHR0cDovL3d3dy5hcGFjaGUub3JnL2R5bi9jbG9zZXIuY2dpL2hhZG9vcC9jb21tb24v" title="http://www.apache.org/dyn/closer.cgi/hadoop/common/">Apache下载镜像<i class="fa fa-external-link"></i></span>下载最新的稳定版本。</p><h2 id="准备启动Hadoop集群"><a href="#准备启动Hadoop集群" class="headerlink" title="准备启动Hadoop集群"></a>准备启动Hadoop集群</h2><p>解压缩下载的Hadoop发行版。在分发中，编辑文件<code>etc / hadoop / hadoop-env.sh</code>以定义一些参数，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">＃设置为Java安装的根目录</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME = / usr / java / latest</span><br></pre></td></tr></table></figure><p>请尝试以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin / hadoop</span><br></pre></td></tr></table></figure><p>这将显示hadoop脚本的使用文档。</p><p>现在，您已准备好以三种支持模式之一启动Hadoop集群：</p><ul><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjU3RhbmRhbG9uZV9PcGVyYXRpb24=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation">本地（独立）模式<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjUHNldWRvLURpc3RyaWJ1dGVkX09wZXJhdGlvbg==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation">伪分布式模式<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjRnVsbHktRGlzdHJpYnV0ZWRfT3BlcmF0aW9u" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Fully-Distributed_Operation">完全分布式模式<i class="fa fa-external-link"></i></span></li></ul><h2 id="独立操作"><a href="#独立操作" class="headerlink" title="独立操作"></a>独立操作</h2><p>默认情况下，Hadoop配置为以非分布式模式运行，作为单个Java进程。这对调试很有用。</p><p>以下示例复制解压缩的conf目录以用作输入，然后查找并显示给定正则表达式的每个匹配项。输出将写入给定的输出目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir input</span><br><span class="line">$ cp etc/hadoop/*.xml input</span><br><span class="line">$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></span><br><span class="line">$ cat output/*</span><br></pre></td></tr></table></figure><h2 id="伪分布式操作"><a href="#伪分布式操作" class="headerlink" title="伪分布式操作"></a>伪分布式操作</h2><p>Hadoop还可以在伪分布式模式下在单节点上运行，其中每个Hadoop后台进程在单独的Java进程中运行。</p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>如下：</p><p>etc/hadoop/core-site.xml:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>etc/hadoop/hdfs-site.xml:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="设置passphraseless-ssh"><a href="#设置passphraseless-ssh" class="headerlink" title="设置passphraseless ssh"></a>设置passphraseless ssh</h3><p>现在检查您是否可以在没有密码的情况下ssh到localhost：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh localhost</span><br></pre></td></tr></table></figure><p>如果在没有密码短语的情况下无法ssh到localhost，请执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P<span class="string">''</span> -  f~ / .ssh / id_rsa</span><br><span class="line">$ cat~ / .ssh / id_rsa.pub &gt;&gt;〜/ .ssh / authorized_keys</span><br><span class="line">$ chmod 0600~ / .ssh / authorized_keys</span><br></pre></td></tr></table></figure><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><p>以下说明是在本地运行MapReduce作业。如果要在YARN上执行作业，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjWUFSTl9vbl9TaW5nbGVfTm9kZQ==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_Single_Node">单节点<i class="fa fa-external-link"></i></span>上的YARN 。</p><ol><li><p>格式化文件系统：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs namenode -format</span><br></pre></td></tr></table></figure></li><li><p>启动NameNode守护程序和DataNode守护程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>hadoop后台进程日志输出将写入<code>$ HADOOP_LOG_DIR</code>目录（默认为<code>$ HADOOP_HOME / logs</code>）。</p></li><li><p>浏览NameNode的Web界面; 默认情况下，它可用于：</p><ul><li>NameNode - <code>http：// localhost：50070 /</code></li></ul></li><li><p>创建执行MapReduce作业所需的HDFS目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -mkdir /user</span><br><span class="line">$ bin/hdfs dfs -mkdir /user/&lt;username&gt;</span><br></pre></td></tr></table></figure></li><li><p>将输入文件复制到分布式文件系统中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -put etc/hadoop input</span><br></pre></td></tr></table></figure></li><li><p>运行一些提供的示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></span><br></pre></td></tr></table></figure></li><li><p>检查输出文件：将输出文件从分布式文件系统复制到本地文件系统并检查它们：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -get output output</span><br><span class="line">$ cat output/*</span><br></pre></td></tr></table></figure><p>或者</p><p>查看分布式文件系统上的输出文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -cat output/*</span><br></pre></td></tr></table></figure></li><li><p>完成后，停止守护进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure></li></ol><h3 id="YARN在单个节点上"><a href="#YARN在单个节点上" class="headerlink" title="YARN在单个节点上"></a>YARN在单个节点上</h3><p>您可以通过设置一些参数并运行ResourceManager守护程序和NodeManager守护程序，以伪分布式模式在YARN上运行MapReduce作业。</p><p>以下说明假定已执行<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9TaW5nbGVDbHVzdGVyLmh0bWwjRXhlY3V0aW9u" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Execution">上述指令的<i class="fa fa-external-link"></i></span> 1.~4步骤。</p><ol><li><p>配置参数如下:<code>etc/hadoop/mapred-site.xml</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><code>etc/hadoop/yarn-site.xml</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>启动ResourceManager守护程序和NodeManager守护程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>浏览ResourceManager的Web界面; 默认情况下，它可用于：</p><ul><li>ResourceManager - <code>http：// localhost：8088 /</code></li></ul></li><li><p>运行MapReduce作业。</p></li><li><p>完成后，停止后台进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure></li></ol><h2 id="全分布式操作"><a href="#全分布式操作" class="headerlink" title="全分布式操作"></a>全分布式操作</h2><p>有关设置完全分布式，非平凡群集的信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9DbHVzdGVyU2V0dXAuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html">群集设置<i class="fa fa-external-link"></i></span>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h2&gt;&lt;p&gt;本文档介绍如何设置和配置单节点Hadoop安装，以便您可以使用Hadoop MapReduce和Hadoop分布式文件系统（HDFS）快速执
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop命令指南</title>
    <link href="https://tangguangen.com/Hadoop%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97/"/>
    <id>https://tangguangen.com/Hadoop命令指南/</id>
    <published>2018-11-22T11:28:54.000Z</published>
    <updated>2018-11-26T12:08:23.645Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>所有hadoop命令都由<code>bin / hadoop</code>脚本调用。不带任何参数运行hadoop脚本会打印所有命令的描述。</p><p>用法: <code>hadoop [--config confdir] [--loglevel loglevel] [COMMAND] [GENERIC_OPTIONS] [COMMAND_OPTIONS]</code></p><table><thead><tr><th>FIELD</th><th>Description</th></tr></thead><tbody><tr><td><code>--config confdir</code></td><td>Overwrites the default Configuration directory. Default is <code>${HADOOP_HOME}/conf</code>.</td></tr><tr><td><code>--loglevel loglevel</code></td><td>Overwrites the log level. Valid log levels are FATAL, ERROR, WARN, INFO, DEBUG, and TRACE. Default is INFO.</td></tr><tr><td>GENERIC_OPTIONS</td><td>The common set of options supported by multiple commands.</td></tr><tr><td>COMMAND_OPTIONS</td><td>Various commands with their options are described in this documention for the Hadoop common sub-project. HDFS and YARN are covered in other documents.</td></tr></tbody></table><h3 id="通用选项"><a href="#通用选项" class="headerlink" title="通用选项"></a>通用选项</h3><p>许多子命令都支持一组通用的配置选项来改变它们的行为：</p><table><thead><tr><th>GENERIC_OPTION</th><th>Description</th></tr></thead><tbody><tr><td><code>-archives &lt;comma separated list of archives&gt;</code></td><td>Specify comma separated archives to be unarchived on the compute machines. Applies only to job.</td></tr><tr><td><code>-conf &lt;configuration file&gt;</code></td><td>Specify an application configuration file.</td></tr><tr><td><code>-D &lt;property&gt;=&lt;value&gt;</code></td><td>Use value for given property.</td></tr><tr><td><code>-files &lt;comma separated list of files&gt;</code></td><td>Specify comma separated files to be copied to the map reduce cluster. Applies only to job.</td></tr><tr><td><code>-fs &lt;file:///&gt; or &lt;hdfs://namenode:port&gt;</code></td><td>Specify default filesystem URL to use. Overrides ‘fs.defaultFS’ property from configurations.</td></tr><tr><td><code>-jt &lt;local&gt; or &lt;resourcemanager:port&gt;</code></td><td>Specify a ResourceManager. Applies only to job.</td></tr><tr><td><code>-libjars &lt;comma seperated list of jars&gt;</code></td><td>Specify comma separated jar files to include in the classpath. Applies only to job.</td></tr></tbody></table><h1 id="Hadoop常用命令"><a href="#Hadoop常用命令" class="headerlink" title="Hadoop常用命令"></a>Hadoop常用命令</h1><p>所有这些命令都是从<code>hadoop</code> shell命令执行的。它们已分解为<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9Db21tYW5kc01hbnVhbC5odG1sI1VzZXJfQ29tbWFuZHM=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CommandsManual.html#User_Commands"><strong>用户命令</strong><i class="fa fa-external-link"></i></span>和<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9Db21tYW5kc01hbnVhbC5odG1sI0FkbWluaXN0cmF0aW9uX0NvbW1hbmRz" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CommandsManual.html#Administration_Commands"><strong>管理命令</strong><i class="fa fa-external-link"></i></span>。</p><h2 id="用户命令"><a href="#用户命令" class="headerlink" title="用户命令"></a>用户命令</h2><p>对hadoop集群的用户有用的命令。</p><h3 id="档案"><a href="#档案" class="headerlink" title="档案"></a><code>档案</code></h3><p>创建一个hadoop存档。有关更多信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1hcmNoaXZlcy9IYWRvb3BBcmNoaXZlcy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-archives/HadoopArchives.html">Hadoop Archives Guide<i class="fa fa-external-link"></i></span>。</p><h3 id="checknative"><a href="#checknative" class="headerlink" title="checknative"></a><code>checknative</code></h3><p>用法：<code>hadoop checknative [-a] [-h]</code></p><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td><code>-a</code></td><td>Check all libraries are available.</td></tr><tr><td><code>-h</code></td><td>print help</td></tr></tbody></table><p>此命令检查Hadoop本机代码的可用性。有关更多信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9OYXRpdmVMaWJyYXJpZXMuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libaries<i class="fa fa-external-link"></i></span>。默认情况下，此命令仅检查libhadoop的可用性。</p><h3 id="classpath"><a href="#classpath" class="headerlink" title="classpath"></a><code>classpath</code></h3><p>Usage: <code>hadoop classpath [--glob |--jar &lt;path&gt; |-h |--help]</code></p><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td><code>--glob</code></td><td>expand wildcards</td></tr><tr><td><code>--jar</code> <em>path</em></td><td>write classpath as manifest in jar named <em>path</em></td></tr><tr><td><code>-h</code>, <code>--help</code></td><td>print help</td></tr></tbody></table><p>打印获取Hadoop jar和所需库所需的类路径。如果不带参数调用，则打印由命令脚本设置的类路径，该脚本可能在类路径条目中包含通配符。其他选项在通配符扩展后打印类路径，或将类路径写入jar文件的清单中。后者在无法使用通配符且扩展类路径超过支持的最大命令行长度的环境中非常有用。</p><h3 id="凭据"><a href="#凭据" class="headerlink" title="凭据"></a><code>凭据</code></h3><p>用法: <code>hadoop credential &lt;subcommand&gt; [options]</code></p><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td>create <em>alias</em> [-provider <em>provider-path</em>] [-strict] [-value <em>credential-value</em>]</td><td>提示用户将凭据存储为给定别名。所述<em>hadoop.security.credential.provider.path</em>芯site.xml文件内将被使用，除非一个<code>-provider</code>被指示。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。使用<code>-value</code>标志提供凭据值（也就是别名密码）而不是提示</td></tr><tr><td>delete <em>alias</em> [-provider <em>provider-path</em>] [-strict] [-f]</td><td>使用提供的别名删除凭据。所述<em>hadoop.security.credential.provider.path</em>芯site.xml文件内将被使用，除非一个<code>-provider</code>被指示。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。除非指定了<code>-f</code>，否则该命令会要求确认</td></tr><tr><td>list [-provider <em>provider-path</em>] [-strict]</td><td>列出所有的凭证的别名<em>hadoop.security.credential.provider.path</em>芯site.xml文件内将被使用，除非一个<code>-provider</code>被指示。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。</td></tr></tbody></table><p>用于管理凭据提供程序中的凭据，密码和机密的命令。</p><p>Hadoop中的CredentialProvider API允许分离应用程序以及它们如何存储所需的密码/秘密。为了指示特定的提供程序类型和位置，用户必须在core-site.xml中提供<em>hadoop.security.credential.provider.path</em>配置元素，或者对以下每个命令使用命令行选项<code>-provider</code>。此提供程序路径是以逗号分隔的URL列表，用于指示应查阅的提供程序列表的类型和位置。例如，以下路径：<code>user：///，jceks：//file/tmp/test.jceks,jceks：//hdfs@nn1.example.com/my/path/test.jceks</code></p><p>表示应通过用户提供程序查询当前用户的凭证文件，位于<code>/tmp/test.jceks</code>的本地文件是Java密钥库提供程序，该文件位于HDFS中的<code>nn1.example.com/my/path/ test.jceks</code>也是Java Keystore Provider的商店。</p><p>当使用凭证命令时，它通常用于向特定凭证存储提供商提供密码或秘密。为了明确指出要使用哪个提供者存储，应该使用<code>-provider</code>选项。否则，给定多个提供者的路径，将使用第一个非瞬态提供者。这可能是也可能不是你想要的那个。</p><p>提供商经常要求提供密码或其他秘密。如果提供程序需要密码而无法找到密码，则它将使用默认密码并发出警告消息，指出正在使用默认密码。如果提供了<code>-strict</code>标志，则警告消息将成为错误消息，并且该命令会立即返回错误状态。</p><p>示例：<code>hadoop凭证列表-provider jceks：//file/tmp/test.jceks</code></p><h3 id="distcp"><a href="#distcp" class="headerlink" title="distcp"></a><code>distcp</code></h3><p>递归复制文件或目录。有关详细信息，请参阅<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1kaXN0Y3AvRGlzdENwLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-distcp/DistCp.html">Hadoop DistCp指南<i class="fa fa-external-link"></i></span>。</p><h3 id="fs"><a href="#fs" class="headerlink" title="fs"></a><code>fs</code></h3><p>“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9GaWxlU3lzdGVtU2hlbGwuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html">文件系统Shell指南”<i class="fa fa-external-link"></i></span>中介绍了此命令。当HDFS正在使用时，它是<code>hdfs dfs</code>的同义词。</p><h3 id="jar"><a href="#jar" class="headerlink" title="jar"></a><code>jar</code></h3><p>Usage: <code>hadoop jar &lt;jar&gt; [mainClass] args...</code></p><p>Runs a jar file.</p><p>Use <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC15YXJuL2hhZG9vcC15YXJuLXNpdGUvWWFybkNvbW1hbmRzLmh0bWwjamFy" title="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YarnCommands.html#jar"><code>yarn jar</code><i class="fa fa-external-link"></i></span> to launch YARN applications instead.</p><h3 id="key"><a href="#key" class="headerlink" title="key"></a><code>key</code></h3><p>Usage: <code>hadoop key &lt;subcommand&gt; [options]</code></p><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td>create <em>keyname</em> [-cipher <em>cipher</em>] [-size <em>size</em>] [-description <em>description</em>] [-attr <em>attribute=value</em>] [-provider <em>provider</em>] [-strict] [-help]</td><td>创建由指定的名称一个新的密钥<em>键名</em>由指定的提供者中的说法<code>-provider</code>说法。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。您可以使用<code>-cipher</code>参数指定密码。默认密码当前为“AES / CTR / NoPadding”。默认密钥大小为128.您可以使用<code>-size</code>参数指定请求的密钥长度。可以使用<code>-attr</code>参数指定任意属性=值样式属性。<code>-attr</code>可以多次指定，每个属性一次。</td></tr><tr><td>roll <em>keyname</em> [-provider <em>provider</em>] [-strict] [-help]</td><td>使用<code>-provider</code>参数为指示的提供程序中的指定键创建新版本。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。</td></tr><tr><td>delete <em>keyname</em> [-provider <em>provider</em>] [-strict] [-f] [-help]</td><td>从<code>-provider</code>指定的提供程序中删除<em>keyname</em>参数指定的所有密钥版本。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。除非指定了<code>-f，</code>否则该命令会要求用户确认。</td></tr><tr><td>list [-provider <em>provider</em>] [-strict] [-metadata] [-help]</td><td>显示在core-site.xml中配置或使用<code>-provider</code>参数指定的特定提供程序中包含的键名。该<code>-strict</code>标志将导致如果提供商使用默认密码的命令失败。<code>-metadata</code>显示元数据。</td></tr><tr><td>-help</td><td>打印此命令的用法</td></tr></tbody></table><p>通过KeyProvider管理密钥。有关KeyProviders的详细信息，请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvVHJhbnNwYXJlbnRFbmNyeXB0aW9uLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">透明加密指南”<i class="fa fa-external-link"></i></span>。</p><p>提供商经常要求提供密码或其他秘密。如果提供程序需要密码而无法找到密码，则它将使用默认密码并发出警告消息，指出正在使用默认密码。如果提供了<code>-strict</code>标志，则警告消息将成为错误消息，并且该命令会立即返回错误状态。</p><p>注意：某些KeyProviders（例如org.apache.hadoop.crypto.key.JavaKeyStoreProvider）不支持大写键名称。</p><p>注意：某些KeyProviders不直接执行密钥删除（例如，执行软删除，或延迟实际删除，以防止错误）。在这些情况下，删除后创建/删除具有相同名称的密钥时可能会遇到错误。有关详细信息，请查看基础KeyProvider。</p><h3 id="trace"><a href="#trace" class="headerlink" title="trace"></a><code>trace</code></h3><p>查看和修改Hadoop跟踪设置。请参阅“ <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWNvbW1vbi9UcmFjaW5nLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/Tracing.html">跟踪指南”<i class="fa fa-external-link"></i></span>。</p><h3 id="version"><a href="#version" class="headerlink" title="version"></a><code>version</code></h3><p>用法：<code>hadoop版本</code></p><p>打印版本。</p><h3 id="CLASSNAME"><a href="#CLASSNAME" class="headerlink" title="CLASSNAME"></a><code>CLASSNAME</code></h3><p>用法：<code>hadoop CLASSNAME</code></p><p>运行名为<code>CLASSNAME</code>的类。</p><h3 id="envvars"><a href="#envvars" class="headerlink" title="envvars"></a><code>envvars</code></h3><p>用法：<code>hadoop envvars</code></p><p>显示计算的Hadoop环境变量。</p><h2 id="管理命令"><a href="#管理命令" class="headerlink" title="管理命令"></a>管理命令</h2><p>对hadoop集群的管理员有用的命令。</p><h3 id="daemonlog"><a href="#daemonlog" class="headerlink" title="daemonlog"></a><code>daemonlog</code></h3><p>Usage:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop daemonlog -getlevel &lt;host:port&gt; &lt;classname&gt; [-protocol (http|https)]</span><br><span class="line">hdoop daemonlog -setlevel &lt;host:port&gt; &lt;classname&gt; &lt;level&gt; [-protocol (http|https)]</span><br></pre></td></tr></table></figure><table><thead><tr><th>COMMAND_OPTION</th><th>Description</th></tr></thead><tbody><tr><td><code>-getlevel</code> <em>host:port</em> <em>classname</em>[-protocol (http\</td><td>https)]</td><td>在<em>host：port</em>运行的守护程序中打印由限定<em>类名</em>标识的日志的日志级别。所述<code>-protocol</code>标志指定用于连接的协议。</td></tr><tr><td><code>-setlevel</code> <em>host:port</em> <em>classname**level</em> [-protocol (http\</td><td>https)]</td><td>设置在<em>host：port</em>运行的守护程序中由限定<em>类名</em>标识的日志的日志级别。所述<code>-protocol</code>标志指定用于连接的协议。</td></tr></tbody></table><p>获取/设置守护程序中由限定类名称标识的日志的日志级别。默认情况下，该命令发送HTTP请求，但可以使用参数<code>-protocol https</code>来覆盖此请求以发送HTTPS请求。</p><p>例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop daemonlog -setlevel 127.0.0.1:50070 org.apache.hadoop.hdfs.server.namenode.NameNode DEBUG</span><br><span class="line">$ bin/hadoop daemonlog -getlevel 127.0.0.1:50470 org.apache.hadoop.hdfs.server.namenode.NameNode DEBUG -protocol https</span><br></pre></td></tr></table></figure><p>请注意，该设置不是永久性的，并且会在重新启动守护程序时重置。此命令通过向守护程序的内部Jetty servlet发送HTTP / HTTPS请求来工作，因此它支持以下守护程序：</p><ul><li>HDFS<ul><li>名称节点</li><li>辅助名称节点</li><li>数据节点</li><li>期刊节点</li></ul></li><li>YARN<ul><li>资源经理</li><li>节点管理员</li><li>时间线服务器</li></ul></li></ul><p>但是，该命令不支持KMS服务器，因为它的Web界面基于Tomcat，后者不支持servlet。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;所有hadoop命令都由&lt;code&gt;bin / hadoop&lt;/code&gt;脚本调用。不带任何参数运行hadoop脚本会打印所有命令的描述。&lt;
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>CS22n笔记01：Introduction to NLP and Deep Learning</title>
    <link href="https://tangguangen.com/CS22n%E7%AC%94%E8%AE%B001-Introduction-to-NLP-and-Deep-Learning/"/>
    <id>https://tangguangen.com/CS22n笔记01-Introduction-to-NLP-and-Deep-Learning/</id>
    <published>2018-11-20T07:49:46.000Z</published>
    <updated>2018-11-28T09:07:49.185Z</updated>
    
    <content type="html"><![CDATA[<p>主要参考自<span class="exturl" data-url="aHR0cDovL3d3dy5oYW5rY3MuY29tL25scC9jczIyNG4taW50cm9kdWN0aW9uLXRvLW5scC1hbmQtZGVlcC1sZWFybmluZy5odG1s" title="http://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html">http://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html<i class="fa fa-external-link"></i></span></p><h2 id="什么是自然语言处理"><a href="#什么是自然语言处理" class="headerlink" title="什么是自然语言处理"></a>什么是自然语言处理</h2><p>自然语言处理是<strong>计算机科学</strong>、<strong>人工智能</strong>和<strong>语言学</strong>的交叉学科。虽然语言只是人工智能的一部分（人工智能还包括计算机视觉等），但它是非常独特的一部分。这个星球上有许多生物拥有超过人类的视觉系统，但只有人类才拥有这么高级的语言。</p><p>自然语言处理的目标是让计算机处理或说“<strong>理解</strong>”自然语言，以完成有意义的任务，比如订机票购物或QA等。<strong>完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。</strong></p><h3 id="自然语言处理涉及的几个层次"><a href="#自然语言处理涉及的几个层次" class="headerlink" title="自然语言处理涉及的几个层次"></a>自然语言处理涉及的几个层次</h3><p>作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词（事实上，跳过分词虽然理所当然地不能做句法分析，但字符级也可以直接做不少应用）。接下来是形态学，援引《统计自然语言处理》中的定义：</p><blockquote><p>形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科［Matthews,2000］。</p></blockquote><p>下面的是<strong>句法分析</strong>和<strong>语义分析</strong>，最后面的在中文中似乎翻译做“<strong>语篇处理</strong>”，需要根据上文语境理解下文。</p><p>这门课主要关注画圈的三个部分，其中中间的两个是重中之重，虽然深度学习在语音识别上的发力最大。</p><h3 id="NLP应用"><a href="#NLP应用" class="headerlink" title="NLP应用"></a>NLP应用</h3><p>NLP中有不同级别的任务，从<strong>语音处理</strong>（speech processing）到<strong>语义解释</strong>（semantic interpretation）和<strong>语篇处理</strong>（ discourse processing）。 NLP的目标是设让计算机“理解”自然语言以执行某些任务的算法。 示例任务从简单到复杂有：</p><p><strong>Easy</strong></p><ul><li>Spell Checking</li><li>Keyword Search</li><li>Finding Synonyms</li></ul><p><strong>Medium</strong></p><ul><li>Parsing information from websites, documents, etc.</li></ul><p><strong>Hard</strong></p><ul><li>Machine Translation (e.g. Translate Chinese text to English)</li><li>Semantic Analysis (What is the meaning of query statement?)</li><li>Coreference (e.g. What does “he” or “it” refer to given a document?)</li><li>Question Answering (e.g. Answering Jeopardy questions).</li></ul><p>在工业界从搜索到广告投放、自动\辅助翻译、情感舆情分析、语音识别、聊天机器人\管家等等五花八门。</p><h3 id="人类语言的特殊之处"><a href="#人类语言的特殊之处" class="headerlink" title="人类语言的特殊之处"></a>人类语言的特殊之处</h3><p>人类（自然）语言有什么特别之处？ 人类语言是专门用于传达有意义的信息的，这种传输连小孩子都能很快学会（amazingly!）。人类语言是<strong>离散</strong>的、<strong>明确</strong>的<strong>符号系统</strong>。但又允许出现各种变种，比如颜文字，随意的错误拼写“I loooove it”。这种自由性可能是因为语言的可靠性。所以说语言文字绝对不是形式逻辑或传统AI的产物。</p><p>语言符号有多种形式（声音、手势、书写），在这些不同的形式中，其意义保持不变：</p><p>虽然人类语言是明确的符号系统，但符号传输到大脑的过程是通过连续的声学光学信号，大脑编码似乎是连续的激活值上的模式。另外巨大的词表也导致<strong>数据稀疏</strong>，不利于机器学习。这构成一种动机，是不是应该用<strong>连续的信号</strong>而不是离散的符号去处理语言。</p><h2 id="什么是深度学习"><a href="#什么是深度学习" class="headerlink" title="什么是深度学习"></a>什么是深度学习</h2><p>深度学习是机器学习的一个子集。传统机器学习中，人类需要对专业问题理解非常透彻，才能手工设计特征。比如地名和机构名识别的特征模板：</p><p>然后把特征交给某个机器学习算法，比如线性分类器。机器为这些特征调整找到合适的权值，将误差优化到最小。</p><p>下面这张图很好地展示了这个过程中的比例：</p><p>而深度学习是表示学习的一部分，用来学习原始输入的多层特征表示，输入的元数据可能是声音、字符、单词……</p><h3 id="“深度学习”的历史"><a href="#“深度学习”的历史" class="headerlink" title="“深度学习”的历史"></a>“深度学习”的历史</h3><p>虽然这个术语大部分时候指代利用各种各样<strong>多层的神经网络</strong>进行<strong>表示学习</strong>，有时候也有一些<strong>概率图</strong>模型参与。统计学家会说，哦，不过是一些<strong>逻辑斯谛回归单元</strong>的堆砌而已。也许的确如此，但这还是以偏概全的说法（电子计算机还是一堆半导体的堆砌呢，大脑还是一堆神经元的堆砌呢）。这门课不会回顾历史（像Hinton老爷子那样博古通今），而只会专注当前在NLP领域大放异彩的方法。</p><h3 id="为什么需要研究深度学习"><a href="#为什么需要研究深度学习" class="headerlink" title="为什么需要研究深度学习"></a>为什么需要研究深度学习</h3><ul><li>手工特征耗时耗力，还不易拓展</li><li>自动特征学习快，方便拓展</li><li>深度学习提供了一种通用的学习框架，可用来表示世界、视觉和语言学信息</li><li>深度学习既可以无监督学习，也可以监督学习</li></ul><p>深度学习可追溯到八九十年代，但在2010年左右才崛起（最先是语音与图像，后来才是NLP），那之前为什么没有呢？</p><p>与Hinton介绍的一样，无非是以前<strong>数据量不够</strong>，<strong>计算力太弱</strong>。当然，最近也的确有许多新模型，新算法。</p><blockquote><ul><li><p>Large amounts of training data favor deep learning</p></li><li><p>Faster machines and multicore CPU/GPUs favor Deep Learning</p></li><li>New models, algorithms, ideas<br>• Better, more flexible learning of intermediate representations<br>• Effective end-to-end joint system learning<br>• Effective learning methods for using contexts and transferring between tasks</li></ul></blockquote><h3 id="语音识别中的深度学习"><a href="#语音识别中的深度学习" class="headerlink" title="语音识别中的深度学习"></a>语音识别中的深度学习</h3><p>深度学习上突破性的研究发生在语音识别领域，来自Hinton老爷子的学生，具体参考：<span class="exturl" data-url="aHR0cDovL3d3dy5oYW5rY3MuY29tL21sL2hpbnRvbi1kZWVwLW5ldXJhbC1uZXRzLXdpdGgtZ2VuZXJhdGl2ZS1wcmUtdHJhaW5pbmcuaHRtbCNoMy0xMQ==" title="http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html#h3-11">http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html#h3-11<i class="fa fa-external-link"></i></span></p><h3 id="计算机视觉中的深度学习"><a href="#计算机视觉中的深度学习" class="headerlink" title="计算机视觉中的深度学习"></a>计算机视觉中的深度学习</h3><p>大多数深度学习的研究都集中在计算机视觉（至少到两年前）</p><p>突破性进展还是来自Hinton的学生。</p><blockquote><p>ImageNet Classification with Deep Convolutional Neural Networks by Krizhevsky, Sutskever, &amp; Hinton, 2012, U. Toronto.</p></blockquote><h2 id="课程相关"><a href="#课程相关" class="headerlink" title="课程相关"></a>课程相关</h2><p>有4次编程练习，会用到TensorFlow。</p><h2 id="为什么NLP难"><a href="#为什么NLP难" class="headerlink" title="为什么NLP难"></a>为什么NLP难</h2><p>人类语言是充满歧义的，不像编程语言那样明确。编程语言中有各种变量名，但人类语言中只有少数几个代词可以用，你得思考到底指代的是谁……</p><p>人类语言的解读依赖于现实世界、常识以及上下文。由于说话速度书写速度阅读速度的限制，人类语言非常简练，省略了大量背景知识。</p><p>接下来是几个英文的歧义例子，对native speaker而言很有趣。为了完整性只看一个：</p><blockquote><p>The Pope’s baby steps on gays</p></blockquote><p>主要歧义发生在baby上面，可以理解为“教皇的孩子踩了基佬”，也可以理解为“教皇在同性恋问题上裹足不前”。</p><p>旧版CS224d里面还有个更直观的例子，推特上关于电影明星“海瑟薇”的评论影响了保险公司哈撒韦的股价，因为两者拼写是一样的。</p><p>说明某些“舆情系统”没做好命名实体识别。</p><h2 id="Deep-NLP-Deep-Learning-NLP"><a href="#Deep-NLP-Deep-Learning-NLP" class="headerlink" title="Deep NLP = Deep Learning + NLP"></a>Deep NLP = Deep Learning + NLP</h2><p>将<strong>自然语言处理</strong>的思想与<strong>表示学习</strong>结合起来，用<strong>深度学习</strong>的手法解决NLP目标。这提高了许多方面的效果：</p><ul><li>层次：语音、词汇、语法、语义</li><li>工具：词性标注、命名实体识别、句法\语义分析</li><li>应用：机器翻译、情感分析、客服系统、问答系统</li></ul><h3 id="Word-Vectors"><a href="#Word-Vectors" class="headerlink" title="Word Vectors"></a>Word Vectors</h3><p>所有NLP任务的第一个也是最重要的共同点是我们如何将<strong>单词</strong>表示为<strong>模型的输入</strong>。 为了在大多数NLP任务上表现良好，我们首先需要有一些相似性和单词之间差异的概念。 使用词向量，我们可以很容易地在向量本身中编码这种能力（如Jaccard，Cosine，Euclidean等距离测量方法）。</p><h3 id="NLP表示层次：形态级别"><a href="#NLP表示层次：形态级别" class="headerlink" title="NLP表示层次：形态级别"></a>NLP表示层次：形态级别</h3><p>传统方法在形态级别的表示是词素：</p><p>深度学习中把词素也作为向量，多个词素向量构成相同纬度语义更丰富的词向量。</p><h3 id="NLP工具：句法分析"><a href="#NLP工具：句法分析" class="headerlink" title="NLP工具：句法分析"></a>NLP工具：句法分析</h3><blockquote><p>我在<span class="exturl" data-url="aHR0cDovL3d3dy5oYW5rY3MuY29tL25scC9wYXJzaW5nL25ldXJhbC1uZXR3b3JrLWJhc2VkLWRlcGVuZGVuY3ktcGFyc2VyLmh0bWw=" title="http://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html">《基于神经网络的高性能依存句法分析器》<i class="fa fa-external-link"></i></span>中分析并<span class="exturl" data-url="aHR0cDovL2hhbmxwLmhhbmtjcy5jb20vP3NlbnRlbmNlPSVFNSVCRSU5MCVFNSU4NSU4OCVFNyU5NCU5RiVFOCVCRiU5OCVFNSU4NSVCNyVFNCVCRCU5MyVFNSVCOCVBRSVFNSU4QSVBOSVFNCVCQiU5NiVFNyVBMSVBRSVFNSVBRSU5QSVFNCVCQSU4NiVFNiU4QSU4QSVFNyU5NCVCQiVFOSU5QiU4NCVFOSVCOSVCMCVFMyU4MCU4MSVFNiU5RCVCRSVFOSVCQyVBMCVFNSU5MiU4QyVFOSVCQSVCQiVFOSU5QiU4MCVFNCVCRCU5QyVFNCVCOCVCQSVFNCVCOCVCQiVFNiU5NCVCQiVFNyU5QiVBRSVFNiVBMCU4NyVFMyU4MCU4Mg==" title="http://hanlp.hankcs.com/?sentence=%E5%BE%90%E5%85%88%E7%94%9F%E8%BF%98%E5%85%B7%E4%BD%93%E5%B8%AE%E5%8A%A9%E4%BB%96%E7%A1%AE%E5%AE%9A%E4%BA%86%E6%8A%8A%E7%94%BB%E9%9B%84%E9%B9%B0%E3%80%81%E6%9D%BE%E9%BC%A0%E5%92%8C%E9%BA%BB%E9%9B%80%E4%BD%9C%E4%B8%BA%E4%B8%BB%E6%94%BB%E7%9B%AE%E6%A0%87%E3%80%82">移植的LTP句法分析器<i class="fa fa-external-link"></i></span>，参考的就是这里介绍的Danqi Chen的<span class="exturl" data-url="aHR0cDovL3d3dy5oYW5rY3MuY29tL3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDE1LzExL0ElMjBGYXN0JTIwYW5kJTIwQWNjdXJhdGUlMjBEZXBlbmRlbmN5JTIwUGFyc2VyJTIwdXNpbmclMjBOZXVyYWwlMjBOZXR3b3Jrcy5wZGY=" title="http://www.hankcs.com/wp-content/uploads/2015/11/A%20Fast%20and%20Accurate%20Dependency%20Parser%20using%20Neural%20Networks.pdf">A Fast and Accurate Dependency Parser using Neural Networks.pdf<i class="fa fa-external-link"></i></span>。原来她是这门课的TA：</p></blockquote><h3 id="NLP语义层面的表示"><a href="#NLP语义层面的表示" class="headerlink" title="NLP语义层面的表示"></a>NLP语义层面的表示</h3><p>传统方法是手写大量的规则函数，叫做Lambda calculus：</p><p>• Carefully engineered functions<br>• Take as inputs specific other functions<br>• No notion of similarity or fuzziness of language</p><p>在深度学习中，每个句子、短语和逻辑表述都是向量。神经网络负责它们的合并。</p><h3 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h3><p>传统方法是请一两百个工人，手工搜集“情感极性词典”在词袋模型上做分类器。</p><p>深度学习复用了RNN来解决这个问题，它可以识别“反话”的情感极性：</p><p>注意这只是为了方便理解的示意图，并不是RNN的工作流程。私以为这张图放在这里不合适，可能会误导一部分人，以为神经网络就是这样的基于规则的“决策树”模型。</p><h3 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h3><p>传统方法是手工编写大量的逻辑规则，比如正则表达式之类：</p><p>深度学习依然使用了类似的学习框架，把事实储存在向量里：</p><h3 id="客服系统"><a href="#客服系统" class="headerlink" title="客服系统"></a>客服系统</h3><p>最著名的例子得数GMail的自动回复：</p><p>这是<strong>Neural Language Models</strong>的又一次成功应用，Neural Language Models是基于<strong>RNN</strong>的：</p><h3 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h3><p>传统方法在许多层级上做了尝试，词语、语法、语义之类。这类方法试图找到一种世界通用的“国际语”（Interlingua）来作为原文和译文的桥梁。</p><p>而<strong>Neural Machine Translation</strong>将原文映射为<strong>向量</strong>，由<strong>向量构建译文</strong>。也许可以说Neural Machine Translation的“国际语”是向量。</p><h3 id="结论：所有层级的表示都是向量"><a href="#结论：所有层级的表示都是向量" class="headerlink" title="结论：所有层级的表示都是向量"></a>结论：所有层级的表示都是向量</h3><p>这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高阶的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。</p><p>下面两次课会详细地讲解向量表示，希望能带来新的体会。</p><blockquote><p>这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高阶的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。</p><p>旧版视频中Socher还顺便广告了一下他的创业公司MetaMind（已被收购，人生赢家）：</p><p>这个demo让我非常惊讶，因为普通NLP演示页面都是让人手工选择要执行的任务的。而这个demo竟然支持用一句话表示自己要执行的意图。不光可以执行情感分析、句法分析之类的常规任务，还可以输入一段话做推理任务。更让我惊讶的是，据说后台所有任务用的都是同一种模型，真乃神机也。据说这种模型是<span class="exturl" data-url="aHR0cHM6Ly9tZXRhbWluZC5pby9yZXNlYXJjaC9uZXctZGVlcC1sZWFybmluZy1tb2RlbC11bmRlcnN0YW5kcy1hbmQtYW5zd2Vycy1xdWVzdGlvbnM=" title="https://metamind.io/research/new-deep-learning-model-understands-and-answers-questions">Dynamic Memory Network<i class="fa fa-external-link"></i></span>。另外，他们又发了篇<span class="exturl" data-url="aHR0cHM6Ly9tZXRhbWluZC5pby9yZXNlYXJjaC9tdWx0aXBsZS1kaWZmZXJlbnQtbmF0dXJhbC1sYW5ndWFnZS1wcm9jZXNzaW5nLXRhc2tzLWluLWEtc2luZ2xlLWRlZXAtbW9kZWw=" title="https://metamind.io/research/multiple-different-natural-language-processing-tasks-in-a-single-deep-model">A Joint Many-Task Model:Growing a Neural Network for Multiple NLP Tasks<i class="fa fa-external-link"></i></span>，不知道两者有什么联系没有。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;主要参考自&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cDovL3d3dy5oYW5rY3MuY29tL25scC9jczIyNG4taW50cm9kdWN0aW9uLXRvLW5scC1hbmQtZGVlcC1sZWFybmluZy5odG1s&quot;
      
    
    </summary>
    
      <category term="NLP" scheme="https://tangguangen.com/categories/NLP/"/>
    
    
      <category term="CS224n" scheme="https://tangguangen.com/tags/CS224n/"/>
    
      <category term="NLP" scheme="https://tangguangen.com/tags/NLP/"/>
    
  </entry>
  
</feed>
