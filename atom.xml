<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>逍遥&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tangguangen.com/"/>
  <updated>2018-12-05T03:22:37.880Z</updated>
  <id>https://tangguangen.com/</id>
  
  <author>
    <name>逍遥</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深入理解Hadoop之HDFS架构</title>
    <link href="https://tangguangen.com/hdfs-architecture/"/>
    <id>https://tangguangen.com/hdfs-architecture/</id>
    <published>2018-12-04T13:43:48.000Z</published>
    <updated>2018-12-05T03:22:37.880Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop分布式文件系统（HDFS）是一种分布式文件系统。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的差异是值得我们注意的：</p><ol><li>HDFS具有<strong>高度容错</strong>能力，旨在部署在<strong>低成本</strong>硬件上。(高容错)</li><li>HDFS提供对数据的<strong>高吞吐量</strong>访问，适用于具有<strong>海量数据集</strong>的应用程序。（高吞吐量）</li><li>HDFS放宽了一些POSIX要求，以实现对文件系统数据的<strong>流式访问</strong>。（流式访问）</li></ol><p>HDFS最初是作为Apache Nutch网络搜索引擎项目的基础设施而构建的。HDFS是Apache Hadoop Core项目的一部分。项目URL是<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnLw==" title="http://hadoop.apache.org/">http://hadoop.apache.org/<i class="fa fa-external-link"></i></span></p><a id="more"></a><h2 id="目标和假设"><a href="#目标和假设" class="headerlink" title="目标和假设"></a>目标和假设</h2><p><strong>硬件故障检测</strong>：<strong>硬件故障是常态而非例外。</strong>Hadoop通常部署在低成本的硬件上，并且通常包含成百上千的服务器，每个服务器都存储文件系统数据的一部分。由于存在大量的组件，并且每个组件都具有不可忽略（non-trivial ）的故障概率，这意味着HDFS的某些组件始终都不起作用。因此，<strong>故障检测</strong>并<strong>快速恢复</strong>是HDFS的<strong>核心</strong>架构目标。</p><p><strong>流式访问</strong>：HDFS更适合<strong>批处理</strong>而不是交互式使用，更加注重数据访问的<strong>高吞吐量</strong>而不是数据访问的低延迟。在HDFS上运行的应用程序需要对其数据集进行<strong>流式访问</strong>。</p><p><strong>海量数据集</strong>：运行在HDFS上的应用程序具有大型数据集，HDFS中的一个典型文件的大小是g到tb，因此，HDFS被调优为支持大文件。它应该提供高聚合数据带宽，并可扩展到单个集群中的数百个节点。它应该在一个实例中支持数千万个文件。</p><p><strong>一致性模型</strong>：HDFS应用程序需要一个一次写入多次读取的文件访问模型。文件一旦创建、写入和关闭，除了追加和截断操作外，无需要更改。支持将内容追加到文件末尾，但无法在任意点更新。该假设简化了数据一致性问题并实现了高吞吐量数据访问。MapReduce应用程序或Web爬虫应用程序完全适合此模型。</p><p><strong>移动计算比移动数据便宜</strong>：应用程序请求的计算如果在其操作的数据附近执行，效率会高得多。当数据集的大小很大时尤其如此。这可以最大限度地减少网络拥塞并提高系统的整体吞吐量。<strong>因此更好的做法是将计算迁移到更靠近数据所在的位置</strong>，而不是将数据移动到运行应用程序的位置。HDFS为应用程序提供了一些接口，使它们自己更接近数据所在的位置。</p><p><strong>跨平台和可移植</strong>：Hadoop使用Java语言开发，使得Hadoop具有良好的跨平台性。</p><h2 id="NameNode和DataNodes"><a href="#NameNode和DataNodes" class="headerlink" title="NameNode和DataNodes"></a>NameNode和DataNodes</h2><p>HDFS具有<strong>主/从</strong>（ <strong>master/slave</strong>）架构。HDFS集群由一个<strong>NameNode</strong>和许多<strong>DataNode</strong>组成，NameNode是一个主服务器（master），管理文件系统名称空间并管理客端对数据的访问（<strong>NameNode在Hadoop集群中充当<u>管家</u>的角色</strong>）。此外集群中每个节点通常是一个DataNode，DataNode管理它们的节点上存储的数据。</p><p>HDFS公开文件系统名称空间，并允许用户数据存储在文件中。在内部，文件被分成一个或多个块（block），这些块存储在DataNode中。NameNode执行文件系统名称空间的相关操作，如打开、关闭和重命名文件和目录。它还确定了块到DataNode的映射（块存储到哪个DataNode中）。数据节点负责服务来自文件系统客户端的读写请求。数据节点还根据NameNode的指令执行块创建、删除和复制。</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-f4c1ebcbc6191438.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="HDFS架构"></p><p>集群中单一NameNode的结构大大简化了系统的架构。NameNode是所有HDFS元数据的仲裁者和管理者，这样，用户数据永远不会流过NameNode。</p><h2 id="文件系统名称空间-namespace"><a href="#文件系统名称空间-namespace" class="headerlink" title="文件系统名称空间(namespace)"></a>文件系统名称空间(namespace)</h2><p>HDFS支持传统的<strong>层次型</strong>文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名称空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。</p><p>NameNode负责维护文件系统的名称空间，任何对文件系统名称空间或属性的修改都将被NameNode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由NameNode保存的。</p><p>如果想深入了解HDFS文件系统名称空间可以查看这篇博文：<span class="exturl" data-url="aHR0cDovL2xlb3RzZTkwLmNvbS8yMDE1LzEwLzAxL0hERlMlRTYlOTYlODclRTQlQkIlQjYlRTclQjMlQkIlRTclQkIlOUYlRTUlOTElQkQlRTUlOTAlOEQlRTclQTklQkElRTklOTclQjQv" title="http://leotse90.com/2015/10/01/HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/">http://leotse90.com/…<i class="fa fa-external-link"></i></span></p><h2 id="数据复制"><a href="#数据复制" class="headerlink" title="数据复制"></a>数据复制</h2><p>HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。</p><p><strong>NameNode</strong>全权管理数据块的复制，它周期性地从集群中的每个<strong>DataNode</strong>接收<strong>心跳</strong>信号(Heartbeat )和<strong>块状态报告</strong>(Blockreport)。</p><ul><li>接收到心跳信号意味着该DataNode节点工作正常。</li><li>块状态报告包含了一个该Datanode上所有数据块的列表。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/15167924-50c8113a159e8a74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="数据块的复制"></p><h3 id="副本存放-最最开始的一步"><a href="#副本存放-最最开始的一步" class="headerlink" title="副本存放: 最最开始的一步"></a>副本存放: 最最开始的一步</h3><p>副本的存放是HDFS<strong>可靠性</strong>和<strong>性能</strong>的关键。<strong>优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。</strong>这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为<strong>机架感知</strong>(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。目前实现的副本存放策略只是在这个方向上的第一步。实现这个策略的短期目标是验证它在生产环境下的有效性，观察它的行为，为实现更先进的策略打下测试和研究的基础。</p><p>大型HDFS实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通讯需要经过交换机。在大多数情况下，<strong>同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。</strong></p><p>通过一个<span class="exturl" data-url="aHR0cHM6Ly9oYWRvb3AuYXBhY2hlLm9yZy9kb2NzL3IxLjAuNC9jbi9jbHVzdGVyX3NldHVwLmh0bWwjSGFkb29wJUU3JTlBJTg0JUU2JTlDJUJBJUU2JTlFJUI2JUU2JTg0JTlGJUU3JTlGJUE1" title="https://hadoop.apache.org/docs/r1.0.4/cn/cluster_setup.html#Hadoop%E7%9A%84%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5">机架感知<i class="fa fa-external-link"></i></span>的过程，NameNode可以确定每个DataNode所属的机架id。一个简单但没有优化的策略就是将副本存放在不同的机架上。这样可以有效防止当整个机架失效时数据的丢失，并且允许读数据的时候充分利用多个机架的带宽。这种策略设置可以将副本均匀分布在集群中，有利于当组件失效情况下的负载均衡。但是，因为这种策略的一个写操作需要传输数据块到多个机架，这增加了写的代价。</p><p>在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。这种策略减少了机架间的数据传输，这就提高了写操作的效率。机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。于此同时，因为数据块只放在两个（不是三个）不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。在这种策略下，副本并不是均匀分布在不同的机架上。三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能。</p><h3 id="副本选择"><a href="#副本选择" class="headerlink" title="副本选择"></a>副本选择</h3><p>为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。(<strong>就近原则</strong>)</p><h3 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h3><p>NameNode启动后会进入一个称为安全模式的特殊状态。处于安全模式的NameNode是不会进行数据块的复制的。NameNode从所有的 DataNode接收心跳信号和块状态报告。块状态报告包括了某个DataNode所有的数据块列表。每个数据块都有一个指定的最小副本数。当NameNode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是<strong>副本安全</strong>(safely replicated)的；在一定百分比（这个参数可配置）的数据块被NameNode检测确认是安全之后（加上一个额外的30秒等待时间），NameNode将退出安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他DataNode上。</p><h2 id="文件系统元数据的持久化"><a href="#文件系统元数据的持久化" class="headerlink" title="文件系统元数据的持久化"></a>文件系统元数据的持久化</h2><p>NameNode上保存着HDFS的DataNode空间。对于任何对文件系统元数据产生修改的操作，NameNode都会使用一种称为EditLog的事务日志记录下来。例如，在HDFS中创建一个文件，NameNode就会在Editlog中插入一条记录来表示；同样地，修改文件的副本系数也将往Editlog插入一条记录。NameNode在本地操作系统的文件系统中存储这个Editlog。整个文件系统的DataNode空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为FsImage的文件中，这个文件也是放在NameNode所在的本地文件系统上。</p><p>NameNode在内存中保存着整个文件系统的DataNode空间和文件数据块映射(Blockmap)的映像。这个关键的元数据结构设计得很紧凑，因而一个有4G内存的NameNode足够支撑大量的文件和目录。当NameNode启动时，它从硬盘中读取Editlog和FsImage，将所有Editlog中的事务作用在内存中的FsImage上，并将这个新版本的FsImage从内存中保存到本地磁盘上，然后删除旧的Editlog，因为这个旧的Editlog的事务都已经作用在FsImage上了。这个过程称为一个检查点(checkpoint)。在当前实现中，检查点只发生在NameNode启动时，在不久的将来将实现支持周期性的检查点。</p><p>Datanode将HDFS数据以文件的形式存储在本地的文件系统中，它并不知道有关HDFS文件的信息。它把每个HDFS数据块存储在本地文件系统的一个单独的文件中。Datanode并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目录中支持大量的文件。当一个Datanode启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有HDFS数据块的列表，然后作为报告发送到NameNode，这个报告就是块状态报告。</p><h2 id="通讯协议"><a href="#通讯协议" class="headerlink" title="通讯协议"></a>通讯协议</h2><p>所有的HDFS通讯协议都是建立在TCP/IP协议之上。客户端通过一个可配置的TCP端口连接到NameNode，通过ClientProtocol协议与NameNode交互。而Datanode使用DatanodeProtocol协议与NameNode交互。一个远程过程调用(RPC)模型被抽象出来封装ClientProtocol和Datanodeprotocol协议。在设计上，NameNode不会主动发起RPC，而是响应来自客户端或 Datanode 的RPC请求。</p><h2 id="健壮性"><a href="#健壮性" class="headerlink" title="健壮性"></a>健壮性</h2><p>HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：NameNode出错, Datanode出错和网络割裂(network partitions)。</p><h3 id="磁盘数据错误，心跳检测和重新复制"><a href="#磁盘数据错误，心跳检测和重新复制" class="headerlink" title="磁盘数据错误，心跳检测和重新复制"></a>磁盘数据错误，心跳检测和重新复制</h3><p>每个Datanode节点周期性地向NameNode发送心跳信号。网络割裂可能导致一部分Datanode跟NameNode失去联系。NameNode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号Datanode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机Datanode上的数据将不再有效。Datanode的宕机可能会引起一些数据块的副本系数低于指定值，NameNode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。在下列情况下，可能需要重新复制：某个Datanode节点失效，某个副本遭到损坏，Datanode上的硬盘错误，或者文件的副本系数增大。</p><h3 id="集群均衡"><a href="#集群均衡" class="headerlink" title="集群均衡"></a>集群均衡</h3><p>HDFS的架构支持数据均衡策略。如果某个Datanode节点上的空闲空间低于特定的临界点，按照均衡策略系统就会自动地将数据从这个Datanode移动到其他空闲的Datanode。当对某个文件的请求突然增加，那么也可能启动一个计划创建该文件新的副本，并且同时重新平衡集群中的其他数据。这些均衡策略目前还没有实现。</p><h3 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h3><p>从某个Datanode获取的数据块有可能是损坏的，损坏可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFSDataNode空间下。当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode获取该数据块的副本。</p><h3 id="元数据磁盘错误"><a href="#元数据磁盘错误" class="headerlink" title="元数据磁盘错误"></a>元数据磁盘错误</h3><p>FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS实例都将失效。因而，NameNode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低NameNode每秒处理的DataNode空间事务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当NameNode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。</p><p>增加故障恢复能力的另一个选择是使用多个NameNode <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSERGU0hpZ2hBdmFpbGFiaWxpdHlXaXRoTkZTLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">在NFS上<i class="fa fa-external-link"></i></span>使用<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSERGU0hpZ2hBdmFpbGFiaWxpdHlXaXRoTkZTLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">共享存储<i class="fa fa-external-link"></i></span>或使用<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSERGU0hpZ2hBdmFpbGFiaWxpdHlXaXRoUUpNLmh0bWw=" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">分布式编辑日志<i class="fa fa-external-link"></i></span>（称为Journal）来启用高可用性。后者是推荐的方法。</p><h3 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h3><p><span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc1NuYXBzaG90cy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">快照<i class="fa fa-external-link"></i></span>支持在特定时刻存储数据副本。快照功能的一种用途可以是将损坏的HDFS实例回滚到先前已知的良好时间点。</p><h2 id="数据组织"><a href="#数据组织" class="headerlink" title="数据组织"></a>数据组织</h2><h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h3><p>HDFS被设计成支持大文件，适用HDFS的是那些需要处理大规模的数据集的应用。这些应用都是只写入数据一次，但却读取一次或多次，并且读取速度应能满足流式读取的需要。HDFS支持文件的“一次写入多次读取”语义。一个典型的数据块大小是128MB。因而，HDFS中的文件总是按照128M被切分成不同的块，每个块尽可能地存储于不同的Datanode中。</p><h3 id="流水线复制"><a href="#流水线复制" class="headerlink" title="流水线复制"></a>流水线复制</h3><p>当客户端向HDFS文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副本系数设置为3，当本地临时文件累积到一个数据块的大小时，客户端会从NameNode获取一个Datanode列表用于存放副本。然后客户端开始向第一个Datanode传输数据，第一个Datanode一小部分一小部分(4 KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个Datanode节点。第二个Datanode也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个Datanode。最后，第三个Datanode接收数据并存储在本地。因此，Datanode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个Datanode复制到下一个。</p><h2 id="可访问性"><a href="#可访问性" class="headerlink" title="可访问性"></a>可访问性</h2><p>可以通过多种不同方式从应用程序访问HDFS。本地，HDFS 为应用程序提供了<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3MvY3VycmVudC9hcGkv" title="http://hadoop.apache.org/docs/current/api/">FileSystem Java API<i class="fa fa-external-link"></i></span>。一<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvTGliSGRmcy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/LibHdfs.html">本Java API的C语言包装<i class="fa fa-external-link"></i></span>和<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvV2ViSERGUy5odG1s" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html">REST API<i class="fa fa-external-link"></i></span>也是可用的。此外，还有HTTP浏览器，也可用于浏览HDFS实例的文件。通过使用<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2hhZG9vcC1wcm9qZWN0LWRpc3QvaGFkb29wLWhkZnMvSGRmc05mc0dhdGV3YXkuaHRtbA==" title="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS网关<i class="fa fa-external-link"></i></span>，HDFS可以作为客户端本地文件系统的一部分进行安装。</p><h3 id="FS-Shell"><a href="#FS-Shell" class="headerlink" title="FS Shell"></a>FS Shell</h3><p>HDFS以文件和目录的形式组织用户数据。它提供了一个命令行的接口(FS Shell)让用户与HDFS中的数据进行交互。命令的语法和用户熟悉的其他shell(例如 bash, csh)工具类似。下面是一些动作/命令的示例：</p><table><thead><tr><th>创建一个名为<code>/foodir</code>的目录</th><th><code>bin /hadoop dfs -mkdir /foodir</code></th></tr></thead><tbody><tr><td>删除名为<code>/foodir</code>的目录</td><td><code>bin /hadoop fs -rm -R /foodir</code></td></tr><tr><td>查看名为<code>/foodir/myfile.txt</code>的文件的内容</td><td><code>bin /hadoop dfs -cat /foodir/myfile.txt</code></td></tr></tbody></table><p>FS shell适用于需要脚本语言与存储数据交互的应用程序。</p><h3 id="DFSAdmin"><a href="#DFSAdmin" class="headerlink" title="DFSAdmin"></a>DFSAdmin</h3><p>典型的HDFS安装配置Web服务器以通过可配置的TCP端口公开HDFS命名空间。这允许用户使用Web浏览器导航HDFS命名空间并查看其文件的内容。</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-3842129a5a9d163b.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Web界面"></p><h2 id="存储空间回收"><a href="#存储空间回收" class="headerlink" title="存储空间回收"></a>存储空间回收</h2><h3 id="文件的删除和恢复"><a href="#文件的删除和恢复" class="headerlink" title="文件的删除和恢复"></a>文件的删除和恢复</h3><p>如果启用了回收站配置，当用户或应用程序删除某个文件时，这个文件并没有立刻从HDFS中删除。实际上，HDFS会将这个文件重命名转移到/trash目录(/user/<username>/.Trash)。只要文件还在/trash目录中，该文件就可以被迅速地恢复。文件在/trash中保存的时间是可配置的，当超过这个时间时，NameNode就会将该文件从DataNode空间中删除。删除文件会使得该文件相关的数据块被释放。注意，从用户删除文件到HDFS空闲空间的增加之间会有一定时间的延迟。</username></p><p>以下是一个示例，它将显示FS Shell如何从HDFS中删除文件。我们在目录delete下创建了2个文件（test1和test2）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -mkdir -p delete/test1</span><br><span class="line">$ hadoop fs -mkdir -p delete/test2</span><br><span class="line">$ hadoop fs -ls delete/</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2015-05-08 12:39 delete/test1</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2015-05-08 12:40 delete/test2</span><br></pre></td></tr></table></figure><p>我们将删除文件test1。下面的注释显示该文件已移至/trash目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -rm -r delete/test1</span><br><span class="line">Moved: hdfs://localhost:8020/user/hadoop/delete/test1 to trash at: hdfs://localhost:8020/user/hadoop/.Trash/Current</span><br></pre></td></tr></table></figure><p>现在我们将使用skipTrash选项删除该文件，该选项不会将文件发送到Trash。它将从HDFS中完全删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -rm -r -skipTrash delete/test2</span><br><span class="line">Deleted delete/test2</span><br></pre></td></tr></table></figure><p>我们现在可以看到Trash目录只包含文件test1。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls .Trash/Current/user/hadoop/delete/</span><br><span class="line">Found 1 items\</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2015-05-08 12:39 .Trash/Current/user/hadoop/delete/test1</span><br></pre></td></tr></table></figure><p>因此文件test1进入垃圾箱并永久删除文件test2。</p><h3 id="减少副本系数"><a href="#减少副本系数" class="headerlink" title="减少副本系数"></a>减少副本系数</h3><p>当一个文件的副本系数被减小后，NameNode会选择过剩的副本删除。下次心跳检测时会将该信息传递给Datanode。Datanode遂即移除相应的数据块，集群中的空闲空间加大。同样，在调用setReplication API结束和集群中空闲空间增加间会有一定的延迟。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>Hadoop <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3MvY3VycmVudC9hcGkv" title="http://hadoop.apache.org/docs/current/api/">JavaDoc API<i class="fa fa-external-link"></i></span></p><p>HDFS源代码：<span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL3ZlcnNpb25fY29udHJvbC5odG1s" title="http://hadoop.apache.org/version_control.html">http://hadoop.apache.org/…<i class="fa fa-external-link"></i></span></p><p>Hadoop Doc: <span class="exturl" data-url="aHR0cDovL2hhZG9vcC5hcGFjaGUub3JnL2RvY3Mvc3RhYmxlL2luZGV4Lmh0bWw=" title="http://hadoop.apache.org/docs/stable/index.html">http://hadoop.apache.org/docs/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop分布式文件系统（HDFS）是一种分布式文件系统。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的差异是值得我们注意的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HDFS具有&lt;strong&gt;高度容错&lt;/strong&gt;能力，旨在部署在&lt;strong&gt;低成本&lt;/strong&gt;硬件上。(高容错)&lt;/li&gt;
&lt;li&gt;HDFS提供对数据的&lt;strong&gt;高吞吐量&lt;/strong&gt;访问，适用于具有&lt;strong&gt;海量数据集&lt;/strong&gt;的应用程序。（高吞吐量）&lt;/li&gt;
&lt;li&gt;HDFS放宽了一些POSIX要求，以实现对文件系统数据的&lt;strong&gt;流式访问&lt;/strong&gt;。（流式访问）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HDFS最初是作为Apache Nutch网络搜索引擎项目的基础设施而构建的。HDFS是Apache Hadoop Core项目的一部分。项目URL是&lt;a href=&quot;http://hadoop.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://hadoop.apache.org/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/HDFS/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="https://tangguangen.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 190.颠倒二进制位</title>
    <link href="https://tangguangen.com/leetcode-190-revers-bits/"/>
    <id>https://tangguangen.com/leetcode-190-revers-bits/</id>
    <published>2018-12-04T12:57:31.000Z</published>
    <updated>2018-12-04T13:05:51.947Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>颠倒给定的 32 位无符号整数的二进制位。</p><p><strong>示例:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: 43261596</span><br><span class="line">&gt; 输出: 964176192</span><br><span class="line">&gt; 解释: 43261596 的二进制表示形式为 00000010100101000001111010011100 ，</span><br><span class="line">&gt;      返回 964176192，其二进制表示形式为 00111001011110000010100101000000 。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>进阶</strong>:<br>如果多次调用这个函数，你将如何优化你的算法？</p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>通过位运算<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU0JUJEJThEJUU2JTkzJThEJUU0JUJEJTlD" title="https://zh.wikipedia.org/wiki/%E4%BD%8D%E6%93%8D%E4%BD%9C">wiki<i class="fa fa-external-link"></i></span>，依次截取n的二进制位的低位，放入result的高位。这一题是简单题，需掌握位运算</p><h2 id="AC代码（Java）"><a href="#AC代码（Java）" class="headerlink" title="AC代码（Java）"></a>AC代码（Java）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="comment">// you need treat n as an unsigned value</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> ans=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">32</span>;i++)</span><br><span class="line">            ans|=((n&gt;&gt;&gt;i)&amp;<span class="number">1</span>)&lt;&lt;(<span class="number">31</span>-i);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;p&gt;颠倒给定的 32 位无符号整数的二进制位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例:&lt;/strong&gt;
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
      <category term="java" scheme="https://tangguangen.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 557. 反转字符串中的单词 III</title>
    <link href="https://tangguangen.com/leetcode-557-reverse-words/"/>
    <id>https://tangguangen.com/leetcode-557-reverse-words/</id>
    <published>2018-12-04T05:51:34.000Z</published>
    <updated>2018-12-04T06:02:59.507Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个字符串，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。</p><p><strong>示例 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: &quot;Let&apos;s take LeetCode contest&quot;</span><br><span class="line">&gt; 输出: &quot;s&apos;teL ekat edoCteeL tsetnoc&quot; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>注意：</strong>在字符串中，每个单词由单个空格分隔，并且字符串中不会有任何额外的空格。</p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>由于字符串中不会有任何额外的空格，根据空格来找单词，将每个单词依次进行字符串的反转即可，字符串的反转可以看我这篇博客：<a href="https://tangguangen.com/leetcode-344-reverse-string/">LeetCode反转字符串</a></p><h2 id="AC代码（Java）"><a href="#AC代码（Java）" class="headerlink" title="AC代码（Java）"></a>AC代码（Java）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">reverseWords</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span>[] cl = s.toCharArray();</span><br><span class="line">        <span class="keyword">int</span> start = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> nextSpace = s.indexOf(<span class="string">' '</span>,start);  <span class="comment">// 返回字符串s从索引start开始的第一个空格的索引，若没有，则返回-1</span></span><br><span class="line">        <span class="keyword">while</span>(nextSpace != -<span class="number">1</span>) &#123;</span><br><span class="line">            reverse(cl,start,nextSpace - <span class="number">1</span>);</span><br><span class="line">            start = nextSpace + <span class="number">1</span>;</span><br><span class="line">            nextSpace = s.indexOf(<span class="string">' '</span>,start);</span><br><span class="line">        &#125;</span><br><span class="line">        reverse(cl,start,cl.length - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String(cl);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reverse</span><span class="params">(<span class="keyword">char</span>[] cl,<span class="keyword">int</span> start,<span class="keyword">int</span> end)</span></span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(start &lt; end)&#123;</span><br><span class="line">            <span class="keyword">char</span> temp = cl[start];</span><br><span class="line">            cl[start] = cl[end];</span><br><span class="line">            cl[end] = temp;</span><br><span class="line">            start ++;</span><br><span class="line">            end --;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;p&gt;给定一个字符串，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle机器学习入门教程一</title>
    <link href="https://tangguangen.com/kaggle-learn-machine-learning-introduction/"/>
    <id>https://tangguangen.com/kaggle-learn-machine-learning-introduction/</id>
    <published>2018-12-03T10:50:30.000Z</published>
    <updated>2018-12-03T14:15:24.185Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型是如何工作的"><a href="#模型是如何工作的" class="headerlink" title="模型是如何工作的"></a>模型是如何工作的</h2><p>原文链接：<span class="exturl" data-url="aHR0cHM6Ly93d3cua2FnZ2xlLmNvbS9kYW5zYmVja2VyL2hvdy1tb2RlbHMtd29yaw==" title="https://www.kaggle.com/dansbecker/how-models-work">https://www.kaggle.com/…<i class="fa fa-external-link"></i></span></p><p>这门课程将从机器学习模型<strong>如何工作</strong>以及<strong>如何使用</strong>它们开始，如果您以前做过统计建模或机器学习，这可能会觉得很基础。别担心，我们很快就会建立强大的模型。</p><p>本课程将让您为以下场景构建模型:</p><p>（<strong>房价预测</strong>）你表弟在房地产投机上赚了数百万美元。由于你对数据科学的兴趣，他愿意和你成为商业伙伴。他提供钱，你提供模型来预测不同房子的价值。</p><p>你问你表弟他过去是如何预测房地产价值的。他说这只是直觉。但更多的问题表明，他已经从过去看到的房子中识别出了价格模式，并利用这些模式对他正在考虑的新房子做出了预测。</p><p>机器学习也是如此。我们将从一个叫做<strong>决策树</strong>的模型开始。用更漂亮的模型可以给出更准确的预测。但是决策树很容易理解，它们是数据科学中一些最佳模型的基本构建块。</p><p><strong>为了简单起见，我们将从最简单的决策树开始。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-f3ca1ac3b54ef275.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="简单决策树"></p><p>它<strong>只把房子分为两类</strong>。任何被考虑的房屋的预测价格是同一类别房屋的历史平均价格。</p><p>我们用数据来决定如何把房子分成两组，然后再确定每组的预测价格。从数据中获取模式的这一步称为模型<strong>拟合</strong>或<strong>训练</strong>。用于<strong>拟合模型</strong>的数据称为<strong>训练数据</strong>。</p><p>模型如何拟合(例如如何分割数据)的细节非常复杂，我们将在以后进行详细讲解。模型拟合后，您可以将其应用于新的数据，以<strong>预测</strong>其他房屋的价格。</p><h3 id="改进决策树"><a href="#改进决策树" class="headerlink" title="改进决策树"></a>改进决策树</h3><p>以下两种决策树中，哪一种更有可能通过拟合房地产训练数据而得到结果?</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-500e8335b0a2ba16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="决策树对比"></p><p>左边的决策树(Decision Tree 1)可能更有意义，因为它捕捉到了这样一个事实: 卧室较多的房子往往比卧室较少的房子售价更高。这种模式最大的缺点是它没有捕捉到影响房价的其他的大部分因素，如浴室数量、面积、位置等。</p><p>您可以使用具有更多“分支”的树捕获更多的因子。这些树被称为“深”树。一个决策树，也考虑每个房子的面积大小，可能是这样的:</p><p><img src="https://upload-images.jianshu.io/upload_images/15167924-1be930db63267c0c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="改进的决策树"></p><p>可以通过追踪决策树来预测房子的价格，每次决策都选择与房子特征相对应的路径。这所房子的预测价格在树的底部。在底部的点叫做<strong>叶子节点</strong>。</p><p>叶节点的分割和值将由数据决定，所以现在是您检查将要处理的数据的时候了。</p><h2 id="探索数据"><a href="#探索数据" class="headerlink" title="探索数据"></a>探索数据</h2><h3 id="使用pandas探索数据"><a href="#使用pandas探索数据" class="headerlink" title="使用pandas探索数据"></a>使用pandas探索数据</h3><p>任何机器学习项目的第一步都是熟悉数据。pandas库是科学家用来探索和操做数据的主要工具。大多数人在他们的代码中将pandas缩写为pd。我们用下面的命令导入pandas库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><p>pandas库最重要的部分是DataFrame。DataFrame保存您可能认为是表的数据类型。这类似于Excel中的工作表，或SQL数据库中的表。</p><p>pandas提供了强大的方法，可以处理您想要处理的大多数此类数据。</p><p>举个例子，我们来看看澳大利亚墨尔本的房价数据。在实践练习中，您将把相同的过程应用到一个新的数据集，该数据集包含衣阿华州的房价。</p><p>示例(墨尔本)数据位于文件路径<strong>../input/melbourne-housing-snapshot/melb_data.csv</strong></p><p>我们使用以下命令加载和浏览数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save filepath to variable for easier access</span></span><br><span class="line">melbourne_file_path = <span class="string">'../input/melbourne-housing-snapshot/melb_data.csv'</span></span><br><span class="line"><span class="comment"># read the data and store data in DataFrame titled melbourne_data</span></span><br><span class="line">melbourne_data = pd.read_csv(melbourne_file_path) </span><br><span class="line"><span class="comment"># print a summary of the data in Melbourne data</span></span><br><span class="line">melbourne_data.describe()</span><br></pre></td></tr></table></figure><table><thead><tr><th>Rooms</th><th>Price</th><th>Distance</th><th>Postcode</th><th>Bedroom2</th><th>Bathroom</th><th>Car</th><th>Landsize</th><th>BuildingArea</th><th>YearBuilt</th><th>Lattitude</th><th>Longtitude</th><th>Propertycount</th><th></th></tr></thead><tbody><tr><td>count</td><td>13580.000000</td><td>1.358000e+04</td><td>13580.000000</td><td>13580.000000</td><td>13580.000000</td><td>13580.000000</td><td>13518.000000</td><td>13580.000000</td><td>7130.000000</td><td>8205.000000</td><td>13580.000000</td><td>13580.000000</td><td>13580.000000</td></tr><tr><td>mean</td><td>2.937997</td><td>1.075684e+06</td><td>10.137776</td><td>3105.301915</td><td>2.914728</td><td>1.534242</td><td>1.610075</td><td>558.416127</td><td>151.967650</td><td>1964.684217</td><td>-37.809203</td><td>144.995216</td><td>7454.417378</td></tr><tr><td>std</td><td>0.955748</td><td>6.393107e+05</td><td>5.868725</td><td>90.676964</td><td>0.965921</td><td>0.691712</td><td>0.962634</td><td>3990.669241</td><td>541.014538</td><td>37.273762</td><td>0.079260</td><td>0.103916</td><td>4378.581772</td></tr><tr><td>min</td><td>1.000000</td><td>8.500000e+04</td><td>0.000000</td><td>3000.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>1196.000000</td><td>-38.182550</td><td>144.431810</td><td>249.000000</td></tr><tr><td>25%</td><td>2.000000</td><td>6.500000e+05</td><td>6.100000</td><td>3044.000000</td><td>2.000000</td><td>1.000000</td><td>1.000000</td><td>177.000000</td><td>93.000000</td><td>1940.000000</td><td>-37.856822</td><td>144.929600</td><td>4380.000000</td></tr><tr><td>50%</td><td>3.000000</td><td>9.030000e+05</td><td>9.200000</td><td>3084.000000</td><td>3.000000</td><td>1.000000</td><td>2.000000</td><td>440.000000</td><td>126.000000</td><td>1970.000000</td><td>-37.802355</td><td>145.000100</td><td>6555.000000</td></tr><tr><td>75%</td><td>3.000000</td><td>1.330000e+06</td><td>13.000000</td><td>3148.000000</td><td>3.000000</td><td>2.000000</td><td>2.000000</td><td>651.000000</td><td>174.000000</td><td>1999.000000</td><td>-37.756400</td><td>145.058305</td><td>10331.000000</td></tr><tr><td>max</td><td>10.000000</td><td>9.000000e+06</td><td>48.100000</td><td>3977.000000</td><td>20.000000</td><td>8.000000</td><td>10.000000</td><td>433014.000000</td><td>44515.000000</td><td>2018.000000</td><td>-37.408530</td><td>145.526350</td><td>21650.000000</td></tr></tbody></table><h3 id="解释Data-Description"><a href="#解释Data-Description" class="headerlink" title="解释Data Description"></a>解释Data Description</h3><p>结果为原始数据集中的每一列显示8个数字。第一个数字是count，它显示有多少行没有缺失值。</p><p>缺失值的原因有很多。例如，在测量1居室的房子时，不会收集第2居室的大小。</p><p>第二个值是均值<strong>mean</strong>，也就是平均值。在这种情况下，<strong>std</strong>是标准偏差，它度量数值的分布情况。</p><p>要解释最小值<strong>min</strong>、<strong>25%</strong>、<strong>50%</strong>、<strong>75%</strong>和最大值<strong>max</strong> ，请想像对每个列从最低值到最高值进行排序。第一个(最小的)值是最小值。如果您遍历列表的四分之一，您会发现一个值大于25%，小于75%。这就是25%的值。第50百分位和第75百分位的定义类似，最大值是最大的数字。</p><h2 id="练习：探索数据"><a href="#练习：探索数据" class="headerlink" title="练习：探索数据"></a>练习：探索数据</h2><p>kaggle链接：<span class="exturl" data-url="aHR0cHM6Ly93d3cua2FnZ2xlLmNvbS9rZXJuZWxzL2ZvcmsvMTI1ODk1NA==" title="https://www.kaggle.com/kernels/fork/1258954">https://www.kaggle.com/…<i class="fa fa-external-link"></i></span></p><p>这个练习将测试您读取数据文件和理解有关数据的统计信息的能力。</p><p>你的数据中最新的房子并不新鲜。对此有几个可能的解释:</p><ol><li>他们还没有在收集这些数据的地方建造新房子。</li><li>这些数据是很久以前收集的。数据发布后建造的房屋不会出现。</li></ol><p>如果原因是上面的解释#1，那么这会影响您对使用这些数据构建的模型的信任吗?如果这是第二个原因呢?</p><p>你如何深入研究这些数据，看看哪种解释更合理?</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;模型是如何工作的&quot;&gt;&lt;a href=&quot;#模型是如何工作的&quot; class=&quot;headerlink&quot; title=&quot;模型是如何工作的&quot;&gt;&lt;/a&gt;模型是如何工作的&lt;/h2&gt;&lt;p&gt;原文链接：&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly
      
    
    </summary>
    
      <category term="机器学习" scheme="https://tangguangen.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="kaggle" scheme="https://tangguangen.com/tags/kaggle/"/>
    
      <category term="机器学习" scheme="https://tangguangen.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：多节点集群</title>
    <link href="https://tangguangen.com/hadoop-multi-node-cluster/"/>
    <id>https://tangguangen.com/hadoop-multi-node-cluster/</id>
    <published>2018-12-01T12:45:48.000Z</published>
    <updated>2018-12-04T06:04:24.991Z</updated>
    
    <content type="html"><![CDATA[<p>本章介绍了Hadoop多节点集群在分布式环境中的设置。</p><p>由于无法演示整个集群，我们使用三个系统(一个主系统和两个从系统)解释Hadoop集群环境;以下是他们的IP地址。</p><ul><li>Hadoop Master: 192.168.1.15 (hadoop-master)</li><li>Hadoop Slave: 192.168.1.16 (hadoop-slave-1)</li><li>Hadoop Slave: 192.168.1.17 (hadoop-slave-2)</li></ul><p>按照下面给出的步骤设置Hadoop多节点集群。</p><h2 id="安装Java"><a href="#安装Java" class="headerlink" title="安装Java"></a>安装Java</h2><p>参考：<a href="https://tangguangen.com/hadoop-enviornment-setup">Hadoop安装与环境设置</a></p><h2 id="映射节点"><a href="#映射节点" class="headerlink" title="映射节点"></a>映射节点</h2><p>您必须在<strong>/etc/</strong>文件夹中编辑所有节点上的<strong>hosts</strong> 文件，指定每个系统的IP地址及其主机名。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vi /etc/hosts</span></span><br><span class="line">enter the following lines in the /etc/hosts file.</span><br><span class="line">192.168.1.109 hadoop-master </span><br><span class="line">192.168.1.145 hadoop-slave-1 </span><br><span class="line">192.168.56.1 hadoop-slave-2</span><br></pre></td></tr></table></figure><h2 id="配置基于密钥的登录"><a href="#配置基于密钥的登录" class="headerlink" title="配置基于密钥的登录"></a>配置基于密钥的登录</h2><p>在每个节点上设置ssh，这样它们就可以彼此通信，而无需提示输入密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># su hadoop </span><br><span class="line">$ ssh-keygen -t rsa </span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub tutorialspoint@hadoop-master </span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop_tp1@hadoop-slave-1 </span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop_tp2@hadoop-slave-2 </span><br><span class="line">$ chmod 0600 ~/.ssh/authorized_keys </span><br><span class="line">$ exit</span><br></pre></td></tr></table></figure><h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h2><p>在主服务器中，使用以下命令下载和安装Hadoop。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir /opt/hadoop </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> /opt/hadoop/ </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> wget http://apache.mesi.com.ar/hadoop/common/hadoop-1.2.1/hadoop-1.2.0.tar.gz </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tar -xzf hadoop-1.2.0.tar.gz </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mv hadoop-1.2.0 hadoop</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> chown -R hadoop /opt/hadoop </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> /opt/hadoop/hadoop/</span></span><br></pre></td></tr></table></figure><h2 id="配置Hadoop"><a href="#配置Hadoop" class="headerlink" title="配置Hadoop"></a>配置Hadoop</h2><p>您必须配置Hadoop服务器，方法如下所示。</p><h3 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h3><p>打开<strong>core-site.xml</strong>文件并编辑它，如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;fs.default.name&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;hdfs://hadoop-master:9000/&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;dfs.permissions&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;false&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h3><p>打开<strong>hdfs-site.xml</strong> 文件并编辑它，如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;dfs.data.dir&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;/opt/hadoop/hadoop/dfs/name/data&lt;/value&gt; </span><br><span class="line">      &lt;final&gt;true&lt;/final&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line"></span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;dfs.name.dir&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;/opt/hadoop/hadoop/dfs/name&lt;/value&gt; </span><br><span class="line">      &lt;final&gt;true&lt;/final&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line"></span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;dfs.replication&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;1&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h3><p>打开<strong>mapred-site.xml</strong>文件并编辑它，如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;mapred.job.tracker&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;hadoop-master:9001&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h3><p>打开<strong>hadoop-env.sh</strong>文件并编辑JAVA_HOME、hadoop op_conf_dir和hadoop op_opts，如下所示。</p><p>注意:按照系统配置设置JAVA_HOME。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/jdk1.7.0_17 export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true export HADOOP_CONF_DIR=/opt/hadoop/hadoop/conf</span><br></pre></td></tr></table></figure><h2 id="在从服务器上安装Hadoop"><a href="#在从服务器上安装Hadoop" class="headerlink" title="在从服务器上安装Hadoop"></a>在从服务器上安装Hadoop</h2><p>按照给定的命令在所有从服务器上安装Hadoop。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># su hadoop </span></span><br><span class="line">$ <span class="built_in">cd</span> /opt/hadoop </span><br><span class="line">$ scp -r hadoop hadoop-slave-1:/opt/hadoop </span><br><span class="line">$ scp -r hadoop hadoop-slave-2:/opt/hadoop</span><br></pre></td></tr></table></figure><h2 id="在主服务器上配置Hadoop"><a href="#在主服务器上配置Hadoop" class="headerlink" title="在主服务器上配置Hadoop"></a>在主服务器上配置Hadoop</h2><p>打开主服务器并按照给定的命令配置它。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># su hadoop </span></span><br><span class="line">$ <span class="built_in">cd</span> /opt/hadoop/hadoop</span><br></pre></td></tr></table></figure><h3 id="配置主节点"><a href="#配置主节点" class="headerlink" title="配置主节点"></a>配置主节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vi etc/hadoop/masters</span><br><span class="line">hadoop-master</span><br></pre></td></tr></table></figure><h3 id="配置从节点"><a href="#配置从节点" class="headerlink" title="配置从节点"></a>配置从节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vi etc/hadoop/slaves</span><br><span class="line">hadoop-slave-1 </span><br><span class="line">hadoop-slave-2</span><br></pre></td></tr></table></figure><h3 id="在Hadoop-Master上格式化NameNode"><a href="#在Hadoop-Master上格式化NameNode" class="headerlink" title="在Hadoop Master上格式化NameNode"></a>在Hadoop Master上格式化NameNode</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># su hadoop </span></span><br><span class="line">$ <span class="built_in">cd</span> /opt/hadoop/hadoop </span><br><span class="line">$ bin/hadoop namenode –format</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">11/10/14 10:58:07 INFO namenode.NameNode: STARTUP_MSG: /************************************************************ </span><br><span class="line">STARTUP_MSG: Starting NameNode </span><br><span class="line">STARTUP_MSG: host = hadoop-master/192.168.1.109 </span><br><span class="line">STARTUP_MSG: args = [-format] </span><br><span class="line">STARTUP_MSG: version = 1.2.0 </span><br><span class="line">STARTUP_MSG: build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2 -r 1479473; compiled by <span class="string">'hortonfo'</span> on Mon May 6 06:59:37 UTC 2013 </span><br><span class="line">STARTUP_MSG: java = 1.7.0_71 ************************************************************/ 11/10/14 10:58:08 INFO util.GSet: Computing capacity <span class="keyword">for</span> map BlocksMap editlog=/opt/hadoop/hadoop/dfs/name/current/edits</span><br><span class="line">………………………………………………….</span><br><span class="line">………………………………………………….</span><br><span class="line">…………………………………………………. 11/10/14 10:58:08 INFO common.Storage: Storage directory /opt/hadoop/hadoop/dfs/name has been successfully formatted. 11/10/14 10:58:08 INFO namenode.NameNode: </span><br><span class="line">SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at hadoop-master/192.168.1.15 ************************************************************/</span><br></pre></td></tr></table></figure><h2 id="启动Hadoop服务"><a href="#启动Hadoop服务" class="headerlink" title="启动Hadoop服务"></a>启动Hadoop服务</h2><p>下面的命令是在Hadoop- master上启动所有Hadoop服务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line">$ start-all.sh</span><br></pre></td></tr></table></figure><h2 id="在Hadoop集群中添加一个新的DataNode"><a href="#在Hadoop集群中添加一个新的DataNode" class="headerlink" title="在Hadoop集群中添加一个新的DataNode"></a>在Hadoop集群中添加一个新的DataNode</h2><p>下面给出了向Hadoop集群添加新节点的步骤。</p><h3 id="配置网络"><a href="#配置网络" class="headerlink" title="配置网络"></a>配置网络</h3><p>使用适当的网络配置向现有Hadoop集群添加新节点。假设以下网络配置。</p><p>新节点配置:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IP address : 192.168.1.103 </span><br><span class="line">netmask : 255.255.255.0</span><br><span class="line">hostname : slave3.in</span><br></pre></td></tr></table></figure><h2 id="添加用户和SSH访问"><a href="#添加用户和SSH访问" class="headerlink" title="添加用户和SSH访问"></a>添加用户和SSH访问</h2><h3 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h3><p>在新节点上，使用以下命令添加“hadoop”用户，并将hadoop用户的密码设置为“hadoop op123”或您想要的任何值。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">useradd hadoop</span><br><span class="line">passwd hadoop</span><br></pre></td></tr></table></figure><p>设置从主服务器到新从服务器的连接。</p><h3 id="在主服务器上执行以下操作"><a href="#在主服务器上执行以下操作" class="headerlink" title="在主服务器上执行以下操作"></a>在主服务器上执行以下操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/.ssh </span><br><span class="line">chmod 700 <span class="variable">$HOME</span>/.ssh </span><br><span class="line">ssh-keygen -t rsa -P <span class="string">''</span> -f <span class="variable">$HOME</span>/.ssh/id_rsa </span><br><span class="line">cat <span class="variable">$HOME</span>/.ssh/id_rsa.pub &gt;&gt; <span class="variable">$HOME</span>/.ssh/authorized_keys </span><br><span class="line">chmod 644 <span class="variable">$HOME</span>/.ssh/authorized_keys</span><br><span class="line">Copy the public key to new slave node <span class="keyword">in</span> hadoop user <span class="variable">$HOME</span> directory</span><br><span class="line">scp <span class="variable">$HOME</span>/.ssh/id_rsa.pub hadoop@192.168.1.103:/home/hadoop/</span><br></pre></td></tr></table></figure><h3 id="对从服务器执行以下操作"><a href="#对从服务器执行以下操作" class="headerlink" title="对从服务器执行以下操作"></a>对从服务器执行以下操作</h3><p>登录到hadoop。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su hadoop ssh -X hadoop@192.168.1.103</span><br></pre></td></tr></table></figure><p>将公钥内容复制到文件<strong>“$HOME/.ssh/authorized_keys”</strong>中。然后通过执行以下命令更改相同的权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.ssh </span><br><span class="line">chmod 700 <span class="variable">$HOME</span>/.ssh</span><br><span class="line">cat id_rsa.pub &gt;&gt;<span class="variable">$HOME</span>/.ssh/authorized_keys </span><br><span class="line">chmod 644 <span class="variable">$HOME</span>/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p>检查主机上的ssh登录。现在检查是否可以在不需要主节点密码的情况下ssh到新节点。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop@192.168.1.103 or hadoop@slave3</span><br></pre></td></tr></table></figure><h2 id="设置新节点的主机名"><a href="#设置新节点的主机名" class="headerlink" title="设置新节点的主机名"></a>设置新节点的主机名</h2><p>您可以在文件<strong>/etc/sysconfig/network</strong>中设置主机名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">On new slave3 machine</span><br><span class="line">NETWORKING=yes </span><br><span class="line">HOSTNAME=slave3.in</span><br></pre></td></tr></table></figure><p>要使更改生效，可以重新启动机器，也可以使用相应的主机名将主机名命令运行到新机器上(重新启动是一个不错的选择)。</p><p>slave3节点机器上:</p><p>主机名slave3.in</p><p>使用以下行更新集群所有机器上的/etc/hosts:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.1.102 slave3.in slave3</span><br></pre></td></tr></table></figure><p>现在尝试用主机名ping计算机，检查它是否解析到IP。</p><p>在新节点机器上:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping master.in</span><br></pre></td></tr></table></figure><h2 id="在新节点上启动DataNode"><a href="#在新节点上启动DataNode" class="headerlink" title="在新节点上启动DataNode"></a>在新节点上启动DataNode</h2><p>使用<strong>$HADOOP_HOME/bin/hadoop-daemon.sh</strong>手动启动datanode守护进程。它将自动联系主节点(NameNode)并加入集群。我们还应该将新节点添加到主服务器中的conf/slave文件中。基于脚本的命令将识别新节点。</p><p><strong>登录到新节点</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su hadoop or ssh -X hadoop@192.168.1.103</span><br></pre></td></tr></table></figure><p><strong>使用以下命令在新添加的从节点上启动HDFS</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure><p><strong>使用jps命令检查新节点</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">7141 DataNode</span><br><span class="line">10312 Jps</span><br></pre></td></tr></table></figure><h2 id="从Hadoop集群中删除DataNode"><a href="#从Hadoop集群中删除DataNode" class="headerlink" title="从Hadoop集群中删除DataNode"></a>从Hadoop集群中删除DataNode</h2><p>我们可以在节点运行时动态地从集群中删除节点，而不会丢失任何数据。HDFS提供了一个退役特性，可以确保安全地删除节点。使用方法如下:</p><h3 id="Step-1-登录到主服务器"><a href="#Step-1-登录到主服务器" class="headerlink" title="Step 1: 登录到主服务器"></a>Step 1: 登录到主服务器</h3><p>登录到安装Hadoop的主机用户。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ su hadoop</span><br></pre></td></tr></table></figure><h3 id="Step-2-改变集群配置"><a href="#Step-2-改变集群配置" class="headerlink" title="Step 2: 改变集群配置"></a>Step 2: 改变集群配置</h3><p>在启动集群之前，必须配置一个排除文件。添加一个名为dfs.hosts的键。排除到我们的<strong>$HADOOP_HOME/etc/hadoop/hdfs-site.xml</strong>文件。与此键关联的值提供了NameNode本地文件系统上文件的完整路径，该文件系统包含不允许连接到HDFS的机器列表。</p><p>例如，将这些行添加到<strong>etc/hadoop/hdfs-site.xml</strong>文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt; </span><br><span class="line">   &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; </span><br><span class="line">   &lt;value&gt;/home/hadoop/hadoop-1.2.1/hdfs_exclude.txt&lt;/value&gt; </span><br><span class="line">   &lt;description&gt;DFS exclude&lt;/description&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h3 id="Step-3-确定要删除的主机"><a href="#Step-3-确定要删除的主机" class="headerlink" title="Step 3: 确定要删除的主机"></a>Step 3: 确定要删除的主机</h3><p>要删除的每台机器都应该添加到<strong>hdfs_exclude.txt</strong>文件中，每行一个域名。这将阻止它们连接到NameNode。如果您想删除DataNode2，则<strong>“/home/hadoop/hadoop-1.2.1/hdfs_exclude.txt”</strong>的内容文件如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slave2.in</span><br></pre></td></tr></table></figure><h3 id="Step-4-强制重加载配置"><a href="#Step-4-强制重加载配置" class="headerlink" title="Step 4: 强制重加载配置"></a>Step 4: 强制重加载配置</h3><p>运行<strong>“$HADOOP_HOME/bin/hadoop dfsadmin -refreshNodes”</strong>命令，不带引号。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop dfsadmin -refreshNodes</span><br></pre></td></tr></table></figure><p>这将强制NameNode重新读取其配置，包括最新更新的“exclude”文件。它将在一段时间内对节点进行退役，允许将每个节点的块复制到计划保持活动的机器上。</p><p>在<strong>slave2.in</strong>，检查jps命令输出。一段时间后，您将看到DataNode进程自动关闭。</p><h3 id="Step-5-关闭节点"><a href="#Step-5-关闭节点" class="headerlink" title="Step 5: 关闭节点"></a>Step 5: 关闭节点</h3><p>删除过程完成后，删除的硬件可以安全停机进行维护。向dfsadmin运行report命令检查状态。下面的命令将描述删除节点和连接到集群的节点的状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop dfsadmin -report</span><br></pre></td></tr></table></figure><h3 id="Step-6-再次排除文件"><a href="#Step-6-再次排除文件" class="headerlink" title="Step 6: 再次排除文件"></a>Step 6: 再次排除文件</h3><p>机器一旦退役，就可以从“exclude”文件中删除它们。再次运行<strong>“$HADOOP_HOME/bin/hadoop dfsadmin -refreshNodes”</strong>会将排斥文件读入NameNode;允许数据节点在维护完成后重新加入集群，或者在集群中再次需要额外的容量，等等。</p><p><strong>特别提示：</strong>如果遵循上述流程，且tasktracker流程仍在节点上运行，则需要关闭该流程。一种方法是断开机器，就像我们在上述步骤中所做的那样。主进程将自动识别进程，并将声明为已死。删除任务跟踪器不需要遵循相同的过程，因为与DataNode相比，任务跟踪器并不重要。DataNode包含您希望安全地删除的数据，而不会丢失任何数据。</p><p>任务跟踪器可以在任何时间点通过以下命令动态运行/关闭。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop-daemon.sh stop tasktracker <span class="variable">$HADOOP_HOME</span>/bin/hadoop-daemon.sh start tasktracker</span><br></pre></td></tr></table></figure><p><strong>原文链接：</strong> <span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfbXVsdGlfbm9kZV9jbHVzdGVyLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_multi_node_cluster.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本章介绍了Hadoop多节点集群在分布式环境中的设置。&lt;/p&gt;
&lt;p&gt;由于无法演示整个集群，我们使用三个系统(一个主系统和两个从系统)解释Hadoop集群环境;以下是他们的IP地址。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hadoop Master: 192.168.1.15 (had
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：流</title>
    <link href="https://tangguangen.com/hadoop-streaming/"/>
    <id>https://tangguangen.com/hadoop-streaming/</id>
    <published>2018-12-01T12:08:25.000Z</published>
    <updated>2018-12-04T06:04:16.902Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop流是Hadoop发行版附带的实用程序。这个实用程序允许您使用任何可执行文件或脚本作为mapper 和/或reducer创建和运行Map/Reduce作业。</p><h2 id="Python例子"><a href="#Python例子" class="headerlink" title="Python例子"></a>Python例子</h2><p>对于Hadoop流，我们正在考虑word-count 问题。Hadoop中的任何工作都必须有两个阶段:mapper和reducer。我们已经在python脚本中为mapper和reducer编写了在Hadoop下运行它的代码。也可以用Perl和Ruby编写相同的代码。</p><h3 id="Mapper-Phase-Code"><a href="#Mapper-Phase-Code" class="headerlink" title="Mapper Phase Code"></a>Mapper Phase Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">!/usr/bin/python</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="comment"># Input takes from standard input for myline in sys.stdin: </span></span><br><span class="line"><span class="comment"># Remove whitespace either side myline = myline.strip() </span></span><br><span class="line"><span class="comment"># Break the line into words words = myline.split() </span></span><br><span class="line"><span class="comment"># Iterate the words list for myword in words: </span></span><br><span class="line"><span class="comment"># Write the results to standard output print '%s\t%s' % (myword, 1)</span></span><br></pre></td></tr></table></figure><p>确保该文件具有执行权限(chmod +x /home/ expert/hadoop-1.2.1/mapper.py)。</p><h3 id="Reducer-Phase-Code"><a href="#Reducer-Phase-Code" class="headerlink" title="Reducer Phase Code"></a>Reducer Phase Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter </span><br><span class="line"><span class="keyword">import</span> sys </span><br><span class="line">current_word = <span class="string">""</span></span><br><span class="line">current_count = <span class="number">0</span> </span><br><span class="line">word = <span class="string">""</span> </span><br><span class="line"><span class="comment"># Input takes from standard input for myline in sys.stdin: </span></span><br><span class="line"><span class="comment"># Remove whitespace either side myline = myline.strip() </span></span><br><span class="line"><span class="comment"># Split the input we got from mapper.py word, count = myline.split('\t', 1) </span></span><br><span class="line"><span class="comment"># Convert count variable to integer </span></span><br><span class="line">   <span class="keyword">try</span>: </span><br><span class="line">      count = int(count) </span><br><span class="line"><span class="keyword">except</span> ValueError: </span><br><span class="line">   <span class="comment"># Count was not a number, so silently ignore this line continue</span></span><br><span class="line"><span class="keyword">if</span> current_word == word: </span><br><span class="line">   current_count += count </span><br><span class="line"><span class="keyword">else</span>: </span><br><span class="line">   <span class="keyword">if</span> current_word: </span><br><span class="line">      <span class="comment"># Write result to standard output print '%s\t%s' % (current_word, current_count) </span></span><br><span class="line">   current_count = count</span><br><span class="line">   current_word = word</span><br><span class="line"><span class="comment"># Do not forget to output the last word if needed! </span></span><br><span class="line"><span class="keyword">if</span> current_word == word: </span><br><span class="line">   <span class="keyword">print</span> <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br></pre></td></tr></table></figure><p>将mapper和reducer代码分别保存在Hadoop home目录下的mapper.py and reducer.py 文件。确保这些文件具有执行权限(chmod +x mapper.py)。和chmod +x reducer.py)。由于python对缩进敏感，所以可以从下面的链接下载相同的代码。</p><h2 id="WordCount程序的执行"><a href="#WordCount程序的执行" class="headerlink" title="WordCount程序的执行"></a>WordCount程序的执行</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop jar contrib/streaming/hadoop-streaming-1.</span><br><span class="line">2.1.jar \</span><br><span class="line">   -input input_dirs \ </span><br><span class="line">   -output output_dir \ </span><br><span class="line">   -mapper &lt;path/mapper.py \ </span><br><span class="line">   -reducer &lt;path/reducer.py</span><br></pre></td></tr></table></figure><p>其中“\”用于行延续，以确保清晰的可读性。</p><p>例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop jar contrib/streaming/hadoop-streaming-1.2.1.jar -input myinput -output myoutput -mapper /home/expert/hadoop-1.2.1/mapper.py -reducer /home/expert/hadoop-1.2.1/reducer.py</span><br></pre></td></tr></table></figure><h2 id="流是如何工作的"><a href="#流是如何工作的" class="headerlink" title="流是如何工作的"></a>流是如何工作的</h2><p>在上面的例子中，mapper和reducer都是python脚本，它们从标准输入读取输入并将输出输出到标准输出。该实用程序将创建Map/Reduce作业，将作业提交到适当的集群，并监视作业的进度，直到作业完成。</p><p>当为mappers指定脚本时，每个mapper任务将在初始化mapper时作为单独的进程启动脚本。当mapper任务运行时，它将其输入转换为行，并将这些行提供给流程的标准输入(STDIN)。同时，mapper从流程的标准输出(STDOUT)中收集面向行的输出，并将每一行转换为键/值对，作为mapper的输出进行收集。默认情况下，直到第一个制表符的行前缀是键，行其余部分(不包括制表符)是值。如果行中没有制表符，则将整行视为键，值为null。但是，这可以根据需要定制。</p><p>当为reducers指定脚本时，每个reducer任务将作为单独的进程启动脚本，然后初始化reducer。当reducer任务运行时，它将输入键/值对转换为行，并将这些行提供给流程的标准输入(STDIN)。同时，reducer从流程的标准输出(STDOUT)中收集面向行的输出，将每一行转换为键/值对，作为reducer的输出进行收集。默认情况下，直到第一个制表符的行前缀是键，行其余部分(不包括制表符)是值。但是，这可以根据特定的需求进行定制。</p><h2 id="重要的命令"><a href="#重要的命令" class="headerlink" title="重要的命令"></a>重要的命令</h2><table><thead><tr><th>Parameters</th><th>Description</th></tr></thead><tbody><tr><td>-input directory/file-name</td><td>Input location for mapper. (Required)</td></tr><tr><td>-output directory-name</td><td>Output location for reducer. (Required)</td></tr><tr><td>-mapper executable or script or JavaClassName</td><td>Mapper executable. (Required)</td></tr><tr><td>-reducer executable or script or JavaClassName</td><td>Reducer executable. (Required)</td></tr><tr><td>-file file-name</td><td>Makes the mapper, reducer, or combiner executable available locally on the compute nodes.</td></tr><tr><td>-inputformat JavaClassName</td><td>Class you supply should return key/value pairs of Text class. If not specified, TextInputFormat is used as the default.</td></tr><tr><td>-outputformat JavaClassName</td><td>Class you supply should take key/value pairs of Text class. If not specified, TextOutputformat is used as the default.</td></tr><tr><td>-partitioner JavaClassName</td><td>Class that determines which reduce a key is sent to.</td></tr><tr><td>-combiner streamingCommand or JavaClassName</td><td>Combiner executable for map output.</td></tr><tr><td>-cmdenv name=value</td><td>Passes the environment variable to streaming commands.</td></tr><tr><td>-inputreader</td><td>For backwards-compatibility: specifies a record reader class (instead of an input format class).</td></tr><tr><td>-verbose</td><td>Verbose output.</td></tr><tr><td>-lazyOutput</td><td>Creates output lazily. For example, if the output format is based on FileOutputFormat, the output file is created only on the first call to output.collect (or Context.write).</td></tr><tr><td>-numReduceTasks</td><td>Specifies the number of reducers.</td></tr><tr><td>-mapdebug</td><td>Script to call when map task fails.</td></tr><tr><td>-reducedebug</td><td>Script to call when reduce task fails.</td></tr></tbody></table><p><strong>原文链接：</strong> <span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3Bfc3RyZWFtaW5nLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_streaming.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hadoop流是Hadoop发行版附带的实用程序。这个实用程序允许您使用任何可执行文件或脚本作为mapper 和/或reducer创建和运行Map/Reduce作业。&lt;/p&gt;
&lt;h2 id=&quot;Python例子&quot;&gt;&lt;a href=&quot;#Python例子&quot; class=&quot;head
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：MapReduce</title>
    <link href="https://tangguangen.com/hadoop-mapreduce/"/>
    <id>https://tangguangen.com/hadoop-mapreduce/</id>
    <published>2018-12-01T10:25:44.000Z</published>
    <updated>2018-12-04T06:04:09.274Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce是一个框架，我们可以使用它编写应用程序，以一种可靠的方式，并行地在大型商品硬件集群上处理大量数据。</p><h2 id="MapReduce是什么"><a href="#MapReduce是什么" class="headerlink" title="MapReduce是什么"></a>MapReduce是什么</h2><p>MapReduce是一种基于java的分布式计算处理技术和程序模型。MapReduce算法包含两个重要的任务，即Map和Reduce。Map接受一组数据并将其转换为另一组数据，其中单个元素被分解为元组(键/值对)。其次是reduce task，它将来自映射的输出作为输入，并将这些数据元组组合成较小的元组集合。顾名思义，reduce任务总是在映射作业之后执行。</p><p>MapReduce的主要优点是，它很容易在多个计算节点上扩展数据处理。在MapReduce模型下，数据处理原语称为映射器和约简器。将数据处理应用程序分解为映射器和还原器有时是很重要的。但是，一旦我们在MapReduce表单中编写了一个应用程序，将应用程序扩展到集群中的成百上千甚至上万台机器上，这仅仅是一个配置更改。正是这种简单的可伸缩性吸引了许多程序员使用MapReduce模型。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul><li>通常MapReduce范例是基于将计算机发送到数据所在的位置!</li><li>MapReduce程序分三个阶段执行，即map阶段、shuffle阶段和reduce阶段。<ul><li><strong>Map stage</strong> : 映射或映射程序的工作是处理输入数据。通常输入数据以文件或目录的形式存储在Hadoop文件系统(HDFS)中。输入文件逐行传递给mapper函数。映射器处理数据并创建几个小数据块。</li><li><strong>Reduce stage</strong> : 这一阶段是<strong>Shuffle</strong> 阶段和<strong>Reduce</strong> 阶段的结合。<strong>Reduce</strong> 的工作是处理来自mapper的数据。处理之后，它会生成一组新的输出，这些输出将存储在HDFS中。</li></ul></li><li>在MapReduce作业期间，Hadoop将映射和Reduce任务发送到集群中的适当服务器。</li><li>该框架管理数据传递的所有细节，例如发出任务、验证任务完成以及在节点之间围绕集群复制数据。</li><li>大多数计算发生在节点上，节点上的数据位于本地磁盘上，从而减少了网络流量。</li><li>在完成给定的任务后，集群收集并减少数据，形成适当的结果，并将其发送回Hadoop服务器。</li></ul><img title="mapreduce_algorithm" alt="mapreduce_algorithm" src="http://cdn.tangguangen.com/images/mapreduce_algorithm.jpg"><h2 id="输入和输出-Java方面"><a href="#输入和输出-Java方面" class="headerlink" title="输入和输出(Java方面)"></a>输入和输出(Java方面)</h2><p>MapReduce框架对对进行操作，即框架将作业的输入视为一组对，并生成一组对作为作业的输出，可以想象为不同类型。</p><p>键和值类应该由框架序列化，因此需要实现可写接口。此外，关键类必须实现可写可比较的接口，以便按框架进行排序。MapReduce作业的输入输出类型:(Input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt;-&gt; reduce -&gt; &lt;k3, v3&gt;(Output)</p><table><thead><tr><th></th><th>Input</th><th>Output</th></tr></thead><tbody><tr><td>Map</td><td>&lt;k1, v1&gt;</td><td>list (&lt;k2, v2&gt;)</td></tr><tr><td>Reduce</td><td>&lt;k2, list(v2)&gt;</td><td>list (&lt;k3, v3&gt;)</td></tr></tbody></table><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><ul><li><strong>PayLoad</strong> - 应用程序实现了映射和Reduce函数，构成了作业的核心。</li><li><strong>Mapper</strong> - <strong>Mapper</strong>将输入键/值对映射到一组中间键/值对。</li><li><strong>NamedNode</strong> - 管理Hadoop分布式文件系统(HDFS)的节点。</li><li><strong>DataNode</strong> - 存放数据的节点。</li><li><strong>MasterNode</strong> - 作业跟踪程序运行的节点，它接受来自客户端的作业请求。</li><li><strong>SlaveNode</strong> - 节点，Map和Reduce程序在此运行。</li><li><strong>JobTracker</strong> - 计划作业并跟踪分配给Task tracker的作业。</li><li><strong>Task Tracker</strong> - 跟踪任务并向JobTracker报告状态。</li><li><strong>Job</strong> - 程序是Mapper 和Reducer 在数据集上的执行。</li><li><strong>Task</strong> - 在数据片上执行Mapper 或Reducer 。</li><li><strong>Task Attempt</strong> - 试图在SlaveNode上执行任务的特定实例。</li></ul><h2 id="示例场景"><a href="#示例场景" class="headerlink" title="示例场景"></a>示例场景</h2><p>下面是关于一个组织的电力消耗的数据。它包含了每个月的用电量和不同年份的年平均用电量。</p><table><thead><tr><th></th><th>Jan</th><th>Feb</th><th>Mar</th><th>Apr</th><th>May</th><th>Jun</th><th>Jul</th><th>Aug</th><th>Sep</th><th>Oct</th><th>Nov</th><th>Dec</th><th>Avg</th></tr></thead><tbody><tr><td>1979</td><td>23</td><td>23</td><td>2</td><td>43</td><td>24</td><td>25</td><td>26</td><td>26</td><td>26</td><td>26</td><td>25</td><td>26</td><td>25</td></tr><tr><td>1980</td><td>26</td><td>27</td><td>28</td><td>28</td><td>28</td><td>30</td><td>31</td><td>31</td><td>31</td><td>30</td><td>30</td><td>30</td><td>29</td></tr><tr><td>1981</td><td>31</td><td>32</td><td>32</td><td>32</td><td>33</td><td>34</td><td>35</td><td>36</td><td>36</td><td>34</td><td>34</td><td>34</td><td>34</td></tr><tr><td>1984</td><td>39</td><td>38</td><td>39</td><td>39</td><td>39</td><td>41</td><td>42</td><td>43</td><td>40</td><td>39</td><td>38</td><td>38</td><td>40</td></tr><tr><td>1985</td><td>38</td><td>39</td><td>39</td><td>39</td><td>39</td><td>41</td><td>41</td><td>41</td><td>00</td><td>40</td><td>39</td><td>39</td><td>45</td></tr></tbody></table><p>如果以上述数据作为输入，我们必须编写应用程序来处理它，并产生诸如查找最大使用年、最小使用年等结果。这是一个针对记录数量有限的程序员的演练。它们将简单地编写逻辑来生成所需的输出，并将数据传递给所写的应用程序。</p><p>但是，想想一个州自形成以来所有大型工业的电力消耗数据。</p><p>当我们编写应用程序来处理这种大容量数据时，</p><ul><li>它们将花费大量的时间来执行。</li><li>当我们将数据从源移动到网络服务器等时，将会有大量的网络流量。</li></ul><p>为了解决这些问题，我们有<strong>MapReduce</strong>框架。</p><h3 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h3><p>以上数据保存为<strong>sample.txt</strong>作为输入。输入文件如下所示</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1979   23   23   2   43   24   25   26   26   26   26   25   26  25 </span><br><span class="line">1980   26   27   28  28   28   30   31   31   31   30   30   30  29 </span><br><span class="line">1981   31   32   32  32   33   34   35   36   36   34   34   34  34 </span><br><span class="line">1984   39   38   39  39   39   41   42   43   40   39   38   38  40 </span><br><span class="line">1985   38   39   39  39   39   41   41   41   00   40   39   39  45</span><br></pre></td></tr></table></figure><h3 id="程序实例"><a href="#程序实例" class="headerlink" title="程序实例"></a>程序实例</h3><p>下面是使用MapReduce框架对示例数据的程序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> hadoop; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException; </span><br><span class="line"><span class="keyword">import</span> java.io.IOException; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.*; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.*; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.*; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.*; </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessUnits</span> </span></span><br><span class="line"><span class="class"></span>&#123; </span><br><span class="line">   <span class="comment">//Mapper class </span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">E_EMapper</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> </span></span><br><span class="line"><span class="class">   <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span> ,/*<span class="title">Input</span> <span class="title">key</span> <span class="title">Type</span> */ </span></span><br><span class="line"><span class="class">   <span class="title">Text</span>,                /*<span class="title">Input</span> <span class="title">value</span> <span class="title">Type</span>*/ </span></span><br><span class="line"><span class="class">   <span class="title">Text</span>,                /*<span class="title">Output</span> <span class="title">key</span> <span class="title">Type</span>*/ </span></span><br><span class="line"><span class="class">   <span class="title">IntWritable</span>&gt;        /*<span class="title">Output</span> <span class="title">value</span> <span class="title">Type</span>*/ </span></span><br><span class="line"><span class="class">   </span>&#123; </span><br><span class="line">      </span><br><span class="line">      <span class="comment">//Map function </span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, </span></span></span><br><span class="line"><span class="function"><span class="params">      OutputCollector&lt;Text, IntWritable&gt; output,   </span></span></span><br><span class="line"><span class="function"><span class="params">      Reporter reporter)</span> <span class="keyword">throws</span> IOException </span></span><br><span class="line"><span class="function">      </span>&#123; </span><br><span class="line">         String line = value.toString(); </span><br><span class="line">         String lasttoken = <span class="keyword">null</span>; </span><br><span class="line">         StringTokenizer s = <span class="keyword">new</span> StringTokenizer(line,<span class="string">"\t"</span>); </span><br><span class="line">         String year = s.nextToken(); </span><br><span class="line">         </span><br><span class="line">         <span class="keyword">while</span>(s.hasMoreTokens())</span><br><span class="line">            &#123;</span><br><span class="line">               lasttoken=s.nextToken();</span><br><span class="line">            &#125; </span><br><span class="line">            </span><br><span class="line">         <span class="keyword">int</span> avgprice = Integer.parseInt(lasttoken); </span><br><span class="line">         output.collect(<span class="keyword">new</span> Text(year), <span class="keyword">new</span> IntWritable(avgprice)); </span><br><span class="line">      &#125; </span><br><span class="line">   &#125; </span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   <span class="comment">//Reducer class </span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">E_EReduce</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> </span></span><br><span class="line"><span class="class">   <span class="title">Reducer</span>&lt; <span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span> &gt; </span></span><br><span class="line"><span class="class">   </span>&#123;  </span><br><span class="line">   </span><br><span class="line">      <span class="comment">//Reduce function </span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">( Text key, Iterator &lt;IntWritable&gt; values, </span></span></span><br><span class="line"><span class="function"><span class="params">         OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span></span><br><span class="line"><span class="function">         </span>&#123; </span><br><span class="line">            <span class="keyword">int</span> maxavg=<span class="number">30</span>; </span><br><span class="line">            <span class="keyword">int</span> val=Integer.MIN_VALUE; </span><br><span class="line">            </span><br><span class="line">            <span class="keyword">while</span> (values.hasNext()) </span><br><span class="line">            &#123; </span><br><span class="line">               <span class="keyword">if</span>((val=values.next().get())&gt;maxavg) </span><br><span class="line">               &#123; </span><br><span class="line">                  output.collect(key, <span class="keyword">new</span> IntWritable(val)); </span><br><span class="line">               &#125; </span><br><span class="line">            &#125; </span><br><span class="line"> </span><br><span class="line">         &#125; </span><br><span class="line">   &#125;  </span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   <span class="comment">//Main function </span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span><span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">   </span>&#123; </span><br><span class="line">      JobConf conf = <span class="keyword">new</span> JobConf(ProcessUnits.class); </span><br><span class="line">      </span><br><span class="line">      conf.setJobName(<span class="string">"max_eletricityunits"</span>); </span><br><span class="line">      conf.setOutputKeyClass(Text.class);</span><br><span class="line">      conf.setOutputValueClass(IntWritable.class); </span><br><span class="line">      conf.setMapperClass(E_EMapper.class); </span><br><span class="line">      conf.setCombinerClass(E_EReduce.class); </span><br><span class="line">      conf.setReducerClass(E_EReduce.class); </span><br><span class="line">      conf.setInputFormat(TextInputFormat.class); </span><br><span class="line">      conf.setOutputFormat(TextOutputFormat.class); </span><br><span class="line">      </span><br><span class="line">      FileInputFormat.setInputPaths(conf, <span class="keyword">new</span> Path(args[<span class="number">0</span>])); </span><br><span class="line">      FileOutputFormat.setOutputPath(conf, <span class="keyword">new</span> Path(args[<span class="number">1</span>])); </span><br><span class="line">      </span><br><span class="line">      JobClient.runJob(conf); </span><br><span class="line">   &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将上述程序保存为<strong>Process .java</strong>。下面将解释程序的编译和执行。</p><h2 id="编译和执行"><a href="#编译和执行" class="headerlink" title="编译和执行"></a>编译和执行</h2><p>让我们假设我们在Hadoop用户的主目录中(例如/home/hadoop)。</p><p>按照下面给出的步骤编译和执行上述程序</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>下面的命令是创建一个目录来存储编译后的java类。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir units</span><br></pre></td></tr></table></figure><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p>下载<strong>Hadoop-core-1.2.1.jar</strong>，它用于编译和执行MapReduce程序。请访问以下链接<span class="exturl" data-url="aHR0cDovL212bnJlcG9zaXRvcnkuY29tL2FydGlmYWN0L29yZy5hcGFjaGUuaGFkb29wL2hhZG9vcC1jb3JlLzEuMi4x" title="http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core/1.2.1">http://mvnrepository.com/…<i class="fa fa-external-link"></i></span>下载jar。让我们假设下载的文件夹是<strong>/home/hadoop/</strong>。</p><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>以下命令用于编译<strong>ProcessUnits.java</strong>程序，并为该程序创建一个jar。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ javac -classpath hadoop-core-1.2.1.jar -d units ProcessUnits.java </span><br><span class="line">$ jar -cvf units.jar -C units/ .</span><br></pre></td></tr></table></figure><h3 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h3><p>下面的命令用于在HDFS中创建一个输入目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir input_dir</span><br></pre></td></tr></table></figure><h3 id="Step-7"><a href="#Step-7" class="headerlink" title="Step 7"></a>Step 7</h3><p>下面的命令用于通过从输入目录中获取输入文件来运行Eleunit_max应用程序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop jar units.jar hadoop.ProcessUnits input_dir output_dir</span><br></pre></td></tr></table></figure><p>稍等片刻，直到执行该文件。执行后，如下图所示，输出将包含输入分割数、映射任务数、reducer任务数等。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">INFO mapreduce.Job: Job job_1414748220717_0002 </span><br><span class="line">completed successfully </span><br><span class="line"><span class="number">14</span>/<span class="number">10</span>/<span class="number">31</span> <span class="number">06</span>:<span class="number">02</span>:<span class="number">52</span> </span><br><span class="line">INFO mapreduce.Job: Counters: <span class="number">49</span> </span><br><span class="line">File System Counters </span><br><span class="line"> </span><br><span class="line">FILE: Number of bytes read=<span class="number">61</span> </span><br><span class="line">FILE: Number of bytes written=<span class="number">279400</span> </span><br><span class="line">FILE: Number of read operations=<span class="number">0</span> </span><br><span class="line">FILE: Number of large read operations=<span class="number">0</span>   </span><br><span class="line">FILE: Number of write operations=<span class="number">0</span> </span><br><span class="line">HDFS: Number of bytes read=<span class="number">546</span> </span><br><span class="line">HDFS: Number of bytes written=<span class="number">40</span> </span><br><span class="line">HDFS: Number of read operations=<span class="number">9</span> </span><br><span class="line">HDFS: Number of large read operations=<span class="number">0</span> </span><br><span class="line">HDFS: Number of write operations=<span class="number">2</span> Job Counters </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   Launched map tasks=<span class="number">2</span>  </span><br><span class="line">   Launched reduce tasks=<span class="number">1</span> </span><br><span class="line">   Data-local map tasks=<span class="number">2</span>  </span><br><span class="line">   <span class="function">Total time spent by all maps in occupied <span class="title">slots</span> <span class="params">(ms)</span></span>=<span class="number">146137</span> </span><br><span class="line">   <span class="function">Total time spent by all reduces in occupied <span class="title">slots</span> <span class="params">(ms)</span></span>=<span class="number">441</span>   </span><br><span class="line">   <span class="function">Total time spent by all map <span class="title">tasks</span> <span class="params">(ms)</span></span>=<span class="number">14613</span> </span><br><span class="line">   <span class="function">Total time spent by all reduce <span class="title">tasks</span> <span class="params">(ms)</span></span>=<span class="number">44120</span> </span><br><span class="line">   Total vcore-seconds taken by all map tasks=<span class="number">146137</span> </span><br><span class="line">   </span><br><span class="line">   Total vcore-seconds taken by all reduce tasks=<span class="number">44120</span> </span><br><span class="line">   Total megabyte-seconds taken by all map tasks=<span class="number">149644288</span> </span><br><span class="line">   Total megabyte-seconds taken by all reduce tasks=<span class="number">45178880</span> </span><br><span class="line">   </span><br><span class="line">Map-Reduce Framework </span><br><span class="line"> </span><br><span class="line">Map input records=<span class="number">5</span>  </span><br><span class="line">   Map output records=<span class="number">5</span>   </span><br><span class="line">   Map output bytes=<span class="number">45</span>  </span><br><span class="line">   Map output materialized bytes=<span class="number">67</span>  </span><br><span class="line">   Input split bytes=<span class="number">208</span> </span><br><span class="line">   Combine input records=<span class="number">5</span>  </span><br><span class="line">   Combine output records=<span class="number">5</span> </span><br><span class="line">   Reduce input groups=<span class="number">5</span>  </span><br><span class="line">   Reduce shuffle bytes=<span class="number">6</span>  </span><br><span class="line">   Reduce input records=<span class="number">5</span>  </span><br><span class="line">   Reduce output records=<span class="number">5</span>  </span><br><span class="line">   Spilled Records=<span class="number">10</span>  </span><br><span class="line">   Shuffled Maps =<span class="number">2</span>  </span><br><span class="line">   Failed Shuffles=<span class="number">0</span>  </span><br><span class="line">   Merged Map outputs=<span class="number">2</span>  </span><br><span class="line">   <span class="function">GC time <span class="title">elapsed</span> <span class="params">(ms)</span></span>=<span class="number">948</span>  </span><br><span class="line">   <span class="function">CPU time <span class="title">spent</span> <span class="params">(ms)</span></span>=<span class="number">5160</span>  </span><br><span class="line">   <span class="function">Physical <span class="title">memory</span> <span class="params">(bytes)</span> snapshot</span>=<span class="number">47749120</span>  </span><br><span class="line">   <span class="function">Virtual <span class="title">memory</span> <span class="params">(bytes)</span> snapshot</span>=<span class="number">2899349504</span>  </span><br><span class="line">   <span class="function">Total committed heap <span class="title">usage</span> <span class="params">(bytes)</span></span>=<span class="number">277684224</span></span><br><span class="line">     </span><br><span class="line">File Output Format Counters </span><br><span class="line"> </span><br><span class="line">   Bytes Written=<span class="number">40</span></span><br></pre></td></tr></table></figure><h3 id="Step-8"><a href="#Step-8" class="headerlink" title="Step 8"></a>Step 8</h3><p>下面的命令用于验证输出文件夹中生成的文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -ls output_dir/</span><br></pre></td></tr></table></figure><h3 id="Step-9"><a href="#Step-9" class="headerlink" title="Step 9"></a>Step 9</h3><p>下面的命令用于查看<strong>Part-00000</strong>文件中的输出。这个文件是由HDFS生成的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -cat output_dir/part-00000</span><br></pre></td></tr></table></figure><p>下面是MapReduce程序生成的输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1981    34 </span><br><span class="line">1984    40 </span><br><span class="line">1985    45</span><br></pre></td></tr></table></figure><h3 id="Step-10"><a href="#Step-10" class="headerlink" title="Step 10"></a>Step 10</h3><p>下面的命令用于将输出文件夹从HDFS复制到本地文件系统进行分析。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -cat output_dir/part-00000/bin/hadoop dfs get output_dir /home/hadoop</span><br></pre></td></tr></table></figure><h2 id="重要的命令"><a href="#重要的命令" class="headerlink" title="重要的命令"></a>重要的命令</h2><p>所有Hadoop命令都由<strong>$HADOOP_HOME/bin/hadoop</strong>命令调用。在没有任何参数的情况下运行Hadoop脚本将打印所有命令的描述。</p><p><strong>Usage</strong> : hadoop [–config confdir] COMMAND</p><p>下表列出了可用的选项及其描述。</p><table><thead><tr><th>Options</th><th>Description</th></tr></thead><tbody><tr><td>namenode -format</td><td>Formats the DFS filesystem.</td></tr><tr><td>secondarynamenode</td><td>Runs the DFS secondary namenode.</td></tr><tr><td>namenode</td><td>Runs the DFS namenode.</td></tr><tr><td>datanode</td><td>Runs a DFS datanode.</td></tr><tr><td>dfsadmin</td><td>Runs a DFS admin client.</td></tr><tr><td>mradmin</td><td>Runs a Map-Reduce admin client.</td></tr><tr><td>fsck</td><td>Runs a DFS filesystem checking utility.</td></tr><tr><td>fs</td><td>Runs a generic filesystem user client.</td></tr><tr><td>balancer</td><td>Runs a cluster balancing utility.</td></tr><tr><td>oiv</td><td>Applies the offline fsimage viewer to an fsimage.</td></tr><tr><td>fetchdt</td><td>Fetches a delegation token from the NameNode.</td></tr><tr><td>jobtracker</td><td>Runs the MapReduce job Tracker node.</td></tr><tr><td>pipes</td><td>Runs a Pipes job.</td></tr><tr><td>tasktracker</td><td>Runs a MapReduce task Tracker node.</td></tr><tr><td>historyserver</td><td>Runs job history servers as a standalone daemon.</td></tr><tr><td>job</td><td>Manipulates the MapReduce jobs.</td></tr><tr><td>queue</td><td>Gets information regarding JobQueues.</td></tr><tr><td>version</td><td>Prints the version.</td></tr><tr><td>jar <jar></jar></td><td>Runs a jar file.</td></tr><tr><td>distcp <srcurl> <desturl></desturl></srcurl></td><td>Copies file or directories recursively.</td></tr><tr><td>distcp2 <srcurl> <desturl></desturl></srcurl></td><td>DistCp version 2.</td></tr><tr><td>archive -archiveName NAME -p</td><td>Creates a hadoop archive.</td></tr><tr><td><parent path=""> <src>* <dest></dest></src></parent></td><td></td></tr><tr><td>classpath</td><td>Prints the class path needed to get the Hadoop jar and the required libraries.</td></tr><tr><td>daemonlog</td><td>Get/Set the log level for each daemon</td></tr></tbody></table><h2 id="如何与MapReduce作业交互"><a href="#如何与MapReduce作业交互" class="headerlink" title="如何与MapReduce作业交互"></a>如何与MapReduce作业交互</h2><p>Usage: hadoop job [GENERIC_OPTIONS]</p><p>以下是Hadoop作业中可用的通用选项。</p><table><thead><tr><th>GENERIC_OPTIONS</th><th>Description</th></tr></thead><tbody><tr><td>-submit <job-file></job-file></td><td>Submits the job.</td></tr><tr><td>-status <job-id></job-id></td><td>Prints the map and reduce completion percentage and all job counters.</td></tr><tr><td>-counter <job-id> <group-name> <countername></countername></group-name></job-id></td><td>Prints the counter value.</td></tr><tr><td>-kill <job-id></job-id></td><td>Kills the job.</td></tr><tr><td>-events <job-id> &lt;fromevent-#&gt; &lt;#-of-events&gt;</job-id></td><td>Prints the events’ details received by jobtracker for the given range.</td></tr><tr><td>-history [all] <joboutputdir> - history &lt; jobOutputDir&gt;</joboutputdir></td><td>Prints job details, failed and killed tip details. More details about the job such as successful tasks and task attempts made for each task can be viewed by specifying the [all] option.</td></tr><tr><td>-list[all]</td><td>Displays all jobs. -list displays only jobs which are yet to complete.</td></tr><tr><td>-kill-task <task-id></task-id></td><td>Kills the task. Killed tasks are NOT counted against failed attempts.</td></tr><tr><td>-fail-task <task-id></task-id></td><td>Fails the task. Failed tasks are counted against failed attempts.</td></tr><tr><td>-set-priority <job-id> <priority></priority></job-id></td><td>Changes the priority of the job. Allowed priority values are VERY_HIGH, HIGH, NORMAL, LOW, VERY_LOW</td></tr></tbody></table><h3 id="查看工作状态"><a href="#查看工作状态" class="headerlink" title="查看工作状态"></a>查看工作状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -status &lt;JOB-ID&gt; </span><br><span class="line">e.g. </span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -status job_201310191043_0004</span><br></pre></td></tr></table></figure><h3 id="查看作业输出目录的历史"><a href="#查看作业输出目录的历史" class="headerlink" title="查看作业输出目录的历史"></a>查看作业输出目录的历史</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -<span class="built_in">history</span> &lt;DIR-NAME&gt; </span><br><span class="line">e.g. </span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -<span class="built_in">history</span> /user/expert/output</span><br></pre></td></tr></table></figure><h3 id="kill作业"><a href="#kill作业" class="headerlink" title="kill作业"></a>kill作业</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -<span class="built_in">kill</span> &lt;JOB-ID&gt; </span><br><span class="line">e.g. </span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop job -<span class="built_in">kill</span> job_201310191043_0004</span><br></pre></td></tr></table></figure><p><strong>原文链接：</strong> <span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfbWFwcmVkdWNlLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;MapReduce是一个框架，我们可以使用它编写应用程序，以一种可靠的方式，并行地在大型商品硬件集群上处理大量数据。&lt;/p&gt;
&lt;h2 id=&quot;MapReduce是什么&quot;&gt;&lt;a href=&quot;#MapReduce是什么&quot; class=&quot;headerlink&quot; title=&quot;Ma
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：命令手册</title>
    <link href="https://tangguangen.com/hadoop-command-reference/"/>
    <id>https://tangguangen.com/hadoop-command-reference/</id>
    <published>2018-12-01T10:11:02.000Z</published>
    <updated>2018-12-04T06:04:42.912Z</updated>
    
    <content type="html"><![CDATA[<p>与这里演示的相比，<strong> “$HADOOP_HOME/bin/hadoop fs” </strong> 中有更多的命令，尽管这些基本操作可以帮助您入门。不带附加参数运行 ./bin/hadoop dfs 将列出所有可以与 FsShell 系统一起运行的命令。此外，如果遇到问题，<strong> $HADOOP_HOME/bin/hadoop fs -help </strong>命令名将显示有关操作的简短使用摘要。</p><p>所有操作的表如下所示。参数使用以下约定:</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"&lt;path&gt;"</span> means any file or directory name. </span><br><span class="line"><span class="string">"&lt;path&gt;..."</span> means one or more file or directory names. </span><br><span class="line"><span class="string">"&lt;file&gt;"</span> means any filename. </span><br><span class="line"><span class="string">"&lt;src&gt;"</span> and <span class="string">"&lt;dest&gt;"</span> are path names <span class="keyword">in</span> a directed operation. </span><br><span class="line"><span class="string">"&lt;localSrc&gt;"</span> and <span class="string">"&lt;localDest&gt;"</span> are paths as above, but on the <span class="built_in">local</span> file system.</span><br></pre></td></tr></table></figure><p>所有其他文件和路径名都引用HDFS中的对象。</p><table><thead><tr><th>序号</th><th>命令</th></tr></thead><tbody><tr><td>1.</td><td><strong>ls <path></path></strong> <br>列出路径指定的目录的内容，显示每个条目的名称、权限、所有者、大小和修改日期。</td></tr><tr><td>2.</td><td><strong>lsr <path></path></strong><br>行为类似于-ls，但是递归地显示path的所有子目录中的条目。</td></tr><tr><td>3.</td><td><strong>du <path></path></strong><br>显示与路径匹配的所有文件的磁盘使用情况(以字节为单位);使用完整的HDFS协议前缀报告文件名。</td></tr><tr><td>4.</td><td><strong>dus <path></path></strong><br>与-du类似，但打印路径中所有文件/目录的磁盘使用情况摘要。</td></tr><tr><td>5.</td><td><strong>mv <src><dest></dest></src></strong><br>将src指示的文件或目录移动到HDFS内的dest。</td></tr><tr><td>6.</td><td><strong>cp <src> <dest></dest></src></strong><br>在HDFS中将src标识的文件或目录复制到dest。</td></tr><tr><td>7.</td><td><strong>rm <path></path></strong><br>删除路径标识的文件或空目录。</td></tr><tr><td>8.</td><td><strong>rmr <path></path></strong><br>删除路径标识的文件或目录。递归地删除任何子条目 (i.e., files or subdirectories of path).</td></tr><tr><td>9.</td><td><strong>put <localsrc> <dest></dest></localsrc></strong><br>将由localSrc标识的本地文件系统中的文件或目录复制到DFS中的dest。</td></tr><tr><td>10.</td><td><strong>copyFromLocal <localsrc> <dest><br></dest></localsrc></strong>-put相同</td></tr><tr><td>11.</td><td><strong>moveFromLocal <localsrc> <dest></dest></localsrc></strong><br>将由localSrc标识的本地文件系统中的文件或目录复制到HDFS中的dest，然后成功删除本地副本。</td></tr><tr><td>12.</td><td><strong>get [-crc] <src> <localdest></localdest></src></strong><br>将src标识的HDFS中的文件或目录复制到localDest标识的本地文件系统路径。</td></tr><tr><td>13.</td><td><strong>getmerge <src> <localdest></localdest></src></strong><br>检索与HDFS中的路径src匹配的所有文件，并将它们复制到localDest标识的本地文件系统中合并的单个文件。</td></tr><tr><td>14.</td><td><strong>cat <filen-ame></filen-ame></strong><br>在标准输出上显示文件名的内容。</td></tr><tr><td>15.</td><td><strong>copyToLocal <src> <localdest></localdest></src></strong><br>与 -get相同</td></tr><tr><td>16.</td><td><strong>moveToLocal <src> <localdest></localdest></src></strong><br>类似于-get，但成功时删除HDFS副本。</td></tr><tr><td>17.</td><td><strong>mkdir <path></path></strong><br>在HDFS中创建一个名为path的目录。<br>在路径中创建缺少的任何父目录(e.g., mkdir -p in Linux).</td></tr><tr><td>18.</td><td><strong>setrep [-R][-w] rep <path></path></strong><br>为通过路径到rep标识的文件设置目标复制因子(随着时间的推移，实际复制因子将向目标移动)</td></tr><tr><td>19.</td><td><strong>touchz <path></path></strong><br>在包含当前时间作为时间戳的路径上创建一个文件。如果文件在路径上已经存在，则失败，除非文件的大小已经为0。</td></tr><tr><td>20.</td><td><strong>test -[ezd] <path></path></strong><br>如果路径存在，返回1;长度为零;或者是目录，或者是0。</td></tr><tr><td>21.</td><td><strong>stat [format] <path></path></strong><br>打印关于路径的信息。Format是一个字符串，它接受块大小(%b)、文件名(%n)、块大小(%o)、复制(%r)和修改日期(%y， %y)。</td></tr><tr><td>22.</td><td><strong>tail [-f] <file2name></file2name></strong><br>显示stdout上文件的最后1KB。</td></tr><tr><td>23.</td><td><strong>chmod [-R] mode,mode,… <path></path>…</strong><br>与一个或多个相关联的文件权限对象的更改了路径….使用r模式递归执行更改是3位八进制模式，或{augo}+/-{rwxX}。假设没有指定范围且不应用umask。</td></tr><tr><td>24.</td><td><strong>chown [-R][owner][:[group]] <path></path>…</strong><br>集拥有用户和/或组的文件或目录被路径….如果指定-R，则递归设置所有者。</td></tr><tr><td>25.</td><td>chgrp [-R] group <path></path>…<br>设置拥有小组确认的文件或目录路径….如果指定-R，则递归地设置组。</td></tr><tr><td>26.</td><td><strong>help <cmd-name></cmd-name></strong><br>返回上述命令之一的使用信息。你必须省略cmd中的“-”字符。</td></tr></tbody></table><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfY29tbWFuZF9yZWZlcmVuY2UuaHRt" title="https://www.tutorialspoint.com/hadoop/hadoop_command_reference.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;与这里演示的相比，&lt;strong&gt; “$HADOOP_HOME/bin/hadoop fs” &lt;/strong&gt; 中有更多的命令，尽管这些基本操作可以帮助您入门。不带附加参数运行 ./bin/hadoop dfs 将列出所有可以与 FsShell 系统一起运行的命令。此外，如果遇到问题，&lt;strong&gt; $HADOOP_HOME/bin/hadoop fs -help &lt;/strong&gt;命令名将显示有关操作的简短使用摘要。&lt;/p&gt;
&lt;p&gt;所有操作的表如下所示。参数使用以下约定:&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：HDFS操作</title>
    <link href="https://tangguangen.com/hadoop-hdfs-operations/"/>
    <id>https://tangguangen.com/hadoop-hdfs-operations/</id>
    <published>2018-12-01T09:58:19.000Z</published>
    <updated>2018-12-04T06:04:51.496Z</updated>
    
    <content type="html"><![CDATA[<h2 id="启动HDFS"><a href="#启动HDFS" class="headerlink" title="启动HDFS"></a>启动HDFS</h2><p>首先，您必须格式化配置的HDFS文件系统，打开namenode (HDFS服务器)，并执行以下命令。</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop namenode -format</span><br></pre></td></tr></table></figure><p>格式化HDFS之后，启动分布式文件系统。下面的命令将启动namenode以及数据节点作为集群。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-dfs.sh</span><br></pre></td></tr></table></figure><h2 id="列出HDFS中的文件"><a href="#列出HDFS中的文件" class="headerlink" title="列出HDFS中的文件"></a>列出HDFS中的文件</h2><p>在服务器中加载信息后，我们可以使用“ls”查找目录中的文件列表、文件状态。下面给出了可以作为参数传递到目录或文件名的ls语法。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -ls &lt;args&gt;</span><br></pre></td></tr></table></figure><h2 id="将数据插入HDFS"><a href="#将数据插入HDFS" class="headerlink" title="将数据插入HDFS"></a>将数据插入HDFS</h2><p>假设我们在本地系统中一个名为file.txt的文件，应该保存在hdfs文件系统中。按照下面给出的步骤在Hadoop文件系统中插入所需的文件。</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>您必须创建一个输入目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir /user/input</span><br></pre></td></tr></table></figure><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p>使用put命令将数据文件从本地系统传输和存储到Hadoop文件系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -put /home/file.txt /user/input</span><br></pre></td></tr></table></figure><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>您可以使用ls命令验证该文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -ls /user/input</span><br></pre></td></tr></table></figure><h2 id="从HDFS检索数据"><a href="#从HDFS检索数据" class="headerlink" title="从HDFS检索数据"></a>从HDFS检索数据</h2><p>假设HDFS中有一个名为outfile的文件。下面是一个从Hadoop文件系统检索所需文件的简单演示。</p><h3 id="Step-1-1"><a href="#Step-1-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>首先，使用cat命令查看来自HDFS的数据。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -cat /user/output/outfile</span><br></pre></td></tr></table></figure><h3 id="Step-2-1"><a href="#Step-2-1" class="headerlink" title="Step 2"></a>Step 2</h3><p>使用get命令将文件从HDFS获取到本地文件系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -get /user/output/ /home/hadoop_tp/</span><br></pre></td></tr></table></figure><h2 id="关闭HDFS"><a href="#关闭HDFS" class="headerlink" title="关闭HDFS"></a>关闭HDFS</h2><p>可以使用以下命令关闭HDFS</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stop-dfs.sh</span><br></pre></td></tr></table></figure><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfaGRmc19vcGVyYXRpb25zLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_hdfs_operations.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;启动HDFS&quot;&gt;&lt;a href=&quot;#启动HDFS&quot; class=&quot;headerlink&quot; title=&quot;启动HDFS&quot;&gt;&lt;/a&gt;启动HDFS&lt;/h2&gt;&lt;p&gt;首先，您必须格式化配置的HDFS文件系统，打开namenode (HDFS服务器)，并执行以下命令。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：HDFS概述</title>
    <link href="https://tangguangen.com/hadoop-hdfs-overview/"/>
    <id>https://tangguangen.com/hadoop-hdfs-overview/</id>
    <published>2018-12-01T09:35:37.000Z</published>
    <updated>2018-12-04T06:04:00.752Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop文件系统采用分布式文件系统设计开发。它在普通硬件上运行。与其他分布式系统不同，HDFS具有很高的容错性，并且使用低成本的硬件进行设计。</p><p>HDFS存储大量数据并提供更容易的访问。为了存储如此巨大的数据，文件被存储在多台机器上。这些文件以冗余的方式存储，以便在发生故障时将系统从可能的数据损失中拯救出来。HDFS还使应用程序可用于并行处理。</p><h2 id="HDFS的特点"><a href="#HDFS的特点" class="headerlink" title="HDFS的特点"></a>HDFS的特点</h2><ul><li>适用于分布式存储和处理。</li><li>Hadoop提供了一个与HDFS交互的命令接口。</li><li>namenode和datanode的内置服务器可以方便地检查集群的状态。</li><li>对文件系统数据的流访问。</li><li>HDFS提供文件权限和身份验证。</li></ul><h2 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h2><p>下面给出Hadoop文件系统的架构。</p><img title="hdfs_architecture" alt="hdfs_architecture" src="http://cdn.tangguangen.com/images/hdfs_architecture.jpg"><p>HDFS遵循主从体系结构，它具有以下元素。</p><h3 id="Namenode"><a href="#Namenode" class="headerlink" title="Namenode"></a>Namenode</h3><p>namenode是包含GNU/Linux操作系统和namenode软件的商品硬件。它是一种可以在普通硬件上运行的软件。具有namenode的系统充当主服务器，它执行以下任务:</p><ul><li>管理文件系统名称空间。</li><li>管理客户对文件的访问。</li><li>它还执行文件系统操作，如重命名、关闭和打开文件和目录。</li></ul><h3 id="Datanode"><a href="#Datanode" class="headerlink" title="Datanode"></a>Datanode</h3><p>datanode是一种具有GNU/Linux操作系统和datanode软件的普通硬件。对于集群中的每个节点(商品硬件/系统)，都将有一个datanode。这些节点管理其系统的数据存储。</p><ul><li>数据节点根据客户端请求在文件系统上执行读写操作。</li><li>它们还根据namenode的指令执行块创建、删除和复制等操作。</li></ul><h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><p>用户数据一般存储在HDFS文件中。文件系统中的文件将被分成一个或多个段和/或存储在单个数据节点中。这些文件段称为块。换句话说，HDFS可以读写的最小数据量称为块。默认块大小为64MB，但是可以根据需要在HDFS配置中进行更改而增加。</p><h2 id="HDFS的目标"><a href="#HDFS的目标" class="headerlink" title="HDFS的目标"></a>HDFS的目标</h2><ul><li><strong>故障检测与恢复:</strong> 由于HDFS包含大量的商用硬件，部件故障频繁。因此，HDFS应该具有快速、自动的故障检测和恢复机制。</li><li><strong>海量的数据集:</strong> HDFS每个集群应该有数百个节点，以管理拥有庞大数据集的应用程序。</li><li><strong>硬件在数据上</strong> 当计算发生在数据附近时，可以有效地完成请求的任务。特别是在涉及到大量数据集的情况下，它会减少网络流量并增加吞吐量。</li></ul><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfaGRmc19vdmVydmlldy5odG0=" title="https://www.tutorialspoint.com/hadoop/hadoop_hdfs_overview.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hadoop文件系统采用分布式文件系统设计开发。它在普通硬件上运行。与其他分布式系统不同，HDFS具有很高的容错性，并且使用低成本的硬件进行设计。&lt;/p&gt;
&lt;p&gt;HDFS存储大量数据并提供更容易的访问。为了存储如此巨大的数据，文件被存储在多台机器上。这些文件以冗余的方式存储
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：安装与环境设置</title>
    <link href="https://tangguangen.com/hadoop-enviornment-setup/"/>
    <id>https://tangguangen.com/hadoop-enviornment-setup/</id>
    <published>2018-12-01T08:23:55.000Z</published>
    <updated>2018-12-04T06:03:52.213Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop支持Windows, Mac, Linux, 但推荐是用Linux环境学习Hadoop。因此，我们必须安装一个Linux操作系统来设置Hadoop环境。如果您的操作系统不是Linux，那么您可以在其中安装一个Virtualbox软件，并在Virtualbox中包含Linux。</p><h2 id="安装前配置"><a href="#安装前配置" class="headerlink" title="安装前配置"></a>安装前配置</h2><p>在将Hadoop安装到Linux环境之前，我们需要使用ssh(Secure Shell)来设置Linux。按照下面给出的步骤设置Linux环境。</p><h3 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h3><p>首先，建议为Hadoop创建一个单独的用户，以便将Hadoop文件系统与Unix文件系统隔离开来。按照以下步骤创建用户:</p><ul><li>使用“su”命令打开根目录。</li><li>使用“useradd username”命令从根帐户创建一个用户。</li><li>现在您可以使用“su用户名”命令打开一个现有的用户帐户。</li></ul><p>打开Linux终端，输入以下命令来创建用户。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ su </span><br><span class="line">   password: </span><br><span class="line"><span class="comment"># useradd hadoop </span></span><br><span class="line"><span class="comment"># passwd hadoop </span></span><br><span class="line">   New passwd: </span><br><span class="line">   Retype new passwd</span><br></pre></td></tr></table></figure><h2 id="SSH设置和密钥生成"><a href="#SSH设置和密钥生成" class="headerlink" title="SSH设置和密钥生成"></a>SSH设置和密钥生成</h2><p>在集群上执行操作需要设置SSH，例如启动、停止、分布式守护进程shell操作。为了验证Hadoop的不同用户，需要为Hadoop用户提供公钥/私钥对，并与不同的用户共享。</p><p>下面的命令用于使用SSH生成键值对。从id_rsa.pub复制公钥到authorized_keys，并分别向所有者提供authorized_keys文件的读写权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa </span><br><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys </span><br><span class="line">$ chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h2 id="安装Java"><a href="#安装Java" class="headerlink" title="安装Java"></a>安装Java</h2><p>Hadoop必须安装Java。首先，您应该使用“java -version”命令验证系统中是否存在java。查看java版本命令的语法如下所示。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ java -version</span><br></pre></td></tr></table></figure><p>如果一切正常，它将给出以下输出。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java version <span class="string">"1.7.0_71"</span> </span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_71-b13) </span><br><span class="line">Java HotSpot(TM) Client VM (build 25.0-b02, mixed mode)</span><br></pre></td></tr></table></figure><p>如果您的系统中没有安装java，那么按照下面给出的步骤安装java。</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>下载java (JDK &lt;最新版本&gt; - X64.tar.gz)，请访问以下链接<span class="exturl" data-url="aHR0cDovL3d3dy5vcmFjbGUuY29tL3RlY2huZXR3b3JrL2phdmEvamF2YXNlL2Rvd25sb2Fkcy9qZGs3LWRvd25sb2FkczE4ODAyNjAuaHRtbOOAgg==" title="http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads1880260.html。">http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads1880260.html。<i class="fa fa-external-link"></i></span></p><p>然后jdk-7u71-linux-x64.tar.gz将被下载到您的系统中。</p><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p>通常您会在下载文件夹中找到下载的java文件。验证它并提取jdk-7u71-linux-x64.gz文件使用以下命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> Downloads/ </span><br><span class="line">$ ls </span><br><span class="line">jdk-7u71-linux-x64.gz </span><br><span class="line">$ tar zxf jdk-7u71-linux-x64.gz </span><br><span class="line">$ ls </span><br><span class="line">jdk1.7.0_71   jdk-7u71-linux-x64.gz</span><br></pre></td></tr></table></figure><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>要让所有用户都可以使用java，必须将其移动到“/usr/local/”位置。打开root，并键入以下命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ su </span><br><span class="line">password: </span><br><span class="line"><span class="comment"># mv jdk1.7.0_71 /usr/local/ </span></span><br><span class="line"><span class="comment"># exit</span></span><br></pre></td></tr></table></figure><h3 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h3><p>要设置PATH和JAVA_HOME变量，请向~/.bashrc添加以下命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.7.0_71 </span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure><p>现在令所有更改生效。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="Step-5"><a href="#Step-5" class="headerlink" title="Step 5"></a>Step 5</h3><p>使用以下命令配置java替代方案:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># alternatives --install /usr/bin/java java usr/local/java/bin/java 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --install /usr/bin/javac javac usr/local/java/bin/javac 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --install /usr/bin/jar jar usr/local/java/bin/jar 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --set java usr/local/java/bin/java</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --set javac usr/local/java/bin/javac</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatives --set jar usr/local/java/bin/jar</span></span><br></pre></td></tr></table></figure><p>现在验证来自终端的java版本命令，使用之前提到的查看Java版本命令。</p><h2 id="下载Hadoop"><a href="#下载Hadoop" class="headerlink" title="下载Hadoop"></a>下载Hadoop</h2><p>使用以下命令从Apache software foundation下载并提取Hadoop 2.4.1（根据需要选择版本）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ su </span><br><span class="line">password: </span><br><span class="line"><span class="comment"># cd /usr/local </span></span><br><span class="line"><span class="comment"># wget http://apache.claz.org/hadoop/common/hadoop-2.4.1/ </span></span><br><span class="line">hadoop-2.4.1.tar.gz </span><br><span class="line"><span class="comment"># tar xzf hadoop-2.4.1.tar.gz </span></span><br><span class="line"><span class="comment"># mv hadoop-2.4.1/* to hadoop/ </span></span><br><span class="line"><span class="comment"># exit</span></span><br></pre></td></tr></table></figure><h2 id="Hadoop的操作模式"><a href="#Hadoop的操作模式" class="headerlink" title="Hadoop的操作模式"></a>Hadoop的操作模式</h2><p>下载Hadoop后，可以使用以下三种支持模式之一来操作Hadoop集群:</p><ul><li><strong>本地/独立模式:</strong> 在您的系统中下载Hadoop之后，默认情况下，它是在独立模式下配置的，可以作为单个java进程运行。</li><li><strong>伪分布模式:</strong> 这是一个模拟在单机上的分布式。每个Hadoop守护进程(如hdfs、yarn、MapReduce等)将作为一个单独的java进程运行。这种模式对开发很有用。</li><li><strong>全分布模式:</strong> 这种模式是完全分布式的，集群中至少有两台或多台机器。我们将在接下来的章节中详细介绍这种模式。</li></ul><h2 id="以独立模式安装Hadoop"><a href="#以独立模式安装Hadoop" class="headerlink" title="以独立模式安装Hadoop"></a>以独立模式安装Hadoop</h2><p>这里我们将讨论Hadoop 2.4.1在独立模式下的安装。</p><p>没有运行守护进程，所有东西都在单个JVM中运行。独立模式适合在开发过程中运行MapReduce程序，因为它易于测试和调试。</p><h3 id="Hadoop配置"><a href="#Hadoop配置" class="headerlink" title="Hadoop配置"></a>Hadoop配置</h3><p>您可以通过向~/.bashrc文件添加以下命令来设置Hadoop环境变量。</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure><p>在继续之前，您需要确保Hadoop工作正常。使用以下命令查看hadoop是否安装成功:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop version</span><br></pre></td></tr></table></figure><p>如果您的设置一切正常，那么您应该会看到以下结果:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 2.4.1 </span><br><span class="line">Subversion https://svn.apache.org/repos/asf/hadoop/common -r 1529768 </span><br><span class="line">Compiled by hortonmu on 2013-10-07T06:28Z </span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From <span class="built_in">source</span> with checksum 79e53ce7994d1628b240f09af91e1af4</span><br></pre></td></tr></table></figure><p>这意味着Hadoop的独立模式设置工作正常。默认情况下，Hadoop被配置为在一台机器上以非分布式模式运行。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>让我们查看Hadoop的一个简单示例。Hadoop安装提供了以下示例MapReduce jar文件，它提供了MapReduce的基本功能，可以用于计算，如Pi值、给定文件列表中的字数等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar</span><br></pre></td></tr></table></figure><p>让我们有一个输入目录，在这里我们将推动一些文件，我们的要求是计数总字数在这些文件。要计算单词总数，我们不需要编写MapReduce，只要.jar文件包含单词计数的实现即可。您可以使用相同的.jar文件尝试其他示例;只要发出以下命令，检查hadoop- MapReduce -example -2.2.0.jar文件支持的MapReduce功能程序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduceexamples-2.2.0.jar</span><br></pre></td></tr></table></figure><h3 id="Step-1-1"><a href="#Step-1-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>在输入目录中创建临时内容文件。您可以在希望工作的任何地方创建此输入目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir input </span><br><span class="line">$ cp <span class="variable">$HADOOP_HOME</span>/*.txt input </span><br><span class="line">$ ls -l input</span><br></pre></td></tr></table></figure><p>它将在您的输入目录中提供以下文件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total 24 </span><br><span class="line">-rw-r--r-- 1 root root 15164 Feb 21 10:14 LICENSE.txt </span><br><span class="line">-rw-r--r-- 1 root root   101 Feb 21 10:14 NOTICE.txt</span><br><span class="line">-rw-r--r-- 1 root root  1366 Feb 21 10:14 README.txt</span><br></pre></td></tr></table></figure><p>这些文件是从Hadoop安装主目录复制的。对于您的实验，您可以拥有不同的大文件集。</p><h3 id="Step-2-1"><a href="#Step-2-1" class="headerlink" title="Step 2"></a>Step 2</h3><p>步骤2将进行所需的处理，并将输出保存在output/part-r00000文件中，您可以使用以下命令进行检查:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$cat</span> output/*</span><br></pre></td></tr></table></figure><p>它将列出输入目录中所有文件中可用的所有单词及其总数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"AS      4 </span></span><br><span class="line"><span class="string">"</span>Contribution<span class="string">" 1 </span></span><br><span class="line"><span class="string">"</span>Contributor<span class="string">" 1 </span></span><br><span class="line"><span class="string">"</span>Derivative 1</span><br><span class="line"><span class="string">"Legal 1</span></span><br><span class="line"><span class="string">"</span>License<span class="string">"      1</span></span><br><span class="line"><span class="string">"</span>License<span class="string">");     1 </span></span><br><span class="line"><span class="string">"</span>Licensor<span class="string">"      1</span></span><br><span class="line"><span class="string">"</span>NOTICE”        1 </span><br><span class="line"><span class="string">"Not      1 </span></span><br><span class="line"><span class="string">"</span>Object<span class="string">"        1 </span></span><br><span class="line"><span class="string">"</span>Source”        1 </span><br><span class="line"><span class="string">"Work”    1 </span></span><br><span class="line"><span class="string">"</span>You<span class="string">"     1 </span></span><br><span class="line"><span class="string">"</span>Your<span class="string">")   1 </span></span><br><span class="line"><span class="string">"</span>[]<span class="string">"      1 </span></span><br><span class="line"><span class="string">"</span>control<span class="string">"       1 </span></span><br><span class="line"><span class="string">"</span>printed        1 </span><br><span class="line"><span class="string">"submitted"</span>     1 </span><br><span class="line">(50%)     1 </span><br><span class="line">(BIS),    1 </span><br><span class="line">(C)       1 </span><br><span class="line">(Don<span class="string">'t)   1 </span></span><br><span class="line"><span class="string">(ECCN)    1 </span></span><br><span class="line"><span class="string">(INCLUDING      2 </span></span><br><span class="line"><span class="string">(INCLUDING,     2 </span></span><br><span class="line"><span class="string">.............</span></span><br></pre></td></tr></table></figure><h2 id="以伪分布式模式安装Hadoop"><a href="#以伪分布式模式安装Hadoop" class="headerlink" title="以伪分布式模式安装Hadoop"></a>以伪分布式模式安装Hadoop</h2><p>按照下面给出的步骤以伪分布式模式安装Hadoop 2.4.1。</p><h3 id="Step-1-环境配置"><a href="#Step-1-环境配置" class="headerlink" title="Step 1: 环境配置"></a>Step 1: 环境配置</h3><p>您可以通过向~/.bashrc文件添加以下命令来设置Hadoop环境变量。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop </span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> YARN_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$HADOOP_HOME</span>/lib/native </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$HADOOP_HOME</span>/bin </span><br><span class="line"><span class="built_in">export</span> HADOOP_INSTALL=<span class="variable">$HADOOP_HOME</span></span><br></pre></td></tr></table></figure><p>现在将所有更改应用到当前运行的系统中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="Step-2-Hadoop配置"><a href="#Step-2-Hadoop配置" class="headerlink" title="Step 2: Hadoop配置"></a>Step 2: Hadoop配置</h3><p>您可以在“$HADOOP_HOME/etc/hadoop”位置找到所有Hadoop配置文件。需要根据Hadoop基础设施对这些配置文件进行更改。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br></pre></td></tr></table></figure><p>为了用java开发Hadoop程序，必须在<strong>Hadoop -env.sh</strong>文件中替换<strong>JAVA_HOME</strong>值</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.7.0_71</span><br></pre></td></tr></table></figure><p>以下是配置Hadoop需要编辑的文件列表。</p><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a><strong>core-site.xml</strong></h4><p><strong>core-site.xml</strong>文件包含一些信息，例如Hadoop实例使用的端口号、为文件系统分配的内存、存储数据的内存限制以及读/写缓冲区的大小。</p><p>打开<strong>core-site.xml</strong>并在<configuration>, </configuration>之间添加以下属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a><strong>hdfs-site.xml</strong></h4><p><strong>hdfs-site.xml</strong>文件包含数据备份数量值、namenode路径和本地文件系统的datanode路径等信息。它意味着您希望存储Hadoop基础结构的地方。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dfs.replication (data replication value) = 1 </span><br><span class="line">(In the below given path /hadoop/ is the user name. </span><br><span class="line">hadoopinfra/hdfs/namenode is the directory created by hdfs file system.) </span><br><span class="line">namenode path = //home/hadoop/hadoopinfra/hdfs/namenode </span><br><span class="line">(hadoopinfra/hdfs/datanode is the directory created by hdfs file system.) </span><br><span class="line">datanode path = //home/hadoop/hadoopinfra/hdfs/datanode</span><br></pre></td></tr></table></figure><p>打开该文件，并在该文件中<configuration> </configuration>标签之间添加以下属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;file:///home/hadoop/hadoopinfra/hdfs/namenode &lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.data.dir&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;file:///home/hadoop/hadoopinfra/hdfs/datanode &lt;/value&gt; </span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">       </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 在上面的文件中，所有属性值都是用户定义的，您可以根据Hadoop基础结构进行更改。</p><h4 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a><strong>yarn-site.xml</strong></h4><p>这个文件用于将yarn配置到Hadoop中。打开yarn-site.xml文件并在该文件中的<configuration>, </configuration>标签之间添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;mapreduce_shuffle&lt;/value&gt; </span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a><strong>mapred-site.xml</strong></h4><p>这个文件用于指定我们正在使用的MapReduce框架。默认情况下，Hadoop包含一个yarn-site.xml模板。首先，需要从<strong>mapred-site.xml.template</strong>复制文件到<strong>mapred-site.xml</strong>文件。使用以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p>打开mapred-site.xml文件。并在该文件中的<configuration>, </configuration>标签之间添加以下属性。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> </span><br><span class="line">   &lt;property&gt; </span><br><span class="line">      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">&lt;/configuration</span><br></pre></td></tr></table></figure><h2 id="验证Hadoop安装"><a href="#验证Hadoop安装" class="headerlink" title="验证Hadoop安装"></a>验证Hadoop安装</h2><p>以下步骤用于验证Hadoop的安装。</p><h3 id="Step-1-Name-Node"><a href="#Step-1-Name-Node" class="headerlink" title="Step 1: Name Node"></a>Step 1: Name Node</h3><p>使用“hdfs namenode -format”命令设置namenode，如下所示。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~ </span><br><span class="line">$ hdfs namenode -format</span><br></pre></td></tr></table></figure><p>预期结果如下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">10/24/14 21:30:55 INFO namenode.NameNode: STARTUP_MSG: </span><br><span class="line">/************************************************************ </span><br><span class="line">STARTUP_MSG: Starting NameNode </span><br><span class="line">STARTUP_MSG:   host = localhost/192.168.1.11 </span><br><span class="line">STARTUP_MSG:   args = [-format] </span><br><span class="line">STARTUP_MSG:   version = 2.4.1 </span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">10/24/14 21:30:56 INFO common.Storage: Storage directory </span><br><span class="line">/home/hadoop/hadoopinfra/hdfs/namenode has been successfully formatted. </span><br><span class="line">10/24/14 21:30:56 INFO namenode.NNStorageRetentionManager: Going to </span><br><span class="line">retain 1 images with txid &gt;= 0 </span><br><span class="line">10/24/14 21:30:56 INFO util.ExitUtil: Exiting with status 0 </span><br><span class="line">10/24/14 21:30:56 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************ </span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at localhost/192.168.1.11 </span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><h3 id="Step-2-验证Hadoop-dfs"><a href="#Step-2-验证Hadoop-dfs" class="headerlink" title="Step 2: 验证Hadoop dfs"></a>Step 2: 验证Hadoop dfs</h3><p>下面的命令用于启动dfs。执行此命令将启动Hadoop文件系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-dfs.sh</span><br></pre></td></tr></table></figure><p>预期输出如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">10/24/14 21:37:56 </span><br><span class="line">Starting namenodes on [localhost] </span><br><span class="line">localhost: starting namenode, logging to /home/hadoop/hadoop</span><br><span class="line">2.4.1/logs/hadoop-hadoop-namenode-localhost.out </span><br><span class="line">localhost: starting datanode, logging to /home/hadoop/hadoop</span><br><span class="line">2.4.1/logs/hadoop-hadoop-datanode-localhost.out </span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br></pre></td></tr></table></figure><h3 id="Step-3-验证Yarn脚本"><a href="#Step-3-验证Yarn脚本" class="headerlink" title="Step 3: 验证Yarn脚本"></a>Step 3: 验证Yarn脚本</h3><p>下面的命令用于启动纱线脚本。执行此命令将启动纱线守护进程。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-yarn.sh</span><br></pre></td></tr></table></figure><p>预期输出如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">starting yarn daemons </span><br><span class="line">starting resourcemanager, logging to /home/hadoop/hadoop</span><br><span class="line">2.4.1/logs/yarn-hadoop-resourcemanager-localhost.out </span><br><span class="line">localhost: starting nodemanager, logging to /home/hadoop/hadoop</span><br><span class="line">2.4.1/logs/yarn-hadoop-nodemanager-localhost.out</span><br></pre></td></tr></table></figure><h3 id="Step-4-在浏览器上访问Hadoop"><a href="#Step-4-在浏览器上访问Hadoop" class="headerlink" title="Step 4: 在浏览器上访问Hadoop"></a>Step 4: 在浏览器上访问Hadoop</h3><p>访问Hadoop的默认端口号是50070。使用以下url在浏览器上获取Hadoop服务。</p><p>｛% qnimg hadoop_on_browser.jpg title:hadoop_on_browser alt: hadoop_on_browser%｝</p><h3 id="Step-5-验证集群中的所有应用程序"><a href="#Step-5-验证集群中的所有应用程序" class="headerlink" title="Step 5: 验证集群中的所有应用程序"></a>Step 5: 验证集群中的所有应用程序</h3><p>访问集群所有应用程序的默认端口号是8088。使用以下url访问此服务。</p><img alt="hadoop_application_cluster" src="http://cdn.tangguangen.com/images/hadoop_application_cluster.jpg"><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfZW52aW9ybm1lbnRfc2V0dXAuaHRt" title="https://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm">https://www.tutorialspoint.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hadoop支持Windows, Mac, Linux, 但推荐是用Linux环境学习Hadoop。因此，我们必须安装一个Linux操作系统来设置Hadoop环境。如果您的操作系统不是Linux，那么您可以在其中安装一个Virtualbox软件，并在Virtualbox中包
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：Hadoop介绍</title>
    <link href="https://tangguangen.com/hadoop-introduction/"/>
    <id>https://tangguangen.com/hadoop-introduction/</id>
    <published>2018-11-30T13:05:54.000Z</published>
    <updated>2018-12-04T06:03:43.609Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop是一个用java编写的Apache开源框架，它允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序工作在一个跨计算机集群提供分布式存储和计算的环境中。Hadoop被设计成从单个服务器扩展到数千台机器，每台机器都提供本地计算和存储。</p><h2 id="Hadoop架构"><a href="#Hadoop架构" class="headerlink" title="Hadoop架构"></a>Hadoop架构</h2><p>Hadoop框架包括以下四个模块:</p><ul><li><strong>Hadoop Common:</strong> 这是其他Hadoop模块依赖的Java库和工具。这些库提供文件系统和操作系统级别的抽象，并包含启动Hadoop所需的Java文件和脚本。</li><li><strong>Hadoop YARN: </strong>这是一个用于作业调度和集群资源管理的框架。</li><li><strong>Hadoop分布式文件系统(HDFS™):</strong>  一种分布式文件系统，提供对应用程序数据的高吞吐量访问。</li><li><strong>MapReduce:</strong> 这是一个基于YARN的大型数据集并行处理系统。</li></ul><p>我们可以使用下面的图来描述Hadoop框架中可用的这四个组件。</p><img title="hadoop-architecture" alt="hadoop-architecture" src="http://cdn.tangguangen.com/images/hadoop_architecture.jpg"><p>自2012年以来，“Hadoop”一词通常不仅指上述基本模块，还指可以安装在Hadoop之上或与Hadoop并行的附加软件包集合，如<strong>Apache Pig、Apache Hive、Apache HBase、Apache Spark</strong>等。</p><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>Hadoop <strong>MapReduce</strong>是一个易于编写应用程序的软件框架，这些应用程序以可靠、容错的方式并行处理大集群(数千个节点)上的海量数据。</p><p>MapReduce这个词实际上是指Hadoop程序执行的两个不同的任务:</p><ul><li><strong>Map任务:</strong> 这是第一个任务，它接受输入数据并将其转换为一组数据集合，其中单个元素被分解为元组(键/值对)。</li><li><strong>Reduce任务:</strong> 该任务将map任务的输出作为输入，并将这些数据元组组合成较小的元组集合。reduce任务总是在map任务之后执行。</li></ul><p>通常输入和输出都存储在文件系统中。该框架负责调度任务、监视任务并重新执行失败的任务。</p><p>MapReduce框架由一个master <strong>JobTracker</strong>和每个集群节点的一个slave <strong>TaskTracker</strong>组成。master 负责资源管理，跟踪资源消耗/可用性，调度从服务器上的作业组件任务，监视它们并重新执行失败的任务。slave 按照主任务的指示执行任务，并定期向主任务提供任务状态信息。</p><p>JobTracker是Hadoop MapReduce服务的一个单点故障，这意味着如果JobTracker宕机，所有正在运行的作业都会停止。</p><h2 id="Hadoop-Distributed-File-System"><a href="#Hadoop-Distributed-File-System" class="headerlink" title="Hadoop Distributed File System"></a>Hadoop Distributed File System</h2><p>Hadoop可以直接与任何可挂载的分布式文件系统(如本地FS、HFTP FS、S3 FS等)一起工作，但是Hadoop最常用的文件系统是Hadoop分布式文件系统(HDFS)。</p><p>Hadoop分布式文件系统(HDFS)是基于谷歌文件系统(GFS)的，它提供了一个分布式文件系统，可以在小型计算机的大型集群(数千台计算机)上以可靠、容错的方式运行。</p><p>HDFS使用主/从（master/slave）架构，其中主架构由管理文件系统元数据的单个名称节点<strong>NameNode</strong>和存储实际数据的一个或多个从数据节点<strong>DataNodes</strong>组成。</p><p>HDFS名称空间中的文件被分成几个块，这些块存储在一组数据节点DataNodes中。NameNode确定块到DataNodes的映射。DataNodes 负责文件系统的读写操作。它们还根据NameNode给出的指令负责块的创建、删除和复制。</p><p>HDFS提供了与任何其他文件系统一样的shell，可以使用命令列表与文件系统交互。这些shell命令将在单独的一章中介绍，并提供适当的示例。</p><h2 id="Hadoop是如何工作的"><a href="#Hadoop是如何工作的" class="headerlink" title="Hadoop是如何工作的"></a>Hadoop是如何工作的</h2><h3 id="Stage-1"><a href="#Stage-1" class="headerlink" title="Stage 1"></a>Stage 1</h3><p>用户/应用程序可以通过以下项目将作业提交给Hadoop (Hadoop作业客户端)进行所需的处理:</p><ol><li>在分布式文件系统中输入和输出文件的位置。</li><li>java类以jar文件的形式包含map和reduce函数的实现。</li><li>通过设置特定于作业的不同参数进行作业配置。</li></ol><h3 id="Stage-2"><a href="#Stage-2" class="headerlink" title="Stage 2"></a>Stage 2</h3><p>然后Hadoop客户端将作业(jar/可执行文件等)和配置提交给JobTracker, JobTracker负责将软件/配置分发给slaves服务器，调度和监视任务，向客户端提供状态和诊断信息。</p><h3 id="Stage-3"><a href="#Stage-3" class="headerlink" title="Stage 3"></a>Stage 3</h3><p>不同节点上的TaskTrackers 按照MapReduce实现执行任务，reduce函数的输出存储在文件系统上的输出文件中。</p><h2 id="Hadoop的优点"><a href="#Hadoop的优点" class="headerlink" title="Hadoop的优点"></a>Hadoop的优点</h2><ul><li>Hadoop框架允许用户快速编写和测试分布式系统。它是高效的，它自动分配数据并跨机器工作，反过来利用CPU核心的底层并行性。</li><li>Hadoop不依赖硬件来提供容错和高可用性(FTHA)，Hadoop库本身被设计来检测和处理应用层的故障。</li><li>可以动态地从集群中添加或删除服务器，Hadoop可以不间断地继续运行。</li><li>Hadoop的另一个巨大优势是，它不仅是开源的，而且可以在所有平台上兼容，因为它是基于Java的。</li></ul><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfaW50cm9kdWN0aW9uLmh0bQ==" title="https://www.tutorialspoint.com/hadoop/hadoop_introduction.htm">https://www.tutorialspoint.com/hadoop/hadoop_introduction.htm<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hadoop是一个用java编写的Apache开源框架，它允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序工作在一个跨计算机集群提供分布式存储和计算的环境中。Hadoop被设计成从单个服务器扩展到数千台机器，每台机器都提供本地计算和存储
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：大数据解决方案</title>
    <link href="https://tangguangen.com/hadoop-big-data-solutions/"/>
    <id>https://tangguangen.com/hadoop-big-data-solutions/</id>
    <published>2018-11-30T12:50:44.000Z</published>
    <updated>2018-12-04T06:05:01.499Z</updated>
    
    <content type="html"><![CDATA[<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><p>在这种方式下，企业将使用一台计算机来存储和处理数据，处理所需的数据，并将其呈现给用户以供分析之用。在这里，数据将存储在RDBMS，如：Oracle数据库、MS SQL Server或DB2以及可以与数据库交互的复杂软件。</p><a id="more"></a><img title="traditional_approach" alt="traditional_approach" src="http://cdn.tangguangen.com/images/traditional_approach.jpg"><h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>这种方法在标准数据库服务器可以容纳的数据量更少或处理数据的处理器的极限的情况下工作得很好。但是当涉及到处理大量数据时，通过传统的数据库服务器来处理这些数据确实是一项非常繁琐的任务。</p><h2 id="Google的解决方案"><a href="#Google的解决方案" class="headerlink" title="Google的解决方案"></a>Google的解决方案</h2><p>谷歌使用<strong>MapReduce</strong>算法解决了这个问题。该算法将任务划分为多个小部分，并将这些小部分分配给通过网络连接的多台计算机，最后收集结果形成最终的结果数据集。</p><img title="mapreduce" alt="mapreduce" src="http://cdn.tangguangen.com/images/mapreduce.jpg"><p>上图显示了各种各样的商品硬件，这些硬件可以是单CPU机器，也可以是容量更大的服务器。</p><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><p>Doug Cutting、Mike Cafarella和团队采用了谷歌提供的解决方案，并在2005年启动了一个名为HADOOP 的开源项目，Doug以他儿子的玩具大象命名了这个项目。现在Apache Hadoop是Apache软件基金会的注册商标。</p><p>Hadoop使用MapReduce算法运行应用程序，数据在不同的CPU节点上并行处理。简而言之，Hadoop框架有足够的能力开发能够在计算机集群上运行的应用程序，并且能够对大量数据执行完整的统计分析。</p><img title="hadoop_framework" alt="hadoop_framework" src="http://cdn.tangguangen.com/images/hadoop_framework.jpg"><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfYmlnX2RhdGFfc29sdXRpb25zLmh0bWw=" title="https://www.tutorialspoint.com/hadoop/hadoop_big_data_solutions.html">https://www.tutorialspoint.com/hadoop/hadoop_big_data_solutions.html<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;传统方法&quot;&gt;&lt;a href=&quot;#传统方法&quot; class=&quot;headerlink&quot; title=&quot;传统方法&quot;&gt;&lt;/a&gt;传统方法&lt;/h2&gt;&lt;p&gt;在这种方式下，企业将使用一台计算机来存储和处理数据，处理所需的数据，并将其呈现给用户以供分析之用。在这里，数据将存储在RDBMS，如：Oracle数据库、MS SQL Server或DB2以及可以与数据库交互的复杂软件。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程：大数据概述</title>
    <link href="https://tangguangen.com/hadoop-big-data-overview/"/>
    <id>https://tangguangen.com/hadoop-big-data-overview/</id>
    <published>2018-11-30T12:12:28.000Z</published>
    <updated>2018-12-04T06:03:30.038Z</updated>
    
    <content type="html"><![CDATA[<p>由于新技术、新设备和社交网站等通信手段的出现，人类产生的数据量每年都在迅速增长。2003年之前的所有数据量总和是50亿G。如果你把数据以磁盘的形式堆起来，它可能会填满整个足球场。到了2011年，每两天就能创造同样的数量，2013年每十分钟创造同样的数量。这一比例仍在大幅增长。虽然所有这些信息都是有意义的，并且在处理时很有用，但它却被忽略了。</p><blockquote><p>世界上90%的数据是在过去几年生成的。</p></blockquote><h2 id="什么是大数据"><a href="#什么是大数据" class="headerlink" title="什么是大数据"></a>什么是大数据</h2><p>大数据其实就是海量的数据，它是不能用传统计算技术处理的海量数据集的集合。<strong>大数据不仅仅是一种数据，它已经成为一门完整的学科，涉及到各种工具、技术和框架。</strong></p><h2 id="大数据的来源"><a href="#大数据的来源" class="headerlink" title="大数据的来源"></a>大数据的来源</h2><p>大数据涉及不同设备和应用产生的数据。以下是大数据保护下的一些领域。</p><ul><li><strong>黑匣子数据</strong>：是直升机、飞机、喷气机等的组成部分，它可以捕捉机组人员的声音、麦克风和耳机的录音，以及飞机的性能信息。</li><li><strong>社交媒体数据：</strong>Facebook和Twitter等社交媒体包含全球数百万人发布的信息和观点。</li><li><strong>股票交易数据：</strong>股票交易数据包含客户对不同公司股票的“买入”和“卖出”决策的信息。</li><li><strong>电网数据：</strong>电网数据包含特定节点相对于基站所消耗的信息。</li><li><strong>搜索引擎数据：</strong>搜索引擎从不同的数据库检索大量数据。</li></ul><p>｛% qnimg big_data.jpg title: big data alt: Big Data %｝</p><p>因此，大数据包括大容量、高速度和可扩展的各种数据。其中的数据有三种类型。</p><ul><li><strong>结构化数据:</strong> 关系型数据库。</li><li><strong>半结构化数据: </strong>XML数据。</li><li><strong>非结构化数据:</strong> Word、PDF、文本、媒体日志。</li></ul><h2 id="大数据带来的好处"><a href="#大数据带来的好处" class="headerlink" title="大数据带来的好处"></a>大数据带来的好处</h2><p>大数据对我们的生活至关重要，它正在成为现代世界最重要的技术之一。下面是我们大家都知道的几个好处:</p><ul><li>利用Facebook等社交网络中保存的信息，营销机构正在了解他们的活动、促销和其他广告媒介的效果。</li><li>利用社交媒体上的信息，如消费者的喜好和对产品满意度，产品公司和零售组织正在优化他们的生产。</li><li>利用患者既往病史资料，医院提供更好、更快的服务。</li></ul><h2 id="大数据技术"><a href="#大数据技术" class="headerlink" title="大数据技术"></a>大数据技术</h2><p>大数据技术在提供更准确的分析方面有很重要的作用，这可以提供更具体的决策，从而提高运营效率，降低成本，降低业务风险。</p><p>想要利用大数据的力量，你需要一个能够实时管理和处理海量结构化和非结构化数据、能够保护数据隐私和安全的基础设施。</p><p>市场上有来自亚马逊、IBM、微软等不同厂商的各种处理大数据的技术。在研究处理大数据的技术时，我们考察了以下两类技术:</p><h3 id="大数据操作"><a href="#大数据操作" class="headerlink" title="大数据操作"></a>大数据操作</h3><p>这包括像MongoDB这样的系统，它提供了实时、交互式工作负载的操作能力，数据主要是在这些工作负载中捕获和存储的。</p><p>NoSQL大数据系统旨在利用过去十年出现的新的云计算架构，以低成本和高效率运行大量计算。这使得操作大数据工作负载更容易管理、更便宜、实现更快。</p><p>一些NoSQL系统可以提供基于实时数据的模式和趋势的洞察，而只需最少的编码，并且不需要数据科学家和额外的基础设施。</p><h3 id="大数据分析"><a href="#大数据分析" class="headerlink" title="大数据分析"></a>大数据分析</h3><p>这包括大规模并行处理(Massively Parallel Processing)数据库系统和MapReduce系统，它们提供可追溯和复杂的分析能力，可能涉及大部分或所有数据的分析。</p><p>MapReduce提供了一种新的数据分析方法，它是SQL提供的功能的补充，并且基于MapReduce的系统可以从单个服务器扩展到数千台高端和低端机器。</p><p>这两类技术是互补的，经常一起部署。</p><h3 id="操作VS分析"><a href="#操作VS分析" class="headerlink" title="操作VS分析"></a>操作VS分析</h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">操作</th><th style="text-align:center">分析</th></tr></thead><tbody><tr><td style="text-align:center">延迟</td><td style="text-align:center">1 ms - 100 ms</td><td style="text-align:center">1 min - 100 min</td></tr><tr><td style="text-align:center">并发</td><td style="text-align:center">1000 - 100,000</td><td style="text-align:center">1 - 10</td></tr><tr><td style="text-align:center">访问模式</td><td style="text-align:center">Writes and Reads</td><td style="text-align:center">Reads</td></tr><tr><td style="text-align:center">查询</td><td style="text-align:center">Selective</td><td style="text-align:center">Unselective</td></tr><tr><td style="text-align:center">数据使用范围</td><td style="text-align:center">Operational</td><td style="text-align:center">Retrospective</td></tr><tr><td style="text-align:center">End User</td><td style="text-align:center">Customer</td><td style="text-align:center">Data Scientist</td></tr><tr><td style="text-align:center">技术</td><td style="text-align:center">NoSQL</td><td style="text-align:center">MapReduce, MPP Database</td></tr></tbody></table><h2 id="大数据的挑战"><a href="#大数据的挑战" class="headerlink" title="大数据的挑战"></a>大数据的挑战</h2><p>与大数据相关的主要挑战如下:</p><ul><li>数据采集</li><li>管理</li><li>存储</li><li>搜索</li><li>共享</li><li>传输</li><li>分析</li><li>展示</li></ul><p>为了完成上述挑战，通常需要企业服务器的帮助。</p><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9oYWRvb3BfYmlnX2RhdGFfb3ZlcnZpZXcuaHRt" title="https://www.tutorialspoint.com/hadoop/hadoop_big_data_overview.htm">https://www.tutorialspoint.com/hadoop/hadoop_big_data_overview.htm<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于新技术、新设备和社交网站等通信手段的出现，人类产生的数据量每年都在迅速增长。2003年之前的所有数据量总和是50亿G。如果你把数据以磁盘的形式堆起来，它可能会填满整个足球场。到了2011年，每两天就能创造同样的数量，2013年每十分钟创造同样的数量。这一比例仍在大幅增长
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop教程</title>
    <link href="https://tangguangen.com/hadoop-tutorial-home/"/>
    <id>https://tangguangen.com/hadoop-tutorial-home/</id>
    <published>2018-11-30T11:53:46.000Z</published>
    <updated>2018-12-04T06:04:34.218Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Hadoop教程目录：</strong></p><ol><li><a href="https://tangguangen.com/hadoop-big-data-overview/">大数据概述</a></li><li><a href="https://tangguangen.com/hadoop-big-data-solutions/">大数据解决方案</a></li><li><a href="https://tangguangen.com/hadoop-introduction/">Hadoop介绍</a></li><li><a href="https://tangguangen.com/hadoop-enviornment-setup">Hadoop安装与环境设置</a></li><li><a href="https://tangguangen.com/hadoop-hdfs-overview">HDFS概述</a></li><li><a href="https://tangguangen.com/hadoop-hdfs-operations">HDFS操作</a></li><li><a href="https://tangguangen.com/hadoop-command-reference">Hadoop命令手册</a></li><li><a href="https://tangguangen.com/hadoop-mapreduce">MapReduce</a></li><li><a href="https://tangguangen.com/hadoop-streaming">Hadoop流</a></li><li><a href="https://tangguangen.com/hadoop-multi-node-cluster">Hadoop全分布式安装</a></li></ol><p>Hadoop以Apache 2.0许可协议发布的<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUJDJTgwJUU2JUJBJTkw" title="https://zh.wikipedia.org/wiki/%E5%BC%80%E6%BA%90">开源<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU4JUJCJTlGJUU5JUFCJTk0JUU2JUExJTg2JUU2JTlFJUI2" title="https://zh.wikipedia.org/wiki/%E8%BB%9F%E9%AB%94%E6%A1%86%E6%9E%B6">软件框架<i class="fa fa-external-link"></i></span>。用户可以通过简单的程序模型在分布式环境集群中存储和处理大数据。 它旨在从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和存储。</p><img title="Hadoop生态系统" alt="Hadoop生态系统" src="http://cdn.tangguangen.com/images/apache-hadoop-ecosystem.png"><p>本教程简要介绍了大数据，MapReduce算法和Hadoop分布式文件系统（HDFS）。</p><p><strong>Audience</strong></p><p>本教程是为希望学习使用Hadoop框架进行大数据分析的基础知识并成为Hadoop开发人员的专业人士准备的。软件专业人员、分析专业人员和ETL开发人员是本课程的主要受益者。</p><p><strong>Prerequisites</strong></p><p>在开始学习本教程之前，我们假设您已经接触过核心Java、数据库概念和任何Linux操作系统风格。</p><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly93d3cudHV0b3JpYWxzcG9pbnQuY29tL2hhZG9vcC9pbmRleC5odG0=" title="https://www.tutorialspoint.com/hadoop/index.htm">https://www.tutorialspoint.com/hadoop/index.htm<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Hadoop教程目录：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://tangguangen.com/hadoop-big-data-overview/&quot;&gt;大数据概述&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https:/
      
    
    </summary>
    
      <category term="大数据" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tangguangen.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://tangguangen.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="https://tangguangen.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Java中的String、StringBuffer、StringBuilder有什么区别？</title>
    <link href="https://tangguangen.com/string-stringbuffer-and-stringbuilder-in-java/"/>
    <id>https://tangguangen.com/string-stringbuffer-and-stringbuilder-in-java/</id>
    <published>2018-11-30T06:16:15.000Z</published>
    <updated>2018-11-30T07:46:59.542Z</updated>
    
    <content type="html"><![CDATA[<p>String、StringBuffer、StringBuilder有什么区别？这个问题在面试中经常碰到，今天主要讲解一下如何<strong>理解Java中的String、StringBuffer、StringBuilder</strong>。</p><h2 id="典型回答"><a href="#典型回答" class="headerlink" title="典型回答"></a>典型回答</h2><p><strong>String</strong> 是 Java 语言非常基础和重要的类，提供了构造和管理字符串的各种基本逻辑。它是典型的<strong>不可变类</strong>（ Immutable ），被声明成为 <strong>final class</strong>，所有属性也都是 final 的。也由于它的不可变性，类似拼接、裁剪字符串等动作，都会产生新的 String 对象。由于字符串操作的普遍性，所以相关操作的效率往往对应用性能有明显影响。</p><p><strong>StringBuffer</strong> 是为解决上面提到拼接产生太多中间对象的问题而提供的一个类，我们可以用 append 或者 add 方法，把字符串添加到已有序列的末尾或者指定位置。StringBuffer 本质是一个<strong>线程安全</strong>的可修改字符序列，它保证了线程安全，也随之带来了额外的性能开销，所以除非有线程安全的需要，不然还是推荐使用它的后继者，也就是 StringBuilder。</p><p><strong>StringBuilder</strong> 是 Java 1.5 中新增的，在能力上和 StringBuffer 没有本质区别，但是它<strong>去掉了线程安全</strong>的部分，有效减小了开销，<strong>是绝大部分情况下进行字符串拼接的首选</strong>。</p><h2 id="考点分析"><a href="#考点分析" class="headerlink" title="考点分析"></a>考点分析</h2><p>几乎所有的应用开发都离不开操作字符串，理解字符串的设计和实现以及相关工具如拼接类的使用，对写出高质量代码是非常有帮助的。关于这个问题，前面的回答是一个通常的概要性回答，至少你要知道 <strong>String 是 Immutable</strong> 的，<strong>字符串操作不当可能会产生大量临时字符串，以及线程安全方面的区别</strong>。</p><p>如果继续深入，面试官可以从各种不同的角度考察，比如可以：</p><ul><li>通过 String 和相关类，考察基本的线程安全设计与实现，各种基础编程实践。</li><li>考察 JVM 对象缓存机制的理解以及如何良好地使用。</li><li>考察 JVM 优化 Java 代码的一些技巧。</li><li>String 相关类的演进，比如 Java 9 中实现的巨大变化。</li><li>…</li></ul><h2 id="知识扩展"><a href="#知识扩展" class="headerlink" title="知识扩展"></a>知识扩展</h2><h3 id="字符串设计和实现考量"><a href="#字符串设计和实现考量" class="headerlink" title="字符串设计和实现考量"></a>字符串设计和实现考量</h3><p>String 是 Immutable 类的典型实现，原生的保证了基础线程安全，因为你无法对它内部数据进行任何修改，这种便利甚至体现在拷贝构造函数中，由于不可变，Immutable 对象在拷贝时不需要额外复制数据。</p><p>我们再来看看 StringBuffer 实现的一些细节，它的线程安全是通过把各种修改数据的方法都加上 synchronized 关键字实现的，非常直白。其实，这种简单粗暴的实现方式，非常适合我们常见的线程安全类实现，不必纠结于 synchronized 性能之类的，有人说“过早优化是万恶之源”，考虑可靠性、正确性和代码可读性才是大多数应用开发最重要的因素。</p><p>为了实现修改字符序列的目的，<strong>StringBuffer 和 StringBuilder</strong> 底层都是利用可修改的（char，JDK 9 以后是 byte）数组，二者都继承了 AbstractStringBuilder，里面包含了基本操作，<strong>区别仅在于最终的方法是否加了 synchronized</strong>。</p><p>在具体的代码书写中，应该如何选择呢？</p><p>在没有线程安全问题的情况下，全部拼接操作是应该都用 StringBuilder 实现吗？毕竟这样书写的代码，还是要多敲很多字的，可读性也不理想，下面的对比非常明显。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">String strByBuilder  = <span class="keyword">new</span></span><br><span class="line">StringBuilder().append(<span class="string">"aa"</span>).append(<span class="string">"bb"</span>).append(<span class="string">"cc"</span>).append</span><br><span class="line">            (<span class="string">"dd"</span>).toString();</span><br><span class="line">             </span><br><span class="line">String strByConcat = <span class="string">"aa"</span> + <span class="string">"bb"</span> + <span class="string">"cc"</span> + <span class="string">"dd"</span>;</span><br></pre></td></tr></table></figure><p>其实，在通常情况下，没有必要过于担心，要相信 Java 还是非常智能的。</p><p>我们来做个实验，把下面一段代码，利用不同版本的 JDK 编译，然后再反编译，例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringConcat</span> </span>&#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">          String myStr = <span class="string">"aa"</span> + <span class="string">"bb"</span> + <span class="string">"cc"</span> + <span class="string">"dd"</span>;   </span><br><span class="line">           System.out.println(<span class="string">"My String:"</span> + myStr);   </span><br><span class="line">      &#125; </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>先编译再反编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/javac StringConcat.java</span><br><span class="line"><span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/javap -v StringConcat.class</span><br></pre></td></tr></table></figure><p>JDK 8 的输出片段是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> 0: ldc           <span class="comment">#2                  // String hellojava!</span></span><br><span class="line"> 2: astore_1</span><br><span class="line"> 3: getstatic     <span class="comment">#3                  // Field java/lang/System.out:Ljava/io/PrintStream;</span></span><br><span class="line"> 6: new           <span class="comment">#4                  // class java/lang/StringBuilder</span></span><br><span class="line"> 9: dup</span><br><span class="line">10: invokespecial <span class="comment">#5                  // Method java/lang/StringBuilder."&lt;init&gt;":()V</span></span><br><span class="line">13: ldc           <span class="comment">#6                  // String Concat String:</span></span><br><span class="line">15: invokevirtual <span class="comment">#7                  // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;</span></span><br><span class="line">18: aload_1</span><br><span class="line">19: invokevirtual <span class="comment">#7                  // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;</span></span><br><span class="line">22: invokevirtual <span class="comment">#8                  // Method java/lang/StringBuilder.toString:()Ljava/lang/String;</span></span><br><span class="line">25: invokevirtual <span class="comment">#9                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V</span></span><br><span class="line">28: <span class="built_in">return</span></span><br></pre></td></tr></table></figure><p>你可以看到，在 JDK 8 中，字符串拼接操作会自动被 javac 转换为 StringBuilder 操作，而在 JDK 9 里面则是因为 Java 9 为了更加统一字符串操作优化，提供了 StringConcatFactory，作为一个统一的入口。javac 自动生成的代码，虽然未必是最优化的，但普通场景也足够了，你可以酌情选择。</p><p><strong>参考自极客时间：Java核心技术36讲</strong></p><p><strong>感谢原作者：杨晓峰老师</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;String、StringBuffer、StringBuilder有什么区别？这个问题在面试中经常碰到，今天主要讲解一下如何&lt;strong&gt;理解Java中的String、StringBuffer、StringBuilder&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&quot;典型回
      
    
    </summary>
    
      <category term="Java" scheme="https://tangguangen.com/categories/Java/"/>
    
    
      <category term="java" scheme="https://tangguangen.com/tags/java/"/>
    
      <category term="string" scheme="https://tangguangen.com/tags/string/"/>
    
      <category term="stringbuffer" scheme="https://tangguangen.com/tags/stringbuffer/"/>
    
      <category term="stringbuilder" scheme="https://tangguangen.com/tags/stringbuilder/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 344.反转字符串</title>
    <link href="https://tangguangen.com/leetcode-344-reverse-string/"/>
    <id>https://tangguangen.com/leetcode-344-reverse-string/</id>
    <published>2018-11-30T04:49:20.000Z</published>
    <updated>2018-11-30T07:33:23.791Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><blockquote><p>编写一个函数，其作用是将输入的字符串反转过来。</p><p><strong>示例 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: &quot;hello&quot;</span><br><span class="line">&gt; 输出: &quot;olleh&quot;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>示例 2:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: &quot;A man, a plan, a canal: Panama&quot;</span><br><span class="line">&gt; 输出: &quot;amanaP :lanac a ,nalp a ,nam A&quot;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ol><li><p>使用StringBuider或StringBuffer修改字符序列，关于StringBuilder和Stringbuffer的区别可以参考我的零一篇文章<a href="https://tangguangen.com/string-stringbuffer-and-stringbuilder-in-java/">Java中的String、StringBuffer、StringBuilder有什么区别？</a></p></li><li><p>将字符串转化为字符串数组，对换首尾字符位置即可。</p><h2 id="Java-AC"><a href="#Java-AC" class="headerlink" title="Java AC"></a>Java AC</h2><ol><li>使用StringBuilder</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">reverseString</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        String res = <span class="keyword">new</span> StringBuilder(s).reverse().toString();</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>转换为数组</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">reverseString</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="keyword">char</span>[] arr = s.toCharArray();</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>, j = arr.length - <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt; j) &#123;</span><br><span class="line"><span class="keyword">char</span> tmp = arr[i];</span><br><span class="line">arr[i] = arr[j];</span><br><span class="line">arr[j] = tmp;</span><br><span class="line">i++;</span><br><span class="line">j--;</span><br><span class="line">&#125;</span><br><span class="line">String str = String.valueOf(arr);</span><br><span class="line"><span class="keyword">return</span> str;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;编写一个函数，其作用是将输入的字符串反转过来。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例 1:&lt;/str
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle比赛top2%特征探索之featexp</title>
    <link href="https://tangguangen.com/secret-to-be-in-top-2-of-kaggle/"/>
    <id>https://tangguangen.com/secret-to-be-in-top-2-of-kaggle/</id>
    <published>2018-11-30T02:03:21.000Z</published>
    <updated>2018-11-30T02:39:12.071Z</updated>
    
    <content type="html"><![CDATA[<p>在数值数据上构建任意监督学习模型的一个重要方面是理解特征。查看模型的部分依赖图可帮助理解任意特征对模型输出的影响。</p><img title="模型依赖图" alt="模型依赖图" src="http://cdn.tangguangen.com/images/模型依赖图.png"><p>但是，部分依赖图存在一个问题，即它们是使用训练好的模型创建的。如果我们可以从训练数据中直接创建部分依赖图，那么它将帮助我们更好地理解底层数据。事实上，它能够帮助你做好以下事情：</p><ol><li>特征理解</li><li>识别带噪声的特征 (<strong>the most interesting part!</strong>)</li><li>特征工程</li><li>特征重要性</li><li>特征 debug</li><li>泄露检测和理解</li><li>模型监控</li></ol><p>为了使其更加易于使用，作者将这些技术封装进一个 Python 包 featexp 中，本文将介绍如何使用它进行特征探索。本文使用的是 Kaggle Home Credit Default Risk 竞赛的应用数据集。该竞赛的任务是使用给定数据预测违约者。</p><p>featexp：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FiaGF5c3Bhd2FyL2ZlYXRleHA=" title="https://github.com/abhayspawar/featexp">https://github.com/abhayspawar/…<i class="fa fa-external-link"></i></span></p><p><strong>1. 特征理解</strong></p><img title="Scatter" alt="Scatter" src="http://cdn.tangguangen.com/images/feature.png"><p>如果依赖变量（目标）是二元的，则散点图无效，因为所有点要么是 0 要么是 1。对于连续目标来说，数据点太多会造成难以理解目标 vs 特征趋势。featexp 创建了更好的图，可帮助解决该问题。我们来试一下！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> featexp <span class="keyword">import</span> get_univariate_plots</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plots drawn for all features if nothing is passed in feature_list parameter.</span></span><br><span class="line">get_univariate_plots(data=data_train, target_col=<span class="string">'target'</span>, </span><br><span class="line">                     features_list=[<span class="string">'DAYS_BIRTH'</span>], bins=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><img title="target" alt="Feature" src="http://cdn.tangguangen.com/images/target.png"><p>featexp 为数值特征创建了同等人口数量的 bin（x 轴），然后计算每个 bin 的目标平均值，再绘制出来（如上图左）。在我们的案例中，目标平均值是违约率。该图告诉我们年龄越大的客户违约率越低。这些图帮助我们理解特征表达的意义，及其对模型的影响。右图显示了每个 bin 中客户的数量。</p><p><strong>2. 识别带噪声的特征</strong></p><p>带噪声的特征导致过拟合，识别它们并非易事。在 featexp 中，你可以输出一个测试集（或者验证集），对比训练／测试集中的特征趋势来确定带噪声的特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_univariate_plots(data=data_train, target_col=<span class="string">'target'</span>, data_test=data_test, features_list=[<span class="string">'DAYS_EMPLOYED'</span>])</span><br></pre></td></tr></table></figure><img title="Comparison" alt="Comparison" src="http://cdn.tangguangen.com/images/1_tpjxrjbxhH-lJo0hbRerfg.png"><p>featexp 计算两个指标（如上图所示），来帮助测量噪声：</p><ol><li>趋势相关度（见测试图）：如果某个特征未体现目标在训练集和测试集中的同样趋势，它会导致过拟合，因为模型会学习一些在测试数据中并不使用的东西。趋势相关度有助于理解训练／测试趋势的相似度，如何利用训练和测试集的 bin 的平均目标值来计算趋势相关度。上图中的特征相关度为 99%，几乎没有噪声。</li><li>趋势变化：趋势方向中突然和重复的变化可能表明有噪声。但是，此类趋势变化也会在 bin 的人口数量与其它特征不同时，导致其违约率无法与其它 bin 进行对比。</li></ol><p>下图中的特征没有展现同样的趋势，因为趋势相关度为 85%。这两个指标可用于删除带噪声的特征。</p><img title="Example" alt="Example" src="http://cdn.tangguangen.com/images/1_6lSWurF_qOzm1cMEJFuRmA.png"><p>当特征很多且相互关联时，删除低趋势相关度特征的效果很好。它会带来更少的过拟合，其它相关特征可以避免信息损失。同时需要注意不要删除太多重要特征，因为这可能导致性能下降。此外，你无法利用特征重要性来判断特征是否带噪声，因为重要的特征也会带噪声！</p><p>使用不同时间段的测试数据效果更好，因为你可以借此确定特征趋势是否一直如此。</p><p>featexp 中的 get_trend_stats() 函数返回展示趋势相关度的数据帧，并随着特征而改变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> featexp <span class="keyword">import</span> get_trend_stats</span><br><span class="line">stats = get_trend_stats(data=data_train, target_col=<span class="string">'target'</span>, data_test=data_test)</span><br></pre></td></tr></table></figure><img src="http://cdn.tangguangen.com/images/1_RuxmJA0iWrMRCVxNGlBidw.png"><p>下面我们就试着删除数据中低趋势相关度的特征，然后看结果是否有所改进。</p><img title="AUC" alt="AUC" src="http://cdn.tangguangen.com/images/1_UR-SlR1rZOjp0sjTIaPZ_A.png"><p>我们可以看到，趋势相关度阈值越高，排行榜（LB）AUC 越高。不删除重要的特征进一步将 LB AUC 提高到 0.74。测试 AUC 的变化与 LB AUC 不同，这一点也很有趣。完整代码详见 featexp_demo notebook：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FiaGF5c3Bhd2FyL2ZlYXRleHAvYmxvYi9tYXN0ZXIvZmVhdGV4cF9kZW1vLmlweW5i" title="https://github.com/abhayspawar/featexp/blob/master/featexp_demo.ipynb">https://github.com/abhayspawar/…<i class="fa fa-external-link"></i></span>。</p><p><strong>3. 特征工程</strong></p><p>通过查看这些图所获取的见解可以帮助你创建更好的特征。更好地理解数据将带来更好的特征工程。此外，它还可以帮助你改善现有特征。下面我们来看另一个特征 EXT_SOURCE_1：</p><img src="http://cdn.tangguangen.com/images/1_MQQrhVy5NjtD-7mrKT8jOA.png"><p>具备高 EXT_SOURCE_1 的客户具备较低的违约率。但是，第一个 bin（违约率约 8%）没有遵循该特征趋势（向上升后下降）。它的负值是-99.985，而且人口数量较多。这可能表明这些是特殊的值，因此不遵循特征趋势。幸运的是，非线性模型在学习该关系方面不会有问题。而对于线性模型（如 logistic 回归），此类特殊值和空缺值应该采用类似样本的默认值进行估计，而不是特征平均值。</p><p><strong>4. 特征重要性</strong></p><p>featexp 还可以帮助衡量特征重要性。DAYS_BIRTH 和 EXT_SOURCE_1 都具备很好的趋势。但是 EXT_SOURCE_1 的人口数量集中于特殊值 bin，这表明其重要性可能不如 DAYS_BIRTH。基于 XGBoost 模型的特征重要性，DAYS_BIRTH 的重要性高于 EXT_SOURCE_1。</p><p><strong>5. 特征 debug</strong></p><p>查看 featexp 图可以帮助你捕捉复杂特征工程中的 bug：</p><img src="http://cdn.tangguangen.com/images/1_-NA-fc1LR1yo0JoHp8IFOw.png"><p>检查一下特征的人数分布看起来是否正确。由于存在一些小 bug，我个人经常遭遇上述极端情况。</p><p>在看这些图之前，一定要假设特征趋势会是什么样子。如果特征趋势看起来不像你期望的那样，可能表示其中存在一些问题。坦率地说，这种假设趋势的过程使得构建 ML 模型更加有趣！</p><p><strong>6 泄露检测</strong></p><p>从目标到特征的数据泄露会导致过拟合。泄露特征具有很高的特征重要性，但很难理解为什么特征会发生泄露。查看下列 featexp 图可以帮助你理解。</p><p>下面的特征在「Null」bin 中违约率为 0%，在其它 bin 中为 100%。很明显，这是极端的泄露案例。该特征只有在客户违约时才有价值。根据特征是什么，这可能是因为 bug 或者该特征只为违约者填充（在这种情况下它会下降）。弄清楚特征泄露的原因可以加速 debug。</p><img src="http://cdn.tangguangen.com/images/1_muwhOmAYJTjSZetBv1UOaA.png"><p><strong>7 模型监控</strong></p><p>由于 featexp 计算两个数据集之间的趋势相关性，它可以很轻易地用于模型监控。每次模型被重新训练之后，就可以把新的训练数据与测试好的训练数据（通常从第一次构建模型开始训练数据）进行对比。趋势相关性能够帮助你监控特征信息与目标的关系是否发生任何变化。</p><hr><p><strong>感谢原作者：Abhay Pawar</strong></p><p><strong>原文链接：</strong><span class="exturl" data-url="aHR0cHM6Ly90b3dhcmRzZGF0YXNjaWVuY2UuY29tL215LXNlY3JldC1zYXVjZS10by1iZS1pbi10b3AtMi1vZi1hLWthZ2dsZS1jb21wZXRpdGlvbi01N2NmZjA2NzdkM2M=" title="https://towardsdatascience.com/my-secret-sauce-to-be-in-top-2-of-a-kaggle-competition-57cff0677d3c">https://towardsdatascience.com/…<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在数值数据上构建任意监督学习模型的一个重要方面是理解特征。查看模型的部分依赖图可帮助理解任意特征对模型输出的影响。&lt;/p&gt;
&lt;img title=&quot;模型依赖图&quot; alt=&quot;模型依赖图&quot; src=&quot;http://cdn.tangguangen.com/images/模型依赖图
      
    
    </summary>
    
      <category term="Kaggle" scheme="https://tangguangen.com/categories/Kaggle/"/>
    
    
      <category term="kaggle" scheme="https://tangguangen.com/tags/kaggle/"/>
    
      <category term="featexp" scheme="https://tangguangen.com/tags/featexp/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 网站优化之SEO</title>
    <link href="https://tangguangen.com/Hexo-website-seo/"/>
    <id>https://tangguangen.com/Hexo-website-seo/</id>
    <published>2018-11-29T03:05:10.000Z</published>
    <updated>2018-11-29T11:36:00.206Z</updated>
    
    <content type="html"><![CDATA[<p>搭建好了个人博客，如果想让Google和百度等搜索引擎搜索到自己的博客，增加博客的访问量，那么对网站做一些SEO(Search Engine Optimization)，即搜索引擎优化，是非常有必要的，下面介绍一些简单的SEO优化方法。</p><h2 id="添加sitemap"><a href="#添加sitemap" class="headerlink" title="添加sitemap"></a>添加sitemap</h2><p>Sitemap即网站地图，它的作用在于便于搜索引擎更加智能地抓取网站。最简单和常见的sitemap形式，是XML文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新时间、更新的频率及相对其他网址重要程度等）。</p><p><strong>Setp 1:</strong> 安装sitemap站点地图自动生成插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-sitemap --save</span><br><span class="line">npm install hexo-generator-baidu-sitemap --save</span><br></pre></td></tr></table></figure><p><strong>Setp 2:</strong> 配置站点跟目录下的_config.yml，添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hexo sitemap网站地图</span></span><br><span class="line">sitemap:</span><br><span class="line">path: sitemap.xml</span><br><span class="line">baidusitemap:</span><br><span class="line">path: baidusitemap.xml</span><br></pre></td></tr></table></figure><h2 id="添加robots-txt"><a href="#添加robots-txt" class="headerlink" title="添加robots.txt"></a>添加robots.txt</h2><p>obots.txt是一种存放于网站根目录下的ASCII编码的文本文件，它的作用是告诉搜索引擎此网站中哪些内容是可以被爬取的，哪些是禁止爬取的。robots.txt应该放在站点目录下的source文件中，网站生成后在网站的根目录(<code>站点目录/public/</code>)下。</p><p>我的robots.txt内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># hexo robots.txt</span><br><span class="line">User-agent: * </span><br><span class="line">Allow: /</span><br><span class="line">Allow: /archives/</span><br><span class="line">Allow: /categories/</span><br><span class="line">Allow: /about/</span><br><span class="line"></span><br><span class="line">Disallow: /vendors/</span><br><span class="line">Disallow: /js/</span><br><span class="line">Disallow: /css/</span><br><span class="line">Disallow: /fonts/</span><br><span class="line">Disallow: /vendors/</span><br><span class="line">Disallow: /fancybox/</span><br><span class="line"></span><br><span class="line">Sitemap: https://tangguangen.com/sitemap.xml</span><br><span class="line">Sitemap: https://tangguangen.com/baidusitemap.xml</span><br></pre></td></tr></table></figure><h2 id="提交百度站长"><a href="#提交百度站长" class="headerlink" title="提交百度站长"></a>提交百度站长</h2><p><strong>Setp 1:</strong> 添加网站，百度提交网址入口<span class="exturl" data-url="aHR0cHM6Ly96aXl1YW4uYmFpZHUuY29tL3NpdGUvaW5kZXg=" title="https://ziyuan.baidu.com/site/index">点这里<i class="fa fa-external-link"></i></span></p><img title="百度站长添加网站" alt="百度站长添加网站" src="http://cdn.tangguangen.com/images/Hexo-website-seo/1.PNG"><p>这里按提示操作即可，网站验证方式建议使用CNAME验证。</p><p><strong>Setp2:</strong> robots验证</p><img title="百度站长robots验证" alt="百度站长robots验证" src="http://cdn.tangguangen.com/images/Hexo-website-seo/2.png"><p><strong>Setp3:</strong> 抓取诊断</p><img title="百度站长抓取诊断" alt="百度站长抓取诊断" src="http://cdn.tangguangen.com/images/Hexo-website-seo/3.png"><p>之所以要进行抓取诊断，是因为githubpage是不允许百度爬虫抓取的，如果你的网站是部署在GitHub上面的话就要选择<strong>主动推送</strong>的方式，如果抓取诊断成功的话就可以直接提交站点地图baidusitemap</p><h3 id="方式一：主动推送"><a href="#方式一：主动推送" class="headerlink" title="方式一：主动推送"></a>方式一：主动推送</h3><p><strong>Setp 1:</strong> 安装主动推送插件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-baidu-url-submit --save</span><br></pre></td></tr></table></figure><p><strong>Setp 2:</strong> 配置站点跟目录下的_config.yml，添加一下内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">baidu_url_submit:</span><br><span class="line">  count: 1 ## 提交最新的 1 个链接</span><br><span class="line">  host: tangguangen.com ## 在百度站长平台中注册的域名</span><br><span class="line">  token: your token ## 请注意这是您的秘钥，所以请不要把博客源代码发布在公众仓库里!</span><br><span class="line">  path: baidu_urls.txt ## 文本文档的地址， 新链接会保存在此文本文档里</span><br></pre></td></tr></table></figure><img title="获取主动推送token" alt="获取主动推送token" src="http://cdn.tangguangen.com/images/Hexo-website-seo/4.png"><p><strong>Setp 3:</strong>  配置站点跟目录下的_config.yml，添加deploy类型</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">deploy: </span><br><span class="line">  - type: git</span><br><span class="line">    repo: git@github.com:xguojing/xguojing.github.io.git</span><br><span class="line">    branch: master</span><br><span class="line">  - type: baidu_url_submitter # 这里是新添加的</span><br><span class="line">    repo:</span><br></pre></td></tr></table></figure><p>最后记得把<strong>主题配置文件</strong>中的<code>baidu_push</code>设置为<code>true</code>，重新渲染部署即可。</p><h3 id="方式二：提交sitemap"><a href="#方式二：提交sitemap" class="headerlink" title="方式二：提交sitemap"></a>方式二：提交sitemap</h3><img title="提交百度站点地图" alt="提交百度站点地图" src="http://cdn.tangguangen.com/images/Hexo-website-seo/5.png"><h2 id="提交Google站长"><a href="#提交Google站长" class="headerlink" title="提交Google站长"></a>提交Google站长</h2><p><strong>Setp 1:</strong> 添加网站</p><p>进入<span class="exturl" data-url="aHR0cHM6Ly9saW5rLmppYW5zaHUuY29tLz90PWh0dHBzJTNBJTJGJTJGd3d3Lmdvb2dsZS5jb20lMkZ3ZWJtYXN0ZXJzJTJG" title="https://link.jianshu.com/?t=https%3A%2F%2Fwww.google.com%2Fwebmasters%2F">Google Search Console<i class="fa fa-external-link"></i></span>，相信大家都有Google账号吧。没有的话注册个账号吧，然后登录进去即可。</p><p>按提示操作添加网址，然后Google会让你验证你对网站的所有权，我选择的是HTML标记的验证方式</p><img src="http://cdn.tangguangen.com/images/Hexo-website-seo/6.png"><p>配置<strong>主题配置文件</strong>_config.yml，填写google_site_verification，之后重新渲染部署点击验证即可</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">google_site_verification:</span> <span class="string">your</span> <span class="string">google_site_verification</span></span><br></pre></td></tr></table></figure><p><strong>Setp 2:</strong> 添加站点地图sitemap</p><img title="添加sitemap" alt="添加sitemap" src="http://cdn.tangguangen.com/images/Hexo-website-seo/7.png"><h2 id="优化title"><a href="#优化title" class="headerlink" title="优化title"></a>优化title</h2><p>编辑站点目录下的<code>themes/layout/index.swig</code>文件，</p><p>将下面的代码</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;%</span> <span class="string">block</span> <span class="string">title</span> <span class="string">%&#125;</span> <span class="string">&#123;&#123;</span> <span class="string">config.title</span> <span class="string">&#125;&#125;</span> <span class="string">&#123;%</span> <span class="string">endlock</span> <span class="string">%&#125;</span></span><br></pre></td></tr></table></figure><p>改成</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;%</span> <span class="string">block</span> <span class="string">title</span> <span class="string">%&#125;</span> <span class="string">&#123;&#123;</span> <span class="string">config.title</span> <span class="string">&#125;&#125;</span> <span class="bullet">-</span> <span class="string">&#123;&#123;</span> <span class="string">theme.description</span> <span class="string">&#125;&#125;</span> <span class="string">&#123;%</span> <span class="string">endlock</span> <span class="string">%&#125;</span></span><br></pre></td></tr></table></figure><p>这时将网站的描述及关键词加入了网站的<code>title</code>中，更有利于详细地描述网站。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>开博客主要是为了记录和分享学习经历、技术笔记等，通过记录总结的方式提高学习效率，在互联网中和更多的人交流学习。学无止境，不忘初心。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;搭建好了个人博客，如果想让Google和百度等搜索引擎搜索到自己的博客，增加博客的访问量，那么对网站做一些SEO(Search Engine Optimization)，即搜索引擎优化，是非常有必要的，下面介绍一些简单的SEO优化方法。&lt;/p&gt;
&lt;h2 id=&quot;添加site
      
    
    </summary>
    
      <category term="Hexo" scheme="https://tangguangen.com/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="https://tangguangen.com/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 292. Nim游戏</title>
    <link href="https://tangguangen.com/LeetCode-292-Nim%E6%B8%B8%E6%88%8F/"/>
    <id>https://tangguangen.com/LeetCode-292-Nim游戏/</id>
    <published>2018-11-28T07:19:19.000Z</published>
    <updated>2018-11-29T05:53:20.459Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h2><blockquote><p>你和你的朋友，两个人一起玩 <span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS9OaW0lRTYlQjglQjglRTYlODglOEYvNjczNzEwNQ==" title="https://baike.baidu.com/item/Nim%E6%B8%B8%E6%88%8F/6737105">Nim游戏<i class="fa fa-external-link"></i></span>：桌子上有一堆石头，每次你们轮流拿掉 1 - 3 块石头。 拿掉最后一块石头的人就是获胜者。你作为先手。</p><p>你们是聪明人，每一步都是最优解。 编写一个函数，来判断你是否可以在给定石头数量的情况下赢得游戏。</p><p><strong>示例:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; 输入: 4</span><br><span class="line">&gt; 输出: false </span><br><span class="line">&gt; 解释: 如果堆中有 4 块石头，那么你永远不会赢得比赛；</span><br><span class="line">&gt;      因为无论你拿走 1 块、2 块 还是 3 块石头，最后一块石头总是会被你的朋友拿走。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>简单题，<span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlQjclQjQlRTQlQkIlODAlRTUlOEQlOUElRTUlQTUlOTUvNzEzOTc4Mg==" title="https://baike.baidu.com/item/%E5%B7%B4%E4%BB%80%E5%8D%9A%E5%A5%95/7139782">巴什博弈<i class="fa fa-external-link"></i></span>。</p><p>显然，如果n=m+1，那么由于一次最多只能取m个，所以，无论先取者拿走多少个，后取者都能够一次拿走剩余的物品，后者取胜。因此我们发现了如何取胜的法则：如果n=（m+1）r+s，（r为任意自然数，s≤m),那么先取者要拿走s个物品，如果后取者拿走k（≤m)个，那么先取者再拿走m+1-k个，结果剩下（m+1）（r-1）个，以后保持这样的取法，那么先取者肯定获胜。总之，要保持给对手留下（m+1）的倍数，就能最后获胜。</p><p>对于巴什博弈，那么我们规定，如果最后取光者输，那么又会如何呢？</p><p>（n-1）%（m+1）==0则后手胜利</p><p>先手会重新决定策略，所以不是简单的相反行的</p><h2 id="JAVA-SOLUTION"><a href="#JAVA-SOLUTION" class="headerlink" title="JAVA SOLUTION"></a>JAVA SOLUTION</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canWinNim</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> n % <span class="number">4</span> == <span class="number">0</span> ? <span class="keyword">false</span> : <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><h3 id="威佐夫博奕"><a href="#威佐夫博奕" class="headerlink" title="威佐夫博奕"></a><span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlQTglODElRTQlQkQlOTAlRTUlQTQlQUIlRTUlOEQlOUElRTUlQkMlODgvMTk4NTgyNTY=" title="https://baike.baidu.com/item/%E5%A8%81%E4%BD%90%E5%A4%AB%E5%8D%9A%E5%BC%88/19858256">威佐夫博奕<i class="fa fa-external-link"></i></span></h3><p>威佐夫博弈（Wythoff’s game）：有两堆各若干个物品，两个人轮流从任一堆取至少一个或同时从两堆中取同样多的物品，规定每次至少取一个，多者不限，最后取光者得胜。</p><p><strong>两个人如果都采用正确操作，那么面对非奇异局势，先拿者必胜；反之，则后拿者取胜。</strong></p><p>那么任给一个局势， (a，b)，怎样判断它是不是奇异局势呢？我们有如下公式：<br>$$<br>ak =\lfloor \frac{k}{2}(1+\sqrt{5}) \rfloor ，<br>$$</p><p>$$<br>bk= ak + k \space \space\space\space（k=0，1，2，…n)<br>$$</p><h3 id="尼姆博奕"><a href="#尼姆博奕" class="headerlink" title="尼姆博奕"></a><span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTUlQjAlQkMlRTUlQTclODYlRTUlOEQlOUElRTUlQTUlOTU=" title="https://baike.baidu.com/item/%E5%B0%BC%E5%A7%86%E5%8D%9A%E5%A5%95">尼姆博奕<i class="fa fa-external-link"></i></span></h3><p>指的是这样的一个博弈游戏，目前有任意堆石子，每堆石子个数也是任意的，双方轮流从中取出石子，规则如下：<br>1)每一步应取走至少一枚石子；每一步只能从某一堆中取走部分或全部石子；<br>2)如果谁取到最后一枚石子就胜。</p><p>判断当前局势是否为必胜（必败）局势：<br>把所有堆的石子数目用二进制数表示出来，当<strong>全部这些数按位异或</strong>结果为0时当前局面为必败局面，否则为必胜局面；</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;  </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="keyword">int</span> temp[ <span class="number">20</span> ]; <span class="comment">//火柴的堆数  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> i, n, min;  </span><br><span class="line">    <span class="keyword">while</span>( <span class="built_in">cin</span> &gt;&gt; n )  </span><br><span class="line">    &#123;  </span><br><span class="line">        <span class="keyword">for</span>( i = <span class="number">0</span>; i &lt; n; i++ )  </span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; temp[ i ]; <span class="comment">//第i个火柴堆的数量  </span></span><br><span class="line">        min = temp[ <span class="number">0</span> ];  </span><br><span class="line">        <span class="keyword">for</span>( i = <span class="number">1</span>; i &lt; n ; i++ )  </span><br><span class="line">            min = min^temp[ i ]; <span class="comment">//按位异或  </span></span><br><span class="line">        <span class="keyword">if</span>( min == <span class="number">0</span> )  </span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"Lose"</span> &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//输  </span></span><br><span class="line">        <span class="keyword">else</span>  </span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"Win"</span> &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//赢  </span></span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="斐波那契博弈"><a href="#斐波那契博弈" class="headerlink" title="斐波那契博弈"></a><strong>斐波那契博弈</strong></h3><p>有一堆个数为n的石子，游戏双方轮流取石子，满足：<br>1)先手不能在第一次把所有的石子取完；<br>2)之后每次可以取的石子数介于1到对手刚取的石子数的2倍之间（包含1和对手刚取的石子数的2倍）。<br>约定取走最后一个石子的人为赢家，求必败态。</p><p>这个游戏叫做斐波那契博弈，肯定和<span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS8lRTYlOTYlOTAlRTYlQjMlQTIlRTklODIlQTMlRTUlQTUlOTElRTYlOTUlQjAlRTUlODglOTcvOTkxNDU=" title="https://baike.baidu.com/item/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/99145">斐波那契数列<i class="fa fa-external-link"></i></span>：$f[n]：1,2,3,5,8,13,21,34,55,89,… $有密切的关系。如果试验一番之后，可以猜测：<strong>先手胜当且仅当n不是斐波那契数。换句话说，必败态构成斐波那契数列。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目描述：&quot;&gt;&lt;a href=&quot;#题目描述：&quot; class=&quot;headerlink&quot; title=&quot;题目描述：&quot;&gt;&lt;/a&gt;题目描述：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;你和你的朋友，两个人一起玩 &lt;span class=&quot;exturl&quot; data-url=&quot;
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://tangguangen.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://tangguangen.com/tags/LeetCode/"/>
    
  </entry>
  
</feed>
